{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# Cell 1: GitHub Setup and Auto-Logging\n\nimport os\nimport sys\nimport importlib.util\nimport psycopg2\n\n# GitHub credentials - use Kaggle secrets for security\ngithub_token = os.environ.get(\"GITHUB_TOKEN\")\nrepo_url = f\"https://{github_token}@github.com/amiralpert/SmartReach.git\"\nlocal_path = \"/kaggle/working/SmartReach\"\n\n# Clone or update repo\nif os.path.exists(local_path):\n    !cd {local_path} && git pull\nelse:\n    !git clone {repo_url} {local_path}\n\n# Add to Python path for regular imports\nsys.path.insert(0, f'{local_path}/BizIntel')\n\n# Direct import of pipeline\npipeline_path = f\"{local_path}/BizIntel/Modules/SystemUno/Patents/patentlens_pipeline_v3.py\"\n\nspec = importlib.util.spec_from_file_location(\"patentlens_pipeline_v3\", pipeline_path)\npipeline_module = importlib.util.module_from_spec(spec)\nsys.modules[\"patentlens_pipeline_v3\"] = pipeline_module\nspec.loader.exec_module(pipeline_module)\n\nPatentLensPipeline = pipeline_module.PatentLensPipeline\nPatentData = pipeline_module.PatentData\nKeywordManager = pipeline_module.KeywordManager\n\nprint(\"✓ Pipeline module imported from GitHub!\")\n\n# Set up database configuration\nNEON_CONFIG = {\n    'host': 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech',\n    'database': 'BizIntelSmartReach',\n    'user': 'neondb_owner',\n    'password': 'npg_aTFt6Pug3Kpy',\n    'sslmode': 'require'\n}\n\n# Try to set up logger, but don't fail if there are issues\ntry:\n    # Create separate connection for logger\n    logger_conn = psycopg2.connect(**NEON_CONFIG)\n    print(\"✓ Database connected for logger\")\n\n    # Import auto-logger using direct file import\n    logger_module_path = f\"{local_path}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\n    if os.path.exists(logger_module_path):\n        spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_module_path)\n        auto_logger_module = importlib.util.module_from_spec(spec)\n        sys.modules[\"auto_logger\"] = auto_logger_module\n        spec.loader.exec_module(auto_logger_module)\n\n        setup_auto_logging = auto_logger_module.setup_auto_logging\n        logger = setup_auto_logging(logger_conn, \"PatentLens\")\n        print(\"✓ Auto-logging enabled!\")\n    else:\n        print(f\"✗ Auto-logger not found at {logger_module_path}\")\n        logger = None\nexcept Exception as e:\n    print(f\"⚠️ Logger setup failed: {e}\")\n    print(\"  Continuing without auto-logging...\")\n    logger = None\n\nprint(\"\\n✅ Setup complete. Pipeline ready to use.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install required packages\n!pip install -q transformers accelerate bitsandbytes\n!pip install -q sentence-transformers psycopg2-binary\n!pip install -q torch numpy pandas tqdm\n\nimport os\nimport sys\nimport importlib.util\nimport json\nimport psycopg2\nimport pandas as pd\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"✓ Packages installed and imported\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-27T19:15:45.502346Z",
     "iopub.execute_input": "2025-08-27T19:15:45.502732Z",
     "iopub.status.idle": "2025-08-27T19:17:54.170848Z",
     "shell.execute_reply.started": "2025-08-27T19:15:45.502690Z",
     "shell.execute_reply": "2025-08-27T19:17:54.170124Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-08-27 19:17:33.605841: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756322253.979276      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756322254.083044      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "# Neon Database Configuration\nNEON_CONFIG = {\n    'host': 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech',\n    'database': 'BizIntelSmartReach',\n    'user': 'neondb_owner',\n    'password': 'npg_aTFt6Pug3Kpy',\n    'sslmode': 'require'\n}\n\n# Test database connection\ndef test_database_connection():\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Check tables\n        cursor.execute('''\n            SELECT \n                (SELECT COUNT(*) FROM raw_data.patents_full_text) as patents,\n                (SELECT COUNT(*) FROM core.companies) as companies,\n                (SELECT COUNT(*) FROM system_uno.patents_processing_status) as status_records,\n                (SELECT COUNT(*) FROM system_uno.patents_keywords) as keywords,\n                (SELECT COUNT(*) FROM system_uno.patents_extracted_knowledge) as extractions\n        ''')\n        \n        counts = cursor.fetchone()\n        print(\"✓ Database connected successfully!\")\n        print(f\"  Patents: {counts[0]}\")\n        print(f\"  Companies: {counts[1]}\")\n        print(f\"  Status records: {counts[2]}\")\n        print(f\"  Keywords: {counts[3]}\")\n        print(f\"  Extractions: {counts[4]}\")\n        \n        cursor.close()\n        conn.close()\n        return True\n        \n    except Exception as e:\n        print(f\"✗ Database connection failed: {e}\")\n        return False\n\n# Test connection\ntest_database_connection()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-27T19:18:44.431802Z",
     "iopub.execute_input": "2025-08-27T19:18:44.432528Z",
     "iopub.status.idle": "2025-08-27T19:18:45.018415Z",
     "shell.execute_reply.started": "2025-08-27T19:18:44.432498Z",
     "shell.execute_reply": "2025-08-27T19:18:45.017423Z"
    }
   },
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "# Load Llama-3.1-8B-Instruct with 4-bit quantization\nprint(\"Logging in to HuggingFace...\")\n\nfrom huggingface_hub import login\n\n# Define model name FIRST\nmodel_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n\n# Get HuggingFace token from Kaggle secrets\nhf_token = os.environ.get('HUGGINGFACE_TOKEN')\n\nif hf_token:\n    login(token=hf_token)\n    print(\"✓ Logged in to HuggingFace\")\nelse:\n    print(\"✗ No HuggingFace token found in secrets\")\n\nprint(\"\\nLoading Llama-3.1-8B-Instruct...\")\n\n# Configure 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.chat_template = tokenizer.default_chat_template\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        trust_remote_code=True\n    )\n\n    print(\"✓ Llama model loaded successfully\")\n    \n    # Test generation with chat format\n    test_messages = [\n        {\"role\": \"user\", \"content\": \"What is cancer? Answer in one sentence.\"}\n    ]\n    test_input = tokenizer.apply_chat_template(test_messages, return_tensors=\"pt\", tokenize=True)\n    \n    with torch.no_grad():\n        outputs = model.generate(test_input, max_new_tokens=50, temperature=0.7)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    print(f\"Test response: {response}\")\n    \nexcept Exception as e:\n    print(f\"✗ Error loading Llama: {e}\")\n    print(\"Note: Make sure HUGGINGFACE_TOKEN is added to Kaggle Secrets\")\n    print(\"      and you have access to Llama-3.1-8B-Instruct model\")\n    model = None\n    tokenizer = None\n\n# Load Sentence Transformer for embeddings\nprint(\"\\nLoading Sentence Transformer...\")\ntry:\n    embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n    print(\"✓ Sentence Transformer loaded\")\n\n    # Test embedding\n    test_embedding = embedder.encode([\"test sentence\"])\n    print(f\"Embedding dimension: {test_embedding.shape[1]}\")\n\nexcept Exception as e:\n    print(f\"✗ Error loading embedder: {e}\")\n    embedder = None",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-27T19:21:36.876887Z",
     "iopub.execute_input": "2025-08-27T19:21:36.877861Z",
     "iopub.status.idle": "2025-08-27T19:21:47.286572Z",
     "shell.execute_reply.started": "2025-08-27T19:21:36.877831Z",
     "shell.execute_reply": "2025-08-27T19:21:47.285621Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Initialize the PatentLens Pipeline\n\"\"\"\nThis cell initializes the pipeline with the loaded models and database connection.\nThe pipeline needs to be created AFTER the models are loaded but BEFORE using it in \ncells 5-8.\n\"\"\"\n\n# Check if all required components are loaded\ncomponents_status = {\n  'Database': NEON_CONFIG is not None,\n  'LLM Model': 'model' in locals() and model is not None,\n  'Tokenizer': 'tokenizer' in locals() and tokenizer is not None,\n  'Embedder': 'embedder' in locals() and embedder is not None,\n  'Pipeline Class': 'PatentLensPipeline' in locals()\n}\n\nprint(\"Component Status:\")\nfor component, status in components_status.items():\n    print(f\"  {component}: {'✓ Ready' if status else '✗ Not loaded'}\")\n\n# Initialize pipeline if all components are ready\nif all([components_status['Database'], components_status['Pipeline Class']]):\n    try:\n        pipeline = PatentLensPipeline(\n          db_config=NEON_CONFIG,\n          llm_model=model if components_status['LLM Model'] else None,\n          tokenizer=tokenizer if components_status['Tokenizer'] else None,\n          embedder=embedder if components_status['Embedder'] else None\n        )\n        print(\"\\n✅ Pipeline initialized successfully!\")\n    \n        # Show pipeline configuration\n        version_info = pipeline.get_version_info()\n        print(f\"Pipeline version: {version_info.get('version', 'Unknown')}\")\n        print(f\"LLM loaded: {version_info.get('llm_loaded', 'No')}\")\n        print(f\"Embedder loaded: {version_info.get('embedder_loaded', 'No')}\")\n\n    except Exception as e:\n        print(f\"\\n✗ Error initializing pipeline: {e}\")\n        pipeline = None\nelse:\n    print(\"\\n⚠️ Cannot initialize pipeline - missing required components\")\n    pipeline = None",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-27T19:13:28.084498Z",
     "iopub.execute_input": "2025-08-27T19:13:28.085693Z",
     "iopub.status.idle": "2025-08-27T19:13:30.513541Z",
     "shell.execute_reply.started": "2025-08-27T19:13:28.085651Z",
     "shell.execute_reply": "2025-08-27T19:13:30.512199Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "\"\"\"Test Single Patent \"\"\"\n\n# Test processing a single patent\nif pipeline:\n    print(\"Fetching one patent for testing...\")\n    \n    # Get one patent - CORRECT METHOD NAME\n    test_patents = pipeline.get_patents_to_process(limit=1)\n    \n    if test_patents:\n        test_patent = test_patents[0]\n        \n        print(f\"\\nTesting with patent: {test_patent.patent_number}\")\n        print(f\"Company: {test_patent.company_domain}\")\n        print(f\"Abstract length: {len(test_patent.abstract)} chars\")\n        print(f\"Background length: {len(test_patent.background_text)} chars\")\n        print(f\"Description length: {len(test_patent.description_text)} chars\")\n        print(f\"CPC codes: {', '.join(test_patent.cpc_codes[:3])}...\")\n        \n        print(\"\\nProcessing patent through pipeline...\")\n        result = pipeline.process_patent(test_patent)\n        \n        if result:\n            print(\"\\n✓ Patent processed successfully!\")\n            print(\"\\nExtracted Information:\")\n            print(f\"Field: {result['field_description'][:150]}...\")\n            print(f\"Field keywords: {len(result.get('field_keywords', []))} keyword IDs\")\n            \n            print(f\"\\nTechnical Problem: {result['technical_problem'][:150]}...\")\n            print(f\"Clinical Problem: {result['clinical_problem'][:150]}...\")\n            \n            print(f\"\\nSolution: {result['solution_approach'][:150]}...\")\n            \n            print(f\"\\nCitations:\")\n            print(f\"  Patents cited: {len(result.get('cited_patents', []))}\")\n            if result.get('cited_patents'):\n                print(f\"    Examples: {', '.join(result['cited_patents'][:3])}\")\n            print(f\"  Papers cited: {len(result.get('cited_papers', []))}\")\n            if result.get('cited_papers'):\n                print(f\"    Examples: {', '.join(result['cited_papers'][:3])}\")\n            \n            # Check embeddings\n            has_embeddings = any(\n                result.get(f\"{field}_embedding\") is not None \n                for field in ['field', 'technical_problem', 'clinical_problem', 'solution', 'claims']\n            )\n            print(f\"\\nEmbeddings generated: {has_embeddings}\")\n            \n            # Check keywords in database\n            conn = psycopg2.connect(**NEON_CONFIG)\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT COUNT(*) FROM patents.keywords\")\n            keyword_count = cursor.fetchone()[0]\n            print(f\"\\nTotal keywords in taxonomy: {keyword_count}\")\n            cursor.close()\n            conn.close()\n        else:\n            print(\"✗ Patent processing failed - check logs for error details\")\n    else:\n        print(\"No patents available for testing\")\nelse:\n    print(\"Pipeline not initialized\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-27T19:13:39.718298Z",
     "iopub.execute_input": "2025-08-27T19:13:39.718741Z",
     "iopub.status.idle": "2025-08-27T19:13:46.806210Z",
     "shell.execute_reply.started": "2025-08-27T19:13:39.718713Z",
     "shell.execute_reply": "2025-08-27T19:13:46.804525Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37eed9e129e0443997ea13522b9a7605"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "140ceb1d59a9485c9511eced16402a9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c19355633d624c87822a1d992bcccffd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2d7b788aa7354369b381b0497bc002db"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a2af4cf950e422db320233ca39452ca"
      }
     },
     "metadata": {}
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/1978483453.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mNEON_CONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SELECT COUNT(*) FROM patents.keywords\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mkeyword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTotal keywords in taxonomy: {keyword_count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUndefinedTable\u001b[0m: relation \"patents.keywords\" does not exist\nLINE 1: SELECT COUNT(*) FROM patents.keywords\n                             ^\n"
     ],
     "ename": "UndefinedTable",
     "evalue": "relation \"patents.keywords\" does not exist\nLINE 1: SELECT COUNT(*) FROM patents.keywords\n                             ^\n",
     "output_type": "error"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "# Process multiple patents\nif pipeline:\n    print(\"Starting batch processing...\")\n    \n    # Get processing statistics before\n    stats_before = pipeline.get_processing_stats()\n    pending_count = stats_before.get('pending', 0)\n    \n    print(f\"Patents pending: {pending_count}\")\n    \n    if pending_count > 0:\n        # Process a batch\n        batch_size = min(10, pending_count)  # Process up to 10 patents\n        print(f\"\\\\nProcessing {batch_size} patents...\")\n        \n        results = []\n        patents = pipeline.fetch_patents_batch(limit=batch_size)\n        \n        # Process with progress bar\n        for patent in tqdm(patents, desc=\"Processing patents\"):\n            result = pipeline.process_patent(patent)\n            results.append({\n                'patent_number': patent.patent_number,\n                'success': result is not None,\n                'field': result['field_description'][:50] if result else None\n            })\n            \n            # Clear GPU cache periodically\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        \n        # Show results\n        df_results = pd.DataFrame(results)\n        print(f\"\\\\nProcessing complete!\")\n        print(f\"Success rate: {df_results['success'].mean():.1%}\")\n        \n        # Get updated statistics\n        stats_after = pipeline.get_processing_stats()\n        print(\"\\\\nUpdated statistics:\")\n        for status, count in stats_after.items():\n            before = stats_before.get(status, 0)\n            change = count - before\n            if change != 0:\n                print(f\"  {status}: {count} ({change:+d})\")\n            else:\n                print(f\"  {status}: {count}\")\n        \n        # Show sample results\n        print(\"\\\\nSample results:\")\n        print(df_results.head())\n    else:\n        print(\"No pending patents to process\")\nelse:\n    print(\"Pipeline not initialized\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-25T16:23:35.55735Z",
     "iopub.execute_input": "2025-08-25T16:23:35.557699Z",
     "iopub.status.idle": "2025-08-25T16:23:35.566632Z",
     "shell.execute_reply.started": "2025-08-25T16:23:35.557671Z",
     "shell.execute_reply": "2025-08-25T16:23:35.565734Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Analyze extraction results and keyword taxonomy\nconn = psycopg2.connect(**NEON_CONFIG)\n\n# 1. Keyword Taxonomy Analysis\nprint(\"=== KEYWORD TAXONOMY ANALYSIS ===\\\\n\")\n\nquery_keywords = '''\n    SELECT \n        category,\n        COUNT(*) as keyword_count,\n        AVG(use_count) as avg_usage,\n        MAX(use_count) as max_usage\n    FROM patents.keywords\n    GROUP BY category\n    ORDER BY keyword_count DESC\n'''\n\ndf_keywords = pd.read_sql(query_keywords, conn)\nprint(\"Keywords by category:\")\nprint(df_keywords)\n\n# Top keywords by usage\nprint(\"\\\\nTop 10 most used keywords:\")\nquery_top = '''\n    SELECT keyword_text, category, use_count\n    FROM patents.keywords\n    ORDER BY use_count DESC\n    LIMIT 10\n'''\ndf_top_keywords = pd.read_sql(query_top, conn)\nprint(df_top_keywords)\n\n# 2. Extraction Results Analysis\nprint(\"\\\\n=== EXTRACTION RESULTS ===\\\\n\")\n\nquery_extractions = '''\n    SELECT \n        COUNT(*) as total_extractions,\n        COUNT(DISTINCT patent_number) as unique_patents,\n        COUNT(field_description) as has_field,\n        COUNT(technical_problem) as has_technical,\n        COUNT(clinical_problem) as has_clinical,\n        COUNT(solution_approach) as has_solution,\n        AVG(array_length(cited_patents, 1)) as avg_patent_citations,\n        AVG(array_length(cited_papers, 1)) as avg_paper_citations\n    FROM patents.extracted_knowledge\n'''\n\ndf_extraction_stats = pd.read_sql(query_extractions, conn)\nprint(\"Extraction statistics:\")\nfor col in df_extraction_stats.columns:\n    val = df_extraction_stats[col].iloc[0]\n    if pd.notna(val):\n        if 'avg' in col:\n            print(f\"  {col}: {val:.2f}\")\n        else:\n            print(f\"  {col}: {int(val)}\")\n\n# 3. Sample Extractions\nprint(\"\\\\n=== SAMPLE EXTRACTIONS ===\\\\n\")\n\nquery_samples = '''\n    SELECT \n        ek.patent_number,\n        c.name as company,\n        LEFT(ek.field_description, 100) as field_preview,\n        array_length(ek.cited_patents, 1) as patent_citations,\n        array_length(ek.cited_papers, 1) as paper_citations\n    FROM patents.extracted_knowledge ek\n    JOIN patents.patent_full_text p ON ek.patent_id = p.id\n    JOIN core.companies c ON p.company_domain = c.domain\n    LIMIT 5\n'''\n\ndf_samples = pd.read_sql(query_samples, conn)\nprint(\"Sample extracted patents:\")\nprint(df_samples.to_string())\n\n# 4. Processing Status Overview\nprint(\"\\\\n=== PROCESSING STATUS ===\\\\n\")\n\nquery_status = '''\n    SELECT \n        overall_status,\n        COUNT(*) as count,\n        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1) as percentage\n    FROM patents.processing_status\n    GROUP BY overall_status\n    ORDER BY count DESC\n'''\n\ndf_status = pd.read_sql(query_status, conn)\nprint(\"Overall processing status:\")\nprint(df_status.to_string(index=False))\n\n# 5. Citations Analysis\nprint(\"\\\\n=== CITATIONS ANALYSIS ===\\\\n\")\n\nquery_citations = '''\n    SELECT \n        patent_number,\n        cited_patents,\n        cited_papers\n    FROM patents.extracted_knowledge\n    WHERE array_length(cited_patents, 1) > 0 OR array_length(cited_papers, 1) > 0\n    LIMIT 3\n'''\n\ndf_citations = pd.read_sql(query_citations, conn)\nif not df_citations.empty:\n    print(\"Patents with citations:\")\n    for _, row in df_citations.iterrows():\n        print(f\"\\\\nPatent {row['patent_number']}:\")\n        if row['cited_patents']:\n            print(f\"  Cites patents: {', '.join(row['cited_patents'][:3])}...\")\n        if row['cited_papers']:\n            print(f\"  Cites papers: {', '.join(row['cited_papers'][:3])}...\")\nelse:\n    print(\"No citations extracted yet\")\n\nconn.close()\n\n# Visualization (if matplotlib available)\ntry:\n    import matplotlib.pyplot as plt\n    \n    if not df_keywords.empty:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        \n        # Keywords by category\n        ax1.bar(df_keywords['category'], df_keywords['keyword_count'])\n        ax1.set_title('Keywords by Category')\n        ax1.set_xlabel('Category')\n        ax1.set_ylabel('Count')\n        ax1.tick_params(axis='x', rotation=45)\n        \n        # Processing status pie chart\n        if not df_status.empty:\n            ax2.pie(df_status['count'], labels=df_status['overall_status'], autopct='%1.1f%%')\n            ax2.set_title('Processing Status')\n        \n        plt.tight_layout()\n        plt.show()\n        \nexcept ImportError:\n    print(\"\\\\nMatplotlib not available for visualization\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-25T18:24:35.653557Z",
     "iopub.execute_input": "2025-08-25T18:24:35.653884Z",
     "iopub.status.idle": "2025-08-25T18:24:36.959514Z",
     "shell.execute_reply.started": "2025-08-25T18:24:35.653863Z",
     "shell.execute_reply": "2025-08-25T18:24:36.958598Z"
    },
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "collapsed": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Export results to CSV for download\nconn = psycopg2.connect(**NEON_CONFIG)\n\n# Export extracted knowledge with company info\nquery_export = '''\n    SELECT \n        ek.patent_number,\n        c.name as company_name,\n        c.domain as company_domain,\n        ek.field_description,\n        ek.technical_problem,\n        ek.clinical_problem,\n        ek.solution_approach,\n        array_to_string(ek.cited_patents, '; ') as cited_patents,\n        array_to_string(ek.cited_papers, '; ') as cited_papers,\n        ek.extraction_date\n    FROM patents.extracted_knowledge ek\n    JOIN patents.patent_full_text p ON ek.patent_id = p.id\n    JOIN core.companies c ON p.company_domain = c.domain\n    ORDER BY ek.extraction_date DESC\n'''\n\ndf_export = pd.read_sql(query_export, conn)\n\n# Save to CSV\noutput_file = '/kaggle/working/patentlens_extractions.csv'\ndf_export.to_csv(output_file, index=False)\nprint(f\"✓ Exported {len(df_export)} patent extractions to {output_file}\")\n\n# Export keyword taxonomy\nquery_keywords_export = '''\n    SELECT \n        keyword_text,\n        category,\n        use_count\n    FROM patents.keywords\n    ORDER BY use_count DESC, category, keyword_text\n'''\n\ndf_keywords_export = pd.read_sql(query_keywords_export, conn)\nkeywords_file = '/kaggle/working/patentlens_keywords.csv'\ndf_keywords_export.to_csv(keywords_file, index=False)\nprint(f\"✓ Exported {len(df_keywords_export)} keywords to {keywords_file}\")\n\nconn.close()\n\nprint(\"\\\\nFiles ready for download from Kaggle output\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-08-25T16:23:37.034727Z",
     "iopub.execute_input": "2025-08-25T16:23:37.03501Z",
     "iopub.status.idle": "2025-08-25T16:23:37.347502Z",
     "shell.execute_reply.started": "2025-08-25T16:23:37.034973Z",
     "shell.execute_reply": "2025-08-25T16:23:37.346825Z"
    },
    "jupyter": {
     "source_hidden": true,
     "outputs_hidden": true
    },
    "collapsed": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}