{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC Entity & Relationship Extraction Pipeline - Clean Architecture\n",
    "## Streamlined implementation with zero technical debt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 1: Configuration Management\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional, Any\nfrom enum import Enum\nimport os\nfrom datetime import datetime\n\nclass RelationshipType(Enum):\n    \"\"\"Standard biotech relationship types\"\"\"\n    COMPANY_ENTITY = \"COMPANY_ENTITY\"\n    PARTNERSHIP = \"PARTNERSHIP\"\n    REGULATORY = \"REGULATORY\"\n    CLINICAL_TRIAL = \"CLINICAL_TRIAL\"\n    FINANCIAL = \"FINANCIAL\"\n    LICENSING = \"LICENSING\"\n    COMPETITIVE = \"COMPETITIVE\"\n    SUPPLY_CHAIN = \"SUPPLY_CHAIN\"\n    RESEARCH = \"RESEARCH\"\n    ACQUISITION = \"ACQUISITION\"\n\n@dataclass\nclass DatabaseConfig:\n    host: str = 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech'\n    database: str = 'BizIntelSmartReach'\n    user: str = 'neondb_owner'\n    password: str = 'npg_aTFt6Pug3Kpy'\n    sslmode: str = 'require'\n    pool_min_connections: int = 2\n    pool_max_connections: int = 10\n\n@dataclass\nclass ModelConfig:\n    biobert_model: str = 'alvaroalon2/biobert_diseases_ner'\n    bert_model: str = 'dslim/bert-base-NER'\n    finbert_model: str = 'ProsusAI/finbert'\n    roberta_model: str = 'Jean-Baptiste/roberta-large-ner-english'\n    confidence_threshold: float = 0.5\n    device: str = 'auto'  # 'auto', 'cpu', or 'cuda'\n    \n    # Essential filtering configuration\n    biobert_skip_categories: List[str] = field(default_factory=lambda: ['0'])\n    finbert_common_words: List[str] = field(default_factory=lambda: ['the', 'and', 'or', 'but', 'company', 'inc'])\n    bert_skip_misc: bool = True\n\n@dataclass\nclass LlamaConfig:\n    model_id: str = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\n    max_new_tokens: int = 500\n    temperature: float = 0.7\n    timeout_seconds: int = 30\n    batch_size: int = 10\n    prompt_version: str = '1.0'\n\n@dataclass\nclass SemanticConfig:\n    max_summary_length: int = 200\n    context_window_chars: int = 500\n    confidence_threshold: float = 0.5\n    cache_ttl_seconds: int = 900\n\n@dataclass\nclass ProcessingConfig:\n    batch_size: int = 100\n    max_retries: int = 3\n    retry_delay_seconds: int = 1\n    parallel_workers: int = 4\n    filing_batch_size: int = 5\n\n@dataclass\nclass PipelineConfig:\n    \"\"\"Master configuration for entire pipeline\"\"\"\n    database: DatabaseConfig = field(default_factory=DatabaseConfig)\n    models: ModelConfig = field(default_factory=ModelConfig)\n    llama: LlamaConfig = field(default_factory=LlamaConfig)\n    semantic: SemanticConfig = field(default_factory=SemanticConfig)\n    processing: ProcessingConfig = field(default_factory=ProcessingConfig)\n    \n    # Feature flags\n    enable_llama: bool = True\n    enable_validation: bool = True\n    enable_caching: bool = True\n    \n    # Paths\n    edgar_identity: str = \"SmartReach BizIntel amir@leanbio.consulting\"\n    \n    @classmethod\n    def from_env(cls):\n        \"\"\"Load configuration from environment variables\"\"\"\n        config = cls()\n        \n        # Override from environment if available\n        if os.getenv('LLAMA_ENABLED'):\n            config.enable_llama = os.getenv('LLAMA_ENABLED').lower() == 'true'\n        if os.getenv('DB_HOST'):\n            config.database.host = os.getenv('DB_HOST')\n        if os.getenv('LLAMA_MODEL'):\n            config.llama.model_id = os.getenv('LLAMA_MODEL')\n            \n        return config\n\n# Initialize configuration\nCONFIG = PipelineConfig.from_env()\n\nprint(\"✅ Configuration loaded\")\nprint(f\"   Database: {CONFIG.database.host}\")\nprint(f\"   Llama: {'Enabled' if CONFIG.enable_llama else 'Disabled'}\")\nprint(f\"   Batch size: {CONFIG.processing.batch_size}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 2: Database Connection Manager\n",
    "\n",
    "import psycopg2\n",
    "from psycopg2 import pool\n",
    "from psycopg2.extras import execute_values, RealDictCursor\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "from typing import Generator, List, Dict, Any, Optional\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DatabaseManager:\n",
    "    \"\"\"Centralized database connection management with pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: DatabaseConfig):\n",
    "        self.config = config\n",
    "        self._pool = None\n",
    "        self._initialize_pool()\n",
    "    \n",
    "    def _initialize_pool(self):\n",
    "        \"\"\"Initialize connection pool\"\"\"\n",
    "        try:\n",
    "            self._pool = psycopg2.pool.ThreadedConnectionPool(\n",
    "                minconn=self.config.pool_min_connections,\n",
    "                maxconn=self.config.pool_max_connections,\n",
    "                host=self.config.host,\n",
    "                database=self.config.database,\n",
    "                user=self.config.user,\n",
    "                password=self.config.password,\n",
    "                sslmode=self.config.sslmode\n",
    "            )\n",
    "            logger.info(f\"Database pool initialized with {self.config.pool_max_connections} connections\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize database pool: {e}\")\n",
    "            raise\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_connection(self) -> Generator:\n",
    "        \"\"\"Get connection from pool with automatic cleanup\"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = self._pool.getconn()\n",
    "            yield conn\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            if conn:\n",
    "                conn.rollback()\n",
    "            raise e\n",
    "        finally:\n",
    "            if conn:\n",
    "                self._pool.putconn(conn)\n",
    "    \n",
    "    @contextmanager\n",
    "    def get_cursor(self, dict_cursor=False) -> Generator:\n",
    "        \"\"\"Get cursor with automatic cleanup\"\"\"\n",
    "        with self.get_connection() as conn:\n",
    "            cursor_factory = RealDictCursor if dict_cursor else None\n",
    "            cursor = conn.cursor(cursor_factory=cursor_factory)\n",
    "            try:\n",
    "                yield cursor\n",
    "            finally:\n",
    "                cursor.close()\n",
    "    \n",
    "    def batch_insert(self, table: str, columns: List[str], data: List[tuple],\n",
    "                    on_conflict: Optional[str] = None) -> int:\n",
    "        \"\"\"Efficient batch insert with optional conflict handling\"\"\"\n",
    "        if not data:\n",
    "            return 0\n",
    "        \n",
    "        with self.get_cursor() as cursor:\n",
    "            columns_str = ', '.join(columns)\n",
    "            placeholders = ', '.join(['%s'] * len(columns))\n",
    "            \n",
    "            query = f\"INSERT INTO {table} ({columns_str}) VALUES %s\"\n",
    "            if on_conflict:\n",
    "                query += f\" {on_conflict}\"\n",
    "            \n",
    "            execute_values(cursor, query, data, page_size=self.config.pool_max_connections)\n",
    "            return cursor.rowcount\n",
    "    \n",
    "    def execute_query(self, query: str, params: tuple = None) -> List[Dict]:\n",
    "        \"\"\"Execute query and return results as list of dicts\"\"\"\n",
    "        with self.get_cursor(dict_cursor=True) as cursor:\n",
    "            cursor.execute(query, params)\n",
    "            return cursor.fetchall()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close all connections in pool\"\"\"\n",
    "        if self._pool:\n",
    "            self._pool.closeall()\n",
    "            logger.info(\"Database pool closed\")\n",
    "\n",
    "# Initialize database manager\n",
    "db_manager = DatabaseManager(CONFIG.database)\n",
    "print(\"✅ Database manager initialized with connection pooling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": "# Cell 3: Entity Extraction Module - Optimized to Use Cell 2's Output\n\nset_identity(CONFIG.edgar_identity)\n\nclass EntityExtractionPipeline:\n    \"\"\"Extract entities from Cell 2's pre-processed sections\"\"\"\n    \n    def __init__(self, config: PipelineConfig, db_manager: DatabaseManager):\n        self.config = config\n        self.db = db_manager\n        self.models = {}\n        \n        # Essential filtering from CONFIG\n        self._biobert_skip = set(config.models.biobert_skip_categories)\n        self._finbert_common = set(word.lower() for word in config.models.finbert_common_words)\n        self._bert_skip_misc = config.models.bert_skip_misc\n        \n        # Map Cell 2's routing names to our model names\n        self._routing_to_model_map = {\n            'biobert': 'biobert',\n            'bert_base': 'bert',  # Cell 2 uses 'bert_base', we use 'bert'\n            'roberta': 'roberta', \n            'finbert': 'finbert'\n        }\n        \n        # Load models during initialization\n        self._load_models()\n    \n    def _load_models(self):\n        \"\"\"Load NER models\"\"\"\n        model_configs = [\n            ('biobert', self.config.models.biobert_model),\n            ('bert', self.config.models.bert_model),\n            ('finbert', self.config.models.finbert_model),\n            ('roberta', self.config.models.roberta_model)\n        ]\n        \n        device = 0 if torch.cuda.is_available() and self.config.models.device != 'cpu' else -1\n        \n        for name, model_id in model_configs:\n            try:\n                self.models[name] = pipeline(\n                    \"ner\",\n                    model=model_id,\n                    aggregation_strategy=\"average\",\n                    device=device\n                )\n                logger.info(f\"Loaded {name} model\")\n            except Exception as e:\n                logger.error(f\"Failed to load {name}: {e}\")\n    \n    def _normalize_entity_type(self, entity_type: str) -> str:\n        \"\"\"Normalize entity types across models\"\"\"\n        mappings = {\n            'Disease': 'MEDICAL_CONDITION',\n            'Chemical': 'MEDICATION',\n            'Drug': 'MEDICATION',\n            'PER': 'PERSON',\n            'ORG': 'ORGANIZATION',\n            'LOC': 'LOCATION',\n            'MONEY': 'FINANCIAL',\n            'PERCENT': 'FINANCIAL'\n        }\n        return mappings.get(entity_type, entity_type.upper())\n    \n    def _passes_essential_filters(self, model_name: str, entity_text: str, entity_category: str) -> bool:\n        \"\"\"Essential filtering logic per model\"\"\"\n        entity_lower = entity_text.lower()\n        \n        if model_name == 'biobert':\n            return entity_category not in self._biobert_skip\n        elif model_name == 'finbert':\n            return entity_lower not in self._finbert_common\n        elif model_name == 'bert':\n            return not (self._bert_skip_misc and entity_category == 'MISC')\n        \n        return True\n    \n    def _extract_from_single_section(self, section_text: str, model_name: str, \n                                   section_name: str, section_result: Dict) -> List[Dict]:\n        \"\"\"Extract entities from single section\"\"\"\n        try:\n            raw_entities = self.models[model_name](section_text)\n            \n            filtered_entities = []\n            for entity in raw_entities:\n                # Apply confidence threshold\n                if entity['score'] < self.config.models.confidence_threshold:\n                    continue\n                \n                entity_text = entity['word'].strip()\n                entity_category = entity['entity_group']\n                \n                # Apply essential filtering\n                if not self._passes_essential_filters(model_name, entity_text, entity_category):\n                    continue\n                \n                filtered_entities.append({\n                    'extraction_id': str(uuid.uuid4()),\n                    'company_domain': section_result['company_domain'],\n                    'entity_text': entity_text,\n                    'entity_category': self._normalize_entity_type(entity_category),\n                    'confidence_score': float(entity['score']),\n                    'character_start': entity['start'],\n                    'character_end': entity['end'],\n                    'section_name': section_name,\n                    'sec_filing_ref': f\"SEC_{section_result['filing_id']}\",\n                    'primary_model': model_name,\n                    'filing_type': section_result['filing_type'],\n                    'filing_date': section_result.get('filing_date'),\n                    'accession_number': section_result['accession_number']\n                })\n            \n            return filtered_entities\n            \n        except Exception as e:\n            logger.error(f\"Entity extraction failed with {model_name} on {section_name}: {e}\")\n            return []\n    \n    def _merge_overlapping_entities(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Simplified consensus merging\"\"\"\n        if not entities:\n            return []\n        \n        # Group by position within same section and filing\n        position_groups = {}\n        for entity in entities:\n            key = (entity['sec_filing_ref'], entity['section_name'], \n                  entity['character_start'], entity['character_end'])\n            position_groups.setdefault(key, []).append(entity)\n        \n        # Take highest confidence from each group\n        merged = []\n        for group in position_groups.values():\n            best_entity = max(group, key=lambda x: x['confidence_score'])\n            merged.append(best_entity)\n        \n        return merged\n    \n    def _extract_entities_from_sections(self, section_result: Dict) -> List[Dict]:\n        \"\"\"Extract entities using Cell 2's sections and routing\"\"\"\n        sections = section_result['sections']\n        model_routing = section_result['model_routing']\n        \n        all_entities = []\n        \n        # Process each model's assigned sections (Cell 2's routing)\n        for routing_model_name, assigned_section_names in model_routing.items():\n            \n            # Map Cell 2's model name to our model name\n            our_model_name = self._routing_to_model_map.get(routing_model_name)\n            \n            if not our_model_name or our_model_name not in self.models:\n                logger.warning(f\"Model '{routing_model_name}' -> '{our_model_name}' not available\")\n                continue\n            \n            logger.info(f\"Processing {len(assigned_section_names)} sections with {our_model_name}\")\n            \n            # Extract entities from each assigned section\n            for section_name in assigned_section_names:\n                section_text = sections.get(section_name)\n                if not section_text:\n                    logger.warning(f\"Section '{section_name}' has no text\")\n                    continue\n                \n                section_entities = self._extract_from_single_section(\n                    section_text, our_model_name, section_name, section_result\n                )\n                all_entities.extend(section_entities)\n        \n        # Merge overlapping entities\n        merged_entities = self._merge_overlapping_entities(all_entities)\n        \n        return merged_entities\n    \n    def process_sec_filing_entities(self, filing_data: Dict) -> List[Dict]:\n        \"\"\"Main function: Extract entities from a filing using Cell 2's section extraction\"\"\"\n        \n        # Step 1: Use Cell 2's section extraction\n        section_result = process_sec_filing_with_sections(filing_data)\n        \n        if section_result['processing_status'] != 'success':\n            logger.warning(f\"Cell 2 section extraction failed: {section_result.get('error', 'Unknown')}\")\n            return []\n        \n        # Step 2: Extract entities using Cell 2's output\n        entities = self._extract_entities_from_sections(section_result)\n        \n        logger.info(f\"Extracted {len(entities)} entities from {section_result['total_sections']} sections\")\n        \n        return entities\n    \n    def store_entities_in_database(self, entities: List[Dict]) -> int:\n        \"\"\"Store entities in database\"\"\"\n        if not entities:\n            return 0\n        \n        columns = [\n            'extraction_id', 'company_domain', 'entity_text', 'entity_category',\n            'confidence_score', 'character_start', 'character_end', 'section_name',\n            'sec_filing_ref', 'primary_model', 'data_source', 'extraction_timestamp',\n            'filing_type', 'accession_number'\n        ]\n        \n        data = []\n        for e in entities:\n            data.append((\n                e['extraction_id'], e['company_domain'], e['entity_text'][:1000],\n                e['entity_category'], e['confidence_score'], e['character_start'],\n                e['character_end'], e['section_name'], e['sec_filing_ref'],\n                e['primary_model'], 'sec_filings', datetime.now(),\n                e['filing_type'], e['accession_number']\n            ))\n        \n        on_conflict = \"ON CONFLICT (extraction_id) DO NOTHING\"\n        count = self.db.batch_insert('system_uno.sec_entities_raw', columns, data, on_conflict)\n        logger.info(f\"Stored {count} entities\")\n        return count\n\n# Initialize entity pipeline\nentity_pipeline = EntityExtractionPipeline(CONFIG, db_manager)\n\ndef process_filing_with_entity_extraction(filing_data: Dict) -> Dict:\n    \"\"\"Complete pipeline using Cell 2's section extraction + Cell 3's entity extraction\"\"\"\n    \n    # Extract entities (this calls Cell 2's process_sec_filing_with_sections internally)\n    entities = entity_pipeline.process_sec_filing_entities(filing_data)\n    \n    # Store entities\n    stored_count = 0\n    if entities:\n        stored_count = entity_pipeline.store_entities_in_database(entities)\n    \n    return {\n        'filing_id': filing_data.get('id'),\n        'company_domain': filing_data.get('company_domain'),\n        'filing_type': filing_data.get('filing_type'),\n        'entities_extracted': len(entities),\n        'entities_stored': stored_count,\n        'success': len(entities) > 0\n    }\n\ndef process_multiple_filings_with_entities(limit: int = 5) -> List[Dict]:\n    \"\"\"Process multiple filings through complete Cell 2 + Cell 3 pipeline\"\"\"\n    \n    # Use Cell 2's function to get unprocessed filings\n    filings = get_unprocessed_filings(limit)\n    \n    results = []\n    for filing in filings:\n        result = process_filing_with_entity_extraction(filing)\n        results.append(result)\n        \n        if result['success']:\n            logger.info(f\"✅ {result['company_domain']}: {result['entities_extracted']} entities\")\n        else:\n            logger.warning(f\"❌ {result['company_domain']}: No entities extracted\")\n    \n    return results\n\nlogger.info(f\"✅ Entity extraction pipeline initialized with {len(entity_pipeline.models)} models\")\nprint(f\"✅ Cell 3 complete - Entity pipeline ready to process Cell 2's sections\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 4: Relationship Analysis Module\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class RelationshipAnalyzer:\n",
    "    \"\"\"Clean relationship analysis with batch Llama processing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig, db_manager: DatabaseManager):\n",
    "        self.config = config\n",
    "        self.db = db_manager\n",
    "        self.client = None\n",
    "        \n",
    "        if config.enable_llama:\n",
    "            # Get HuggingFace token from Kaggle secrets\n",
    "            try:\n",
    "                from kaggle_secrets import UserSecretsClient\n",
    "                user_secrets = UserSecretsClient()\n",
    "                hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "                \n",
    "                self.client = InferenceClient(\n",
    "                    model=config.llama.model_id,\n",
    "                    token=hf_token\n",
    "                )\n",
    "                logger.info(f\"Llama client initialized with {config.llama.model_id}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize Llama client: {e}\")\n",
    "                self.config.enable_llama = False\n",
    "    \n",
    "    def analyze_entity_batch(self, entities: List[Dict],\n",
    "                            entity_extractor: EntityExtractor) -> List[Dict]:\n",
    "        \"\"\"Analyze batch of entities for relationships\"\"\"\n",
    "        if not self.config.enable_llama or not entities:\n",
    "            return []\n",
    "        \n",
    "        relationships = []\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = self.config.llama.batch_size\n",
    "        for i in range(0, len(entities), batch_size):\n",
    "            batch = entities[i:i + batch_size]\n",
    "            \n",
    "            # Analyze batch in parallel\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                futures = []\n",
    "                for entity in batch:\n",
    "                    # Get context\n",
    "                    context = self._get_entity_context(entity, entity_extractor)\n",
    "                    if context:\n",
    "                        future = executor.submit(self._analyze_single_entity, entity, context)\n",
    "                        futures.append(future)\n",
    "                \n",
    "                for future in futures:\n",
    "                    try:\n",
    "                        result = future.result(timeout=self.config.llama.timeout_seconds)\n",
    "                        if result:\n",
    "                            relationships.append(result)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Relationship analysis failed: {e}\")\n",
    "        \n",
    "        return relationships\n",
    "    \n",
    "    def _get_entity_context(self, entity: Dict, entity_extractor: EntityExtractor) -> Optional[str]:\n",
    "        \"\"\"Get context around entity from cached sections\"\"\"\n",
    "        section_content = entity_extractor.get_section_content(\n",
    "            entity['sec_filing_ref'],\n",
    "            entity['section_name']\n",
    "        )\n",
    "        \n",
    "        if not section_content:\n",
    "            return None\n",
    "        \n",
    "        # Extract context window\n",
    "        start = max(0, entity['character_start'] - self.config.semantic.context_window_chars)\n",
    "        end = min(len(section_content), entity['character_end'] + self.config.semantic.context_window_chars)\n",
    "        \n",
    "        return section_content[start:end]\n",
    "    \n",
    "    def _analyze_single_entity(self, entity: Dict, context: str) -> Optional[Dict]:\n",
    "        \"\"\"Analyze single entity with Llama\"\"\"\n",
    "        try:\n",
    "            prompt = self._build_prompt(entity, context)\n",
    "            \n",
    "            response = self.client.text_generation(\n",
    "                prompt,\n",
    "                max_new_tokens=self.config.llama.max_new_tokens,\n",
    "                temperature=self.config.llama.temperature,\n",
    "                return_full_text=False\n",
    "            )\n",
    "            \n",
    "            # Parse response\n",
    "            analysis = self._parse_response(response)\n",
    "            \n",
    "            if analysis:\n",
    "                return {\n",
    "                    'entity': entity,\n",
    "                    'analysis': analysis,\n",
    "                    'context': context\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to analyze entity {entity.get('entity_text')}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _build_prompt(self, entity: Dict, context: str) -> str:\n",
    "        \"\"\"Build streamlined prompt for Llama\"\"\"\n",
    "        return f\"\"\"Analyze this entity from an SEC filing for business relationships.\n",
    "\n",
    "Company: {entity['company_domain']}\n",
    "Entity: {entity['entity_text']}\n",
    "Section: {entity['section_name']}\n",
    "Context: {context}\n",
    "\n",
    "Return JSON with:\n",
    "- relationship_type: PARTNERSHIP|REGULATORY|CLINICAL_TRIAL|FINANCIAL|LICENSING|COMPETITIVE|RESEARCH|NONE\n",
    "- semantic_summary: max 200 char description\n",
    "- semantic_action: verb (initiated|expanded|terminated|announced)\n",
    "- confidence: 0.0-1.0\n",
    "- monetary_value: number or null\n",
    "- temporal_info: when mentioned to occur\n",
    "- tags: relevant keywords\n",
    "\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    def _parse_response(self, response: str) -> Optional[Dict]:\n",
    "        \"\"\"Parse Llama response to structured data\"\"\"\n",
    "        try:\n",
    "            # Extract JSON\n",
    "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "            if json_match:\n",
    "                data = json.loads(json_match.group())\n",
    "                \n",
    "                # Validate and normalize\n",
    "                return {\n",
    "                    'relationship_type': data.get('relationship_type', 'COMPANY_ENTITY'),\n",
    "                    'semantic_summary': data.get('semantic_summary', '')[:200],\n",
    "                    'semantic_action': data.get('semantic_action', 'mentioned'),\n",
    "                    'confidence_score': float(data.get('confidence', 0.5)),\n",
    "                    'monetary_value': data.get('monetary_value'),\n",
    "                    'temporal_info': data.get('temporal_info', ''),\n",
    "                    'semantic_tags': data.get('tags', [])\n",
    "                }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to parse response: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def store_relationships_batch(self, relationships: List[Dict], session_id: str) -> int:\n",
    "        \"\"\"Store all relationships in single transaction\"\"\"\n",
    "        if not relationships:\n",
    "            return 0\n",
    "        \n",
    "        with self.db.get_connection() as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            try:\n",
    "                events_stored = 0\n",
    "                buckets_created = 0\n",
    "                \n",
    "                for rel in relationships:\n",
    "                    entity = rel['entity']\n",
    "                    analysis = rel['analysis']\n",
    "                    \n",
    "                    # Find or create bucket\n",
    "                    bucket_id = self._find_or_create_bucket(\n",
    "                        cursor,\n",
    "                        entity['company_domain'],\n",
    "                        entity['entity_text'],\n",
    "                        analysis['relationship_type']\n",
    "                    )\n",
    "                    \n",
    "                    if bucket_id:\n",
    "                        buckets_created += 1\n",
    "                    \n",
    "                    # Store semantic event\n",
    "                    event_id = str(uuid.uuid4())\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO system_uno.relationship_semantic_events\n",
    "                        (event_id, bucket_id, source_entity_id, sec_filing_ref,\n",
    "                         filing_date, filing_type, section_name,\n",
    "                         semantic_summary, semantic_action, confidence_score,\n",
    "                         monetary_value, semantic_tags, original_context_snippet,\n",
    "                         llama_prompt_version)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\", (\n",
    "                        event_id, bucket_id, entity['extraction_id'],\n",
    "                        entity['sec_filing_ref'], entity.get('filing_date'),\n",
    "                        entity['filing_type'], entity['section_name'],\n",
    "                        analysis['semantic_summary'], analysis['semantic_action'],\n",
    "                        analysis['confidence_score'], analysis.get('monetary_value'),\n",
    "                        analysis.get('semantic_tags', []), rel['context'][:500],\n",
    "                        self.config.llama.prompt_version\n",
    "                    ))\n",
    "                    \n",
    "                    events_stored += 1\n",
    "                    \n",
    "                    # Create validation placeholder\n",
    "                    if self.config.enable_validation:\n",
    "                        cursor.execute(\"\"\"\n",
    "                            INSERT INTO system_uno.relationship_validation\n",
    "                            (event_id, bucket_id, validation_method, llama_prompt_version_tested)\n",
    "                            VALUES (%s, %s, 'PENDING_REVIEW', %s)\n",
    "                        \"\"\", (event_id, bucket_id, self.config.llama.prompt_version))\n",
    "                \n",
    "                # Update session\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE system_uno.semantic_analysis_sessions\n",
    "                    SET events_created = events_created + %s,\n",
    "                        buckets_updated = buckets_updated + %s\n",
    "                    WHERE session_id = %s\n",
    "                \"\"\", (events_stored, buckets_created, session_id))\n",
    "                \n",
    "                conn.commit()\n",
    "                logger.info(f\"Stored {events_stored} relationships\")\n",
    "                return events_stored\n",
    "                \n",
    "            except Exception as e:\n",
    "                conn.rollback()\n",
    "                logger.error(f\"Failed to store relationships: {e}\")\n",
    "                return 0\n",
    "    \n",
    "    def _find_or_create_bucket(self, cursor, company: str, entity: str, rel_type: str) -> str:\n",
    "        \"\"\"Find or create relationship bucket\"\"\"\n",
    "        # Check existing\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT bucket_id FROM system_uno.relationship_buckets\n",
    "            WHERE company_domain = %s AND entity_name = %s AND relationship_type = %s\n",
    "        \"\"\", (company, entity, rel_type))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result:\n",
    "            return result[0]\n",
    "        \n",
    "        # Create new\n",
    "        bucket_id = str(uuid.uuid4())\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO system_uno.relationship_buckets\n",
    "            (bucket_id, company_domain, entity_name, relationship_type,\n",
    "             first_mentioned_date, is_active)\n",
    "            VALUES (%s, %s, %s, %s, CURRENT_DATE, true)\n",
    "            RETURNING bucket_id\n",
    "        \"\"\", (bucket_id, company, entity, rel_type))\n",
    "        \n",
    "        return cursor.fetchone()[0]\n",
    "\n",
    "# Initialize relationship analyzer\n",
    "relationship_analyzer = RelationshipAnalyzer(CONFIG, db_manager)\n",
    "print(f\"✅ Relationship analyzer initialized (Llama: {'Enabled' if CONFIG.enable_llama else 'Disabled'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 5: Pipeline Orchestrator\n",
    "\n",
    "class PipelineOrchestrator:\n",
    "    \"\"\"Main orchestrator for complete pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: PipelineConfig, db_manager: DatabaseManager,\n",
    "                 entity_extractor: EntityExtractor,\n",
    "                 relationship_analyzer: RelationshipAnalyzer):\n",
    "        self.config = config\n",
    "        self.db = db_manager\n",
    "        self.entity_extractor = entity_extractor\n",
    "        self.relationship_analyzer = relationship_analyzer\n",
    "    \n",
    "    def process_filing_batch(self, limit: int = None) -> Dict:\n",
    "        \"\"\"Process batch of filings through complete pipeline\"\"\"\n",
    "        limit = limit or self.config.processing.filing_batch_size\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Get unprocessed filings\n",
    "        filings = self._get_unprocessed_filings(limit)\n",
    "        if not filings:\n",
    "            return {'success': False, 'message': 'No filings to process'}\n",
    "        \n",
    "        logger.info(f\"Processing {len(filings)} filings\")\n",
    "        \n",
    "        # Create analysis session\n",
    "        session_id = self._create_analysis_session(filings)\n",
    "        \n",
    "        results = {\n",
    "            'filings_processed': len(filings),\n",
    "            'entities_extracted': 0,\n",
    "            'relationships_found': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Extract all entities\n",
    "            entities = self.entity_extractor.process_filing_batch(filings)\n",
    "            results['entities_extracted'] = len(entities)\n",
    "            logger.info(f\"Extracted {len(entities)} entities\")\n",
    "            \n",
    "            # Step 2: Analyze relationships (if Llama enabled)\n",
    "            if self.config.enable_llama and entities:\n",
    "                # Group entities by filing for efficient processing\n",
    "                entities_by_filing = self._group_entities_by_filing(entities)\n",
    "                \n",
    "                for filing_ref, filing_entities in entities_by_filing.items():\n",
    "                    # Analyze batch\n",
    "                    relationships = self.relationship_analyzer.analyze_entity_batch(\n",
    "                        filing_entities, self.entity_extractor\n",
    "                    )\n",
    "                    \n",
    "                    # Store relationships\n",
    "                    if relationships:\n",
    "                        count = self.relationship_analyzer.store_relationships_batch(\n",
    "                            relationships, session_id\n",
    "                        )\n",
    "                        results['relationships_found'] += count\n",
    "                    \n",
    "                    # Clear cache after each filing\n",
    "                    self.entity_extractor.clear_cache()\n",
    "            \n",
    "            # Complete session\n",
    "            self._complete_analysis_session(session_id, True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline error: {e}\")\n",
    "            results['errors'].append(str(e))\n",
    "            self._complete_analysis_session(session_id, False)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        duration = time.time() - start_time\n",
    "        results['processing_time_seconds'] = round(duration, 2)\n",
    "        results['success'] = len(results['errors']) == 0\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _get_unprocessed_filings(self, limit: int) -> List[Dict]:\n",
    "        \"\"\"Get filings that haven't been processed\"\"\"\n",
    "        query = \"\"\"\n",
    "            SELECT sf.id, sf.company_domain, sf.filing_type, sf.url, sf.filing_date\n",
    "            FROM raw_data.sec_filings sf\n",
    "            LEFT JOIN system_uno.sec_entities_raw ser ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n",
    "            WHERE sf.url IS NOT NULL\n",
    "              AND ser.sec_filing_ref IS NULL\n",
    "            ORDER BY sf.filing_date DESC\n",
    "            LIMIT %s\n",
    "        \"\"\"\n",
    "        return self.db.execute_query(query, (limit,))\n",
    "    \n",
    "    def _group_entities_by_filing(self, entities: List[Dict]) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Group entities by filing for batch processing\"\"\"\n",
    "        grouped = {}\n",
    "        for entity in entities:\n",
    "            filing_ref = entity['sec_filing_ref']\n",
    "            if filing_ref not in grouped:\n",
    "                grouped[filing_ref] = []\n",
    "            grouped[filing_ref].append(entity)\n",
    "        return grouped\n",
    "    \n",
    "    def _create_analysis_session(self, filings: List[Dict]) -> str:\n",
    "        \"\"\"Create analysis session record\"\"\"\n",
    "        session_id = str(uuid.uuid4())\n",
    "        \n",
    "        with self.db.get_cursor() as cursor:\n",
    "            companies = list(set(f['company_domain'] for f in filings))\n",
    "            filing_refs = [f\"SEC_{f['id']}\" for f in filings]\n",
    "            \n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO system_uno.semantic_analysis_sessions\n",
    "                (session_id, company_domain, filing_batch, primary_prompt_version,\n",
    "                 session_start, session_status)\n",
    "                VALUES (%s, %s, %s, %s, NOW(), 'RUNNING')\n",
    "            \"\"\", (session_id, companies[0] if companies else 'multiple',\n",
    "                  filing_refs, self.config.llama.prompt_version))\n",
    "        \n",
    "        return session_id\n",
    "    \n",
    "    def _complete_analysis_session(self, session_id: str, success: bool):\n",
    "        \"\"\"Complete analysis session\"\"\"\n",
    "        with self.db.get_cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                UPDATE system_uno.semantic_analysis_sessions\n",
    "                SET session_end = NOW(),\n",
    "                    session_status = %s,\n",
    "                    total_processing_ms = EXTRACT(EPOCH FROM (NOW() - session_start)) * 1000\n",
    "                WHERE session_id = %s\n",
    "            \"\"\", ('COMPLETED' if success else 'FAILED', session_id))\n",
    "\n",
    "# Initialize orchestrator\n",
    "orchestrator = PipelineOrchestrator(\n",
    "    CONFIG, db_manager, entity_extractor, relationship_analyzer\n",
    ")\n",
    "print(\"✅ Pipeline orchestrator initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 6: Analytics Module\n",
    "\n",
    "class PipelineAnalytics:\n",
    "    \"\"\"Streamlined analytics and reporting\"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager: DatabaseManager):\n",
    "        self.db = db_manager\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate comprehensive analytics report\"\"\"\n",
    "        report = {}\n",
    "        \n",
    "        # Entity metrics\n",
    "        entity_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_entities,\n",
    "                COUNT(DISTINCT company_domain) as companies,\n",
    "                COUNT(DISTINCT sec_filing_ref) as filings,\n",
    "                AVG(confidence_score) as avg_confidence,\n",
    "                COUNT(*) FILTER (WHERE section_name IS NOT NULL) as with_sections\n",
    "            FROM system_uno.sec_entities_raw\n",
    "            WHERE data_source = 'sec_filings'\n",
    "        \"\"\"\n",
    "        entity_stats = self.db.execute_query(entity_query)[0] if self.db.execute_query(entity_query) else {}\n",
    "        report['entities'] = entity_stats\n",
    "        \n",
    "        # Relationship metrics\n",
    "        rel_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(DISTINCT b.bucket_id) as buckets,\n",
    "                COUNT(e.event_id) as events,\n",
    "                AVG(e.confidence_score) as avg_confidence\n",
    "            FROM system_uno.relationship_buckets b\n",
    "            LEFT JOIN system_uno.relationship_semantic_events e ON b.bucket_id = e.bucket_id\n",
    "        \"\"\"\n",
    "        rel_stats = self.db.execute_query(rel_query)[0] if self.db.execute_query(rel_query) else {}\n",
    "        report['relationships'] = rel_stats\n",
    "        \n",
    "        # Session metrics\n",
    "        session_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_sessions,\n",
    "                COUNT(*) FILTER (WHERE session_status = 'COMPLETED') as completed,\n",
    "                AVG(total_processing_ms/1000.0) as avg_duration_seconds\n",
    "            FROM system_uno.semantic_analysis_sessions\n",
    "            WHERE session_start > NOW() - INTERVAL '7 days'\n",
    "        \"\"\"\n",
    "        session_stats = self.db.execute_query(session_query)[0] if self.db.execute_query(session_query) else {}\n",
    "        report['sessions'] = session_stats\n",
    "        \n",
    "        # Validation metrics\n",
    "        val_query = \"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total,\n",
    "                COUNT(*) FILTER (WHERE validation_method != 'PENDING_REVIEW') as reviewed\n",
    "            FROM system_uno.relationship_validation\n",
    "        \"\"\"\n",
    "        val_stats = self.db.execute_query(val_query)[0] if self.db.execute_query(val_query) else {}\n",
    "        report['validation'] = val_stats\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def print_report(self):\n",
    "        \"\"\"Print formatted analytics report\"\"\"\n",
    "        report = self.generate_report()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE ANALYTICS REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if report.get('entities'):\n",
    "            e = report['entities']\n",
    "            print(f\"\\nENTITIES:\")\n",
    "            print(f\"  Total: {e.get('total_entities', 0):,}\")\n",
    "            print(f\"  Companies: {e.get('companies', 0):,}\")\n",
    "            print(f\"  Filings: {e.get('filings', 0):,}\")\n",
    "            print(f\"  Avg Confidence: {e.get('avg_confidence', 0):.3f}\")\n",
    "        \n",
    "        if report.get('relationships'):\n",
    "            r = report['relationships']\n",
    "            print(f\"\\nRELATIONSHIPS:\")\n",
    "            print(f\"  Buckets: {r.get('buckets', 0):,}\")\n",
    "            print(f\"  Events: {r.get('events', 0):,}\")\n",
    "            print(f\"  Avg Confidence: {r.get('avg_confidence', 0):.3f}\")\n",
    "        \n",
    "        if report.get('sessions'):\n",
    "            s = report['sessions']\n",
    "            print(f\"\\nSESSIONS (Last 7 days):\")\n",
    "            print(f\"  Total: {s.get('total_sessions', 0)}\")\n",
    "            print(f\"  Completed: {s.get('completed', 0)}\")\n",
    "            print(f\"  Avg Duration: {s.get('avg_duration_seconds', 0):.1f}s\")\n",
    "        \n",
    "        if report.get('validation'):\n",
    "            v = report['validation']\n",
    "            print(f\"\\nVALIDATION:\")\n",
    "            print(f\"  Total: {v.get('total', 0):,}\")\n",
    "            print(f\"  Reviewed: {v.get('reviewed', 0):,}\")\n",
    "            print(f\"  Pending: {v.get('total', 0) - v.get('reviewed', 0):,}\")\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Initialize analytics\n",
    "analytics = PipelineAnalytics(db_manager)\n",
    "print(\"✅ Analytics module initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cell 7: Main Execution\n",
    "\n",
    "def run_pipeline(filing_limit: int = None):\n",
    "    \"\"\"Main function to run the complete pipeline\"\"\"\n",
    "    print(\"\\n🚀 Starting SEC Entity & Relationship Extraction Pipeline\")\n",
    "    print(f\"   Configuration: Llama={'ON' if CONFIG.enable_llama else 'OFF'}, Batch={CONFIG.processing.batch_size}\")\n",
    "    \n",
    "    # Process filings\n",
    "    results = orchestrator.process_filing_batch(filing_limit)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n📊 RESULTS:\")\n",
    "    print(f\"   Filings Processed: {results['filings_processed']}\")\n",
    "    print(f\"   Entities Extracted: {results['entities_extracted']:,}\")\n",
    "    print(f\"   Relationships Found: {results['relationships_found']:,}\")\n",
    "    print(f\"   Processing Time: {results['processing_time_seconds']:.1f}s\")\n",
    "    \n",
    "    if results['errors']:\n",
    "        print(f\"   ⚠️ Errors: {len(results['errors'])}\")\n",
    "        for error in results['errors'][:3]:  # Show first 3 errors\n",
    "            print(f\"      - {error}\")\n",
    "    \n",
    "    # Generate analytics\n",
    "    analytics.print_report()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n📌 USAGE:\")\n",
    "print(\"   results = run_pipeline(filing_limit=5)  # Process 5 filings\")\n",
    "print(\"   analytics.print_report()  # View analytics\")\n",
    "print(\"\\n✅ Pipeline ready to execute!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}