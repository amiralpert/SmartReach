{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 0: GitHub Setup and Clean Auto-Logging\n\nimport os\nimport sys\nimport importlib\nimport importlib.util\nimport psycopg2\n\n# GitHub credentials - use Kaggle secrets for security\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngithub_token = user_secrets.get_secret(\"GITHUB_TOKEN\")\nrepo_url = f\"https://{github_token}@github.com/amiralpert/SmartReach.git\"\nlocal_path = \"/kaggle/working/SmartReach\"\n\nprint(\"üì¶ Setting up GitHub repository...\")\n\n# Clone or update repo with force pull\nif os.path.exists(local_path):\n    print(f\"üìÇ Repository exists at {local_path}\")\n    print(\"üîÑ Force updating from GitHub...\")\n    !cd {local_path} && git fetch origin\n    !cd {local_path} && git reset --hard origin/main\n    !cd {local_path} && git pull origin main\n    print(\"‚úÖ Repository updated\")\n    \n    # Show current commit\n    !cd {local_path} && echo \"Current commit:\" && git log --oneline -1\nelse:\n    print(f\"üì• Cloning repository to {local_path}\")\n    !git clone {repo_url} {local_path}\n    print(\"‚úÖ Repository cloned\")\n\n# Clear any cached modules from previous runs\nmodules_to_clear = [key for key in sys.modules.keys() if 'auto_logger' in key.lower() or 'clean' in key.lower()]\nfor mod in modules_to_clear:\n    del sys.modules[mod]\n    print(f\"  üßπ Cleared cached module: {mod}\")\n\n# Add to Python path for regular imports\nif f'{local_path}/BizIntel' in sys.path:\n    sys.path.remove(f'{local_path}/BizIntel')\nsys.path.insert(0, f'{local_path}/BizIntel')\n\nprint(\"‚úì Python path configured for SEC entity extraction!\")\n\n# Set up database configuration\nNEON_CONFIG = {\n    'host': 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech',\n    'database': 'BizIntelSmartReach',\n    'user': 'neondb_owner',\n    'password': 'npg_aTFt6Pug3Kpy',\n    'sslmode': 'require'\n}\n\n# Set up the new clean auto-logger\ntry:\n    # Create connection for logger\n    logger_conn = psycopg2.connect(**NEON_CONFIG)\n    print(\"‚úì Database connected for clean logger\")\n\n    # Import the redesigned clean auto-logger\n    logger_module_path = f\"{local_path}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\n    if os.path.exists(logger_module_path):\n        spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_module_path)\n        auto_logger_module = importlib.util.module_from_spec(spec)\n        sys.modules[\"auto_logger\"] = auto_logger_module\n        spec.loader.exec_module(auto_logger_module)\n\n        # Use the new clean logging setup\n        setup_clean_logging = auto_logger_module.setup_clean_logging\n        logger = setup_clean_logging(logger_conn, \"SEC_EntityExtraction\")\n        \n        print(\"‚ú® Clean auto-logging enabled!\")\n        print(\"üìã Features:\")\n        print(\"   ‚Ä¢ One row per cell execution\")\n        print(\"   ‚Ä¢ Complete output capture\")\n        print(\"   ‚Ä¢ Proper cell numbers from # Cell N: comments\")\n        print(\"   ‚Ä¢ Full error tracebacks\")\n        print(\"   ‚Ä¢ Execution timing\")\n        \n    else:\n        print(f\"‚úó Clean auto-logger not found at {logger_module_path}\")\n        logger = None\n        \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Clean logger setup failed: {e}\")\n    print(\"  Continuing without auto-logging...\")\n    logger = None\n\nprint(\"\\nüöÄ Setup complete! SEC Entity Extraction Engine with Clean Logging ready.\")\nprint(\"üí° Run cells with proper # Cell N: comments for best logging.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Neon Database Configuration\n",
    "NEON_CONFIG = {\n",
    "    'host': 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech',\n",
    "    'database': 'BizIntelSmartReach',\n",
    "    'user': 'neondb_owner',\n",
    "    'password': 'npg_aTFt6Pug3Kpy',\n",
    "    'sslmode': 'require'\n",
    "}\n",
    "\n",
    "# Test database connection\n",
    "def test_database_connection():\n",
    "    try:\n",
    "        conn = psycopg2.connect(**NEON_CONFIG)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Check SEC-related tables\n",
    "        cursor.execute('''\n",
    "            SELECT \n",
    "                (SELECT COUNT(*) FROM raw_data.sec_filings) as sec_filings,\n",
    "                (SELECT COUNT(*) FROM core.companies) as companies,\n",
    "                (SELECT COUNT(*) FROM system_uno.sec_entities_raw) as sec_entities_extracted,\n",
    "                (SELECT COUNT(DISTINCT company_domain) FROM raw_data.sec_filings) as companies_with_filings,\n",
    "                (SELECT COUNT(*) FROM raw_data.sec_filings WHERE url IS NOT NULL) as filings_with_urls\n",
    "        ''')\n",
    "        \n",
    "        counts = cursor.fetchone()\n",
    "        print(\"‚úì Database connected successfully!\")\n",
    "        print(f\"  SEC Filings: {counts[0]}\")\n",
    "        print(f\"  Companies: {counts[1]}\")\n",
    "        print(f\"  Extracted SEC Entities: {counts[2]}\")\n",
    "        print(f\"  Companies with SEC Filings: {counts[3]}\")\n",
    "        print(f\"  SEC Filings with URLs: {counts[4]}\")\n",
    "        \n",
    "        # Show sample SEC filing data\n",
    "        cursor.execute('''\n",
    "            SELECT company_domain, filing_type, COUNT(*) as count\n",
    "            FROM raw_data.sec_filings \n",
    "            GROUP BY company_domain, filing_type \n",
    "            ORDER BY company_domain, count DESC\n",
    "            LIMIT 10\n",
    "        ''')\n",
    "        \n",
    "        filing_stats = cursor.fetchall()\n",
    "        print(\"\\nüìä SEC Filing Distribution:\")\n",
    "        for stat in filing_stats:\n",
    "            print(f\"  {stat[0]}: {stat[1]} ({stat[2]} filings)\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Database connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "test_database_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Load All 4 NER Models (BioBERT, BERT-base, FinBERT, RoBERTa)\n\n# Install required packages for all models\n!pip install transformers torch requests beautifulsoup4 'lxml[html_clean]' uuid numpy edgartools newspaper3k\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport uuid\nimport json\nimport time\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nimport re\nimport torch\nimport numpy as np\nfrom abc import ABC, abstractmethod\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# Hugging Face Transformers\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n\nprint(\"üì¶ Loading all 4 NER models for multi-model extraction...\")\n\n# ========== 1. BioBERT (Biomedical Entities) ==========\ntry:\n    biobert_model_name = \"alvaroalon2/biobert_diseases_ner\"\n    print(f\"üß¨ Loading BioBERT: {biobert_model_name}\")\n    biobert_tokenizer = AutoTokenizer.from_pretrained(biobert_model_name)\n    biobert_model = AutoModelForTokenClassification.from_pretrained(biobert_model_name)\n    biobert_pipeline = pipeline(\"ner\", model=biobert_model, tokenizer=biobert_tokenizer, \n                                aggregation_strategy=\"average\", device=0 if torch.cuda.is_available() else -1)\n    print(\"   ‚úì BioBERT loaded (medical/disease entities)\")\nexcept Exception as e:\n    print(f\"   ‚ùå BioBERT failed to load: {e}\")\n    biobert_pipeline = None\n\n# ========== 2. BERT-base-NER (General Entities) ==========\ntry:\n    bert_model_name = \"dslim/bert-base-NER\"\n    print(f\"üìù Loading BERT-base-NER: {bert_model_name}\")\n    bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n    bert_model = AutoModelForTokenClassification.from_pretrained(bert_model_name)\n    bert_pipeline = pipeline(\"ner\", model=bert_model, tokenizer=bert_tokenizer,\n                            aggregation_strategy=\"average\", device=0 if torch.cuda.is_available() else -1)\n    print(\"   ‚úì BERT-base loaded (general entities: PER, ORG, LOC, MISC)\")\nexcept Exception as e:\n    print(f\"   ‚ùå BERT-base failed to load: {e}\")\n    bert_pipeline = None\n\n# ========== 3. FinBERT (Financial Entities) ==========\ntry:\n    finbert_model_name = \"ProsusAI/finbert\"\n    print(f\"üí∞ Loading FinBERT: {finbert_model_name}\")\n    finbert_tokenizer = AutoTokenizer.from_pretrained(finbert_model_name)\n    finbert_model = AutoModelForTokenClassification.from_pretrained(finbert_model_name)\n    finbert_pipeline = pipeline(\"ner\", model=finbert_model, tokenizer=finbert_tokenizer,\n                               aggregation_strategy=\"average\", device=0 if torch.cuda.is_available() else -1)\n    print(\"   ‚úì FinBERT loaded (financial entities)\")\nexcept Exception as e:\n    print(f\"   ‚ùå FinBERT failed to load: {e}\")\n    # Fallback: use BERT-base for financial content\n    finbert_pipeline = bert_pipeline if 'bert_pipeline' in locals() else None\n    if finbert_pipeline:\n        print(\"   üîÑ Using BERT-base as FinBERT fallback\")\n\n# ========== 4. RoBERTa-large-NER (High Precision) ==========\ntry:\n    roberta_model_name = \"Jean-Baptiste/roberta-large-ner-english\"\n    print(f\"üéØ Loading RoBERTa-large-NER: {roberta_model_name}\")\n    roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n    roberta_model = AutoModelForTokenClassification.from_pretrained(roberta_model_name)\n    roberta_pipeline = pipeline(\"ner\", model=roberta_model, tokenizer=roberta_tokenizer,\n                                aggregation_strategy=\"average\", device=0 if torch.cuda.is_available() else -1)\n    print(\"   ‚úì RoBERTa loaded (high-precision entities)\")\nexcept Exception as e:\n    print(f\"   ‚ùå RoBERTa failed to load: {e}\")\n    roberta_pipeline = None\n\n# ========== Multi-Model Configuration ==========\nEXTRACTION_CONFIG = {\n    'chunk_size': 400,           # Words per chunk\n    'overlap_size': 120,         # 30% overlap (120/400)\n    'context_window': 500,       # Characters before/after entity for context\n    'batch_size': 10,           # Process 10 documents at a time\n    'confidence_thresholds': {\n        'biobert': 0.5,\n        'bert_base': 0.5,\n        'finbert': 0.5,\n        'roberta': 0.6          # Higher threshold for high-precision model\n    },\n    'supported_entities': [\n        'DISEASE', 'CHEMICAL', 'MEDICATION', 'MEDICAL_CONDITION',\n        'DRUG', 'COMPOUND', 'THERAPY', 'TREATMENT',\n        'PERSON', 'ORGANIZATION', 'LOCATION', 'MISCELLANEOUS',\n        'FINANCIAL', 'REVENUE', 'COST', 'METRIC'\n    ]\n}\n\n# ========== Model Registry ==========\nLOADED_MODELS = {\n    'biobert': biobert_pipeline,\n    'bert_base': bert_pipeline,\n    'finbert': finbert_pipeline,\n    'roberta': roberta_pipeline\n}\n\n# Document-to-model routing configuration\nDOCUMENT_MODEL_ROUTING = {\n    'sec_filing': {\n        'markdown': ['biobert', 'bert_base', 'roberta'],  # Text sections\n        'xbrl': ['finbert', 'bert_base']                  # Financial data\n    },\n    'press_release': {\n        'markdown': ['biobert', 'bert_base', 'roberta']   # All text models\n    },\n    'stock_data': {\n        'xbrl': ['finbert', 'bert_base']                  # Financial focus\n    }\n}\n\n# Entity type mapping and normalization\nENTITY_TYPE_MAPPING = {\n    # BioBERT mappings\n    'Disease': 'MEDICAL_CONDITION',\n    'Chemical': 'MEDICATION',\n    'CHEMICAL': 'MEDICATION',\n    'DISEASE': 'MEDICAL_CONDITION',\n    'DRUG': 'MEDICATION',\n    'Drug': 'MEDICATION',\n    'Compound': 'MEDICATION',\n    'Treatment': 'THERAPY',\n    'Therapy': 'THERAPY',\n    \n    # BERT-base mappings\n    'PER': 'PERSON',\n    'ORG': 'ORGANIZATION',\n    'LOC': 'LOCATION',\n    'MISC': 'MISCELLANEOUS',\n    \n    # Financial mappings\n    'MONEY': 'FINANCIAL',\n    'PERCENT': 'FINANCIAL',\n    'NUMBER': 'METRIC'\n}\n\n# Count successfully loaded models\nloaded_count = sum(1 for model in LOADED_MODELS.values() if model is not None)\n\nprint(f\"\\n‚úÖ Model Loading Complete!\")\nprint(f\"   üìä Successfully loaded: {loaded_count}/4 models\")\nprint(f\"   üñ•Ô∏è Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\nprint(f\"   üîß Chunk size: {EXTRACTION_CONFIG['chunk_size']} words\")\nprint(f\"   üîÑ Overlap: {EXTRACTION_CONFIG['overlap_size']} words (30%)\")\nprint(f\"   üìã Total entity types: {len(EXTRACTION_CONFIG['supported_entities'])}\")\n\n# Test each model with sample biotech text\nif loaded_count > 0:\n    test_text = \"Pfizer's COVID-19 vaccine generated $37 billion in revenue. The FDA approved treatment for Alzheimer's disease in Boston.\"\n    print(f\"\\nüß™ Testing models with: '{test_text[:50]}...'\")\n    \n    for model_name, pipeline_obj in LOADED_MODELS.items():\n        if pipeline_obj:\n            try:\n                test_entities = pipeline_obj(test_text)\n                print(f\"   {model_name}: Found {len(test_entities)} entities\")\n                for entity in test_entities[:2]:  # Show first 2 entities\n                    print(f\"      ‚Ä¢ {entity['entity_group']}: '{entity['word']}' ({entity['score']:.3f})\")\n            except Exception as e:\n                print(f\"   {model_name}: Test failed - {e}\")\n\nprint(\"\\n‚úÖ Multi-Model NER Setup Complete! Ready for parallel processing.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Universal Document Parser with EdgarTools Integration\n\nimport edgar\nfrom edgar import Filing\nfrom newspaper import Article\nimport yfinance as yf\nimport re\n\n# ========== EXTENSIBLE PARSER REGISTRY ==========\nPARSER_REGISTRY = {}\n\ndef register_parser(doc_type: str):\n    \"\"\"Decorator to register new parsers - easily extensible\"\"\"\n    def decorator(parser_class):\n        PARSER_REGISTRY[doc_type] = parser_class\n        return parser_class\n    return decorator\n\n# ========== BASE PARSER INTERFACE ==========\nclass BaseDocumentParser(ABC):\n    \"\"\"Base interface that all parsers must implement\"\"\"\n    \n    @abstractmethod\n    def can_parse(self, document: Dict) -> bool:\n        \"\"\"Check if this parser can handle the document\"\"\"\n        pass\n    \n    @abstractmethod\n    def extract_content(self, document: Dict) -> Dict[str, Any]:\n        \"\"\"Extract and structure content from document\"\"\"\n        pass\n    \n    def chunk_text(self, text: str, chunk_size: int = 400, overlap: int = 120) -> List[Dict]:\n        \"\"\"Universal chunking strategy - sliding window with 30% overlap\"\"\"\n        if not text or not text.strip():\n            return []\n        \n        words = text.split()\n        if len(words) <= chunk_size:\n            return [{\n                'text': text,\n                'word_start': 0,\n                'word_end': len(words),\n                'chunk_index': 0,\n                'total_words': len(words)\n            }]\n        \n        chunks = []\n        for i in range(0, len(words), chunk_size - overlap):\n            chunk_words = words[i:min(i + chunk_size, len(words))]\n            chunk_text = ' '.join(chunk_words)\n            \n            chunks.append({\n                'text': chunk_text,\n                'word_start': i,\n                'word_end': min(i + chunk_size, len(words)),\n                'chunk_index': len(chunks),\n                'total_words': len(chunk_words)\n            })\n            \n            # Break if we've covered all words\n            if i + chunk_size >= len(words):\n                break\n        \n        return chunks\n\n# ========== SEC FILING PARSER (EdgarTools) ==========\n@register_parser('sec_filing')\nclass EdgarToolsParser(BaseDocumentParser):\n    \"\"\"Parse SEC filings using EdgarTools - eliminates XBRL noise\"\"\"\n    \n    def can_parse(self, document: Dict) -> bool:\n        return document.get('type') == 'sec_filing' and 'url' in document\n    \n    def _extract_accession_number_from_url(self, url: str) -> str:\n        \"\"\"Extract accession number from SEC filing URL\"\"\"\n        # SEC filing URLs typically have format: \n        # https://www.sec.gov/Archives/edgar/data/CIK/ACCESSION_NUMBER/filename\n        # or contain accession number in various formats\n        \n        # Match pattern like: 0001234567-23-000001 or 000123456723000001\n        accession_patterns = [\n            r'(\\d{10}-\\d{2}-\\d{6})',  # Standard format: 0001234567-23-000001\n            r'(\\d{18})',              # Compressed format: 000123456723000001\n        ]\n        \n        for pattern in accession_patterns:\n            match = re.search(pattern, url)\n            if match:\n                accession = match.group(1)\n                # Convert compressed format to standard format if needed\n                if len(accession) == 18 and '-' not in accession:\n                    # Convert 000123456723000001 to 0001234567-23-000001\n                    accession = f\"{accession[:10]}-{accession[10:12]}-{accession[12:]}\"\n                return accession\n        \n        return None\n    \n    def extract_content(self, document: Dict) -> Dict[str, Any]:\n        try:\n            print(f\"üè¢ Processing SEC filing: {document.get('company_domain', 'Unknown')}\")\n            \n            # Extract accession number from URL\n            accession_number = self._extract_accession_number_from_url(document['url'])\n            if not accession_number:\n                raise ValueError(f\"Could not extract accession number from URL: {document['url']}\")\n            \n            print(f\"   üìÑ Using accession number: {accession_number}\")\n            \n            # Create Filing object using accession number (correct API)\n            filing = Filing(accession_no=accession_number)\n            \n            content = {\n                'doc_id': document.get('id'),\n                'doc_type': 'sec_filing',\n                'filing_type': document.get('filing_type', ''),\n                'company': document.get('company_domain', ''),\n                'url': document['url'],\n                'filing_date': document.get('filing_date'),\n                'accession_number': accession_number,\n                'sections': {},\n                'has_markdown': False,\n                'has_xbrl': False,\n                'markdown_chunks': [],\n                'xbrl_data': {}\n            }\n            \n            # Extract clean text sections (no XBRL noise)\n            if hasattr(filing, 'text') and filing.text:\n                markdown_text = filing.text.strip()\n                if markdown_text:\n                    content['markdown_chunks'] = self.chunk_text(markdown_text)\n                    content['has_markdown'] = True\n                    print(f\"   üìÑ Extracted {len(content['markdown_chunks'])} text chunks ({len(markdown_text):,} chars)\")\n            elif hasattr(filing, 'document') and filing.document and hasattr(filing.document, 'text'):\n                # Try alternative text access method\n                markdown_text = filing.document.text.strip()\n                if markdown_text:\n                    content['markdown_chunks'] = self.chunk_text(markdown_text)\n                    content['has_markdown'] = True\n                    print(f\"   üìÑ Extracted {len(content['markdown_chunks'])} text chunks ({len(markdown_text):,} chars)\")\n            \n            # Extract XBRL financial data if available\n            try:\n                if hasattr(filing, 'xbrl') and filing.xbrl:\n                    content['xbrl_data'] = filing.xbrl.to_dict() if hasattr(filing.xbrl, 'to_dict') else {}\n                    content['has_xbrl'] = bool(content['xbrl_data'])\n                    if content['has_xbrl']:\n                        print(f\"   üí∞ Extracted XBRL financial data\")\n                elif hasattr(filing, 'financials') and filing.financials:\n                    # Alternative access to financial data\n                    content['xbrl_data'] = {'financials': str(filing.financials)}\n                    content['has_xbrl'] = True\n                    print(f\"   üí∞ Extracted financial data\")\n            except Exception as xbrl_e:\n                print(f\"   ‚ö†Ô∏è XBRL extraction failed: {xbrl_e}\")\n            \n            # Extract specific sections for detailed analysis\n            try:\n                if hasattr(filing, 'sections') and filing.sections:\n                    for section_name, section_content in filing.sections.items():\n                        if section_content and section_content.strip():\n                            content['sections'][section_name] = {\n                                'text': section_content,\n                                'chunks': self.chunk_text(section_content)\n                            }\n                            print(f\"   üìë Section '{section_name}': {len(content['sections'][section_name]['chunks'])} chunks\")\n            except Exception as sections_e:\n                print(f\"   ‚ö†Ô∏è Section extraction failed: {sections_e}\")\n            \n            # Verify we got some usable content\n            if not content['has_markdown'] and not content['has_xbrl'] and not content['sections']:\n                raise ValueError(\"No usable content extracted from filing\")\n            \n            return content\n            \n        except Exception as e:\n            print(f\"   ‚ùå EdgarTools parsing failed: {e}\")\n            # NO FALLBACK - let it fail properly\n            return {\n                'doc_id': document.get('id'),\n                'doc_type': 'sec_filing',\n                'company': document.get('company_domain', ''),\n                'url': document['url'],\n                'error': f'EdgarTools parsing failed: {e}',\n                'has_markdown': False,\n                'has_xbrl': False,\n                'parsing_failed': True\n            }\n\n# ========== PRESS RELEASE PARSER ==========\n@register_parser('press_release')\nclass PressReleaseParser(BaseDocumentParser):\n    \"\"\"Parse press releases and news articles\"\"\"\n    \n    def can_parse(self, document: Dict) -> bool:\n        return document.get('type') == 'press_release' and 'url' in document\n    \n    def extract_content(self, document: Dict) -> Dict[str, Any]:\n        try:\n            print(f\"üì∞ Processing press release: {document.get('title', 'Unknown')}\")\n            \n            article = Article(document['url'])\n            article.download()\n            article.parse()\n            \n            return {\n                'doc_id': document.get('id'),\n                'doc_type': 'press_release',\n                'title': article.title,\n                'company': document.get('company_domain', ''),\n                'url': document['url'],\n                'markdown_chunks': self.chunk_text(article.text),\n                'has_markdown': bool(article.text),\n                'has_xbrl': False,\n                'publish_date': article.publish_date,\n                'authors': article.authors\n            }\n            \n        except Exception as e:\n            print(f\"   ‚ùå Press release parsing failed: {e}\")\n            return {'doc_id': document.get('id'), 'error': str(e), 'has_markdown': False, 'has_xbrl': False}\n\n# ========== STOCK DATA PARSER (Future Extension Example) ==========\n@register_parser('stock_data')\nclass StockDataParser(BaseDocumentParser):\n    \"\"\"Parse stock market data - demonstrates extensibility\"\"\"\n    \n    def can_parse(self, document: Dict) -> bool:\n        return document.get('type') == 'stock_data' and 'symbol' in document\n    \n    def extract_content(self, document: Dict) -> Dict[str, Any]:\n        try:\n            print(f\"üìà Processing stock data: {document['symbol']}\")\n            \n            ticker = yf.Ticker(document['symbol'])\n            info = ticker.info\n            \n            # Convert financial metrics to text for NER processing\n            financial_text = f\"\"\"\n            Company: {info.get('longName', 'Unknown')}\n            Market Cap: ${info.get('marketCap', 0):,}\n            Revenue: ${info.get('totalRevenue', 0):,}\n            Employees: {info.get('fullTimeEmployees', 0):,}\n            Industry: {info.get('industry', 'Unknown')}\n            Sector: {info.get('sector', 'Unknown')}\n            \"\"\"\n            \n            return {\n                'doc_id': document.get('id'),\n                'doc_type': 'stock_data',\n                'symbol': document['symbol'],\n                'company': info.get('longName', document['symbol']),\n                'markdown_chunks': self.chunk_text(financial_text),\n                'has_markdown': True,\n                'has_xbrl': True,  # Financial data for FinBERT\n                'xbrl_data': {\n                    'market_cap': info.get('marketCap'),\n                    'revenue': info.get('totalRevenue'),\n                    'employees': info.get('fullTimeEmployees'),\n                    'industry': info.get('industry'),\n                    'sector': info.get('sector')\n                }\n            }\n            \n        except Exception as e:\n            print(f\"   ‚ùå Stock data parsing failed: {e}\")\n            return {'doc_id': document.get('id'), 'error': str(e), 'has_markdown': False, 'has_xbrl': False}\n\n# ========== MAIN PARSER ORCHESTRATOR ==========\nclass UniversalDocumentParser:\n    \"\"\"Main parser that routes documents to appropriate parsers\"\"\"\n    \n    def __init__(self):\n        self.parsers = {name: parser() for name, parser in PARSER_REGISTRY.items()}\n        print(f\"üìö Universal Document Parser initialized!\")\n        print(f\"   üîß Loaded {len(self.parsers)} document parsers:\")\n        for parser_name in self.parsers:\n            print(f\"      ‚Ä¢ {parser_name}\")\n    \n    def parse_batch(self, documents: List[Dict]) -> List[Dict]:\n        \"\"\"Parse a batch of mixed document types\"\"\"\n        parsed_results = []\n        \n        print(f\"\\nüöÄ Parsing {len(documents)} documents...\")\n        \n        for i, doc in enumerate(documents, 1):\n            doc_type = doc.get('type', 'unknown')\n            doc_id = doc.get('id', f'doc_{i}')\n            \n            print(f\"üìÑ [{i}/{len(documents)}] Processing {doc_type}: {doc_id}\")\n            \n            if doc_type in self.parsers:\n                parser = self.parsers[doc_type]\n                try:\n                    parsed = parser.extract_content(doc)\n                    parsed_results.append(parsed)\n                    \n                    # Summary - only show success if we actually got content\n                    if parsed.get('parsing_failed'):\n                        print(f\"   ‚ùå Parsing failed: {parsed.get('error', 'Unknown error')}\")\n                    else:\n                        chunks = parsed.get('markdown_chunks', [])\n                        has_content = parsed.get('has_markdown') or parsed.get('has_xbrl')\n                        status = \"‚úì\" if has_content else \"‚ö†Ô∏è\"\n                        print(f\"   {status} Parsed: {len(chunks)} chunks extracted\")\n                    \n                except Exception as e:\n                    print(f\"   ‚ùå Parser exception: {e}\")\n                    parsed_results.append({\n                        'doc_id': doc_id,\n                        'doc_type': doc_type,\n                        'error': str(e),\n                        'has_markdown': False,\n                        'has_xbrl': False,\n                        'parsing_failed': True\n                    })\n            else:\n                print(f\"   ‚ö†Ô∏è No parser available for document type: {doc_type}\")\n                parsed_results.append({\n                    'doc_id': doc_id,\n                    'doc_type': doc_type,\n                    'error': f'No parser for type: {doc_type}',\n                    'has_markdown': False,\n                    'has_xbrl': False,\n                    'parsing_failed': True\n                })\n                \n        return parsed_results\n    \n    def add_parser(self, doc_type: str, parser_class: BaseDocumentParser):\n        \"\"\"Dynamically add new parser at runtime\"\"\"\n        self.parsers[doc_type] = parser_class()\n        print(f\"‚úì Added new parser for: {doc_type}\")\n    \n    def get_documents_from_database(self, limit: int = 5) -> List[Dict]:\n        \"\"\"Get documents from database that need processing\"\"\"\n        try:\n            conn = psycopg2.connect(**NEON_CONFIG)\n            cursor = conn.cursor()\n            \n            # Get SEC filings that haven't been processed yet\n            cursor.execute('''\n                SELECT sf.id, sf.company_domain, sf.filing_type, sf.url, sf.filing_date, sf.title\n                FROM raw_data.sec_filings sf\n                LEFT JOIN system_uno.sec_entities_raw ser ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n                WHERE sf.url IS NOT NULL \n                AND ser.sec_filing_ref IS NULL\n                ORDER BY sf.filing_date DESC\n                LIMIT %s\n            ''', (limit,))\n            \n            filings = cursor.fetchall()\n            cursor.close()\n            conn.close()\n            \n            return [{\n                'id': filing[0],\n                'type': 'sec_filing',\n                'company_domain': filing[1],\n                'filing_type': filing[2],\n                'url': filing[3],\n                'filing_date': filing[4],\n                'title': filing[5]\n            } for filing in filings]\n            \n        except Exception as e:\n            print(f\"‚ùå Database query failed: {e}\")\n            return []\n\n# ========== INITIALIZE UNIVERSAL PARSER ==========\nuniversal_parser = UniversalDocumentParser()\n\n# ========== TEST PARSER ==========\nprint(f\"\\nüß™ Testing parser with database documents...\")\ntest_documents = universal_parser.get_documents_from_database(limit=1)\n\nif test_documents:\n    print(f\"üìÑ Found {len(test_documents)} documents to test\")\n    print(f\"   Sample URL: {test_documents[0].get('url', 'No URL')}\")\n    \n    test_results = universal_parser.parse_batch(test_documents)\n    \n    for result in test_results:\n        if result.get('parsing_failed'):\n            print(f\"   ‚ùå {result.get('doc_type')}: FAILED - {result.get('error')}\")\n        elif not result.get('error'):\n            chunks = len(result.get('markdown_chunks', []))\n            has_xbrl = result.get('has_xbrl', False)\n            print(f\"   ‚úì {result.get('doc_type')}: {chunks} chunks{'+ XBRL' if has_xbrl else ''}\")\n        else:\n            print(f\"   ‚ùå {result.get('doc_type')}: {result.get('error')}\")\nelse:\n    print(\"üì≠ No documents found for testing\")\n\nprint(f\"\\n‚úÖ Universal Document Parser ready!\")\nprint(f\"üîß Supported types: {list(PARSER_REGISTRY.keys())}\")\nprint(f\"üìä Chunking: {EXTRACTION_CONFIG['chunk_size']} words, {EXTRACTION_CONFIG['overlap_size']} overlap\")\nprint(f\"‚ö†Ô∏è SEC filings will FAIL if EdgarTools cannot parse them (no BeautifulSoup fallback)\")\n\n# Future: Easily add new parsers\n# @register_parser('clinical_trial')\n# class ClinicalTrialParser(BaseDocumentParser):\n#     ...\n\n# @register_parser('patent')\n# class PatentParser(BaseDocumentParser):\n#     ..."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Parallel Multi-Model NER Pipeline\n\n# ========== BASE NER MODEL INTERFACE ==========\nclass BaseNERModel(ABC):\n    \"\"\"Base interface for all NER models\"\"\"\n    \n    def __init__(self, model_name: str, pipeline_obj, confidence_threshold: float = 0.5):\n        self.model_name = model_name\n        self.pipeline = pipeline_obj\n        self.confidence_threshold = confidence_threshold\n        self.stats = {\n            'chunks_processed': 0,\n            'entities_found': 0,\n            'processing_time': 0\n        }\n    \n    def extract_entities_from_text(self, text: str, chunk_metadata: Dict = None) -> List[Dict]:\n        \"\"\"Extract entities from a single text chunk\"\"\"\n        if not self.pipeline or not text.strip():\n            return []\n        \n        try:\n            start_time = time.time()\n            \n            # Run NER pipeline\n            raw_entities = self.pipeline(text)\n            \n            # Process and filter results\n            processed_entities = []\n            for entity in raw_entities:\n                if entity['score'] >= self.confidence_threshold:\n                    # Normalize entity type\n                    entity_type = ENTITY_TYPE_MAPPING.get(entity['entity_group'], entity['entity_group'])\n                    \n                    processed_entity = {\n                        'entity_text': entity['word'].strip(),\n                        'entity_type': entity_type,\n                        'confidence_score': float(entity['score']),\n                        'char_start': entity['start'],\n                        'char_end': entity['end'],\n                        'model_source': self.model_name,\n                        'original_label': entity['entity_group']\n                    }\n                    \n                    # Add chunk metadata if provided\n                    if chunk_metadata:\n                        processed_entity.update(chunk_metadata)\n                    \n                    processed_entities.append(processed_entity)\n            \n            # Update statistics\n            processing_time = time.time() - start_time\n            self.stats['chunks_processed'] += 1\n            self.stats['entities_found'] += len(processed_entities)\n            self.stats['processing_time'] += processing_time\n            \n            return processed_entities\n            \n        except Exception as e:\n            print(f\"   ‚ùå {self.model_name} extraction failed: {e}\")\n            return []\n    \n    def process_document_chunks(self, chunks: List[Dict], doc_metadata: Dict) -> List[Dict]:\n        \"\"\"Process all chunks from a document\"\"\"\n        all_entities = []\n        \n        for chunk in chunks:\n            chunk_metadata = {\n                'chunk_index': chunk.get('chunk_index', 0),\n                'word_start': chunk.get('word_start', 0),\n                'word_end': chunk.get('word_end', 0),\n                'doc_id': doc_metadata.get('doc_id'),\n                'doc_type': doc_metadata.get('doc_type'),\n                'company': doc_metadata.get('company', ''),\n                'filing_type': doc_metadata.get('filing_type', ''),\n                'filing_date': doc_metadata.get('filing_date')\n            }\n            \n            entities = self.extract_entities_from_text(chunk['text'], chunk_metadata)\n            all_entities.extend(entities)\n            \n            # Small delay to prevent overwhelming the model\n            time.sleep(0.01)\n        \n        return all_entities\n\n# ========== NER MODEL IMPLEMENTATIONS ==========\nclass MultiModelNERPipeline:\n    \"\"\"Orchestrates parallel processing across multiple NER models\"\"\"\n    \n    def __init__(self, loaded_models: Dict, max_workers: int = 4):\n        self.models = self._initialize_model_objects(loaded_models)\n        self.max_workers = max_workers\n        self.processing_stats = {\n            'documents_processed': 0,\n            'total_entities_found': 0,\n            'total_processing_time': 0,\n            'models_used': list(self.models.keys())\n        }\n        \n        print(f\"ü§ñ Multi-Model NER Pipeline initialized!\")\n        print(f\"   üìä Active models: {list(self.models.keys())}\")\n        print(f\"   üßµ Max workers: {max_workers}\")\n    \n    def _initialize_model_objects(self, loaded_models: Dict) -> Dict:\n        \"\"\"Initialize model wrapper objects\"\"\"\n        models = {}\n        \n        for model_name, pipeline_obj in loaded_models.items():\n            if pipeline_obj is not None:\n                confidence_threshold = EXTRACTION_CONFIG['confidence_thresholds'].get(model_name, 0.5)\n                models[model_name] = BaseNERModel(model_name, pipeline_obj, confidence_threshold)\n                print(f\"   ‚úì Initialized {model_name} (threshold: {confidence_threshold})\")\n        \n        return models\n    \n    def route_document_to_models(self, parsed_doc: Dict) -> List[str]:\n        \"\"\"Determine which models should process this document\"\"\"\n        doc_type = parsed_doc.get('doc_type', 'unknown')\n        applicable_models = set()\n        \n        if doc_type in DOCUMENT_MODEL_ROUTING:\n            routing = DOCUMENT_MODEL_ROUTING[doc_type]\n            \n            # Route text content to appropriate models\n            if parsed_doc.get('has_markdown') and 'markdown' in routing:\n                applicable_models.update(routing['markdown'])\n            \n            # Route financial data to FinBERT\n            if parsed_doc.get('has_xbrl') and 'xbrl' in routing:\n                applicable_models.update(routing['xbrl'])\n        else:\n            # Default: use all text models for unknown document types\n            if parsed_doc.get('has_markdown'):\n                applicable_models.update(['biobert', 'bert_base', 'roberta'])\n        \n        # Filter to only include loaded models\n        return [model for model in applicable_models if model in self.models]\n    \n    def process_single_document(self, parsed_doc: Dict) -> List[Dict]:\n        \"\"\"Process a single document through applicable models\"\"\"\n        if parsed_doc.get('error'):\n            return []\n        \n        doc_id = parsed_doc.get('doc_id', 'unknown')\n        doc_type = parsed_doc.get('doc_type', 'unknown')\n        \n        print(f\"üîç Processing {doc_type} document: {doc_id}\")\n        \n        # Determine applicable models\n        applicable_models = self.route_document_to_models(parsed_doc)\n        \n        if not applicable_models:\n            print(f\"   ‚ö†Ô∏è No applicable models for {doc_type}\")\n            return []\n        \n        print(f\"   üéØ Using models: {applicable_models}\")\n        \n        all_entities = []\n        \n        # Process with each applicable model\n        for model_name in applicable_models:\n            if model_name not in self.models:\n                continue\n                \n            model = self.models[model_name]\n            model_entities = []\n            \n            # Process markdown content\n            if model_name != 'finbert' and parsed_doc.get('markdown_chunks'):\n                chunks = parsed_doc['markdown_chunks']\n                print(f\"   üìù {model_name}: processing {len(chunks)} text chunks\")\n                model_entities.extend(model.process_document_chunks(chunks, parsed_doc))\n            \n            # Process XBRL content (FinBERT only)\n            elif model_name == 'finbert' and parsed_doc.get('has_xbrl'):\n                xbrl_text = json.dumps(parsed_doc.get('xbrl_data', {}))\n                if xbrl_text and xbrl_text != '{}':\n                    print(f\"   üí∞ {model_name}: processing XBRL data\")\n                    # Create pseudo-chunk for XBRL data\n                    xbrl_chunks = [{'text': xbrl_text, 'chunk_index': 0, 'word_start': 0, 'word_end': len(xbrl_text.split())}]\n                    model_entities.extend(model.process_document_chunks(xbrl_chunks, parsed_doc))\n            \n            print(f\"      ‚Üí Found {len(model_entities)} entities\")\n            all_entities.extend(model_entities)\n        \n        return all_entities\n    \n    def process_batch_parallel(self, parsed_documents: List[Dict]) -> List[Dict]:\n        \"\"\"Process multiple documents in parallel across all models\"\"\"\n        start_time = time.time()\n        all_entities = []\n        \n        print(f\"\\nüöÄ Processing {len(parsed_documents)} documents with multi-model pipeline...\")\n        \n        # Process each document (models run in parallel within each document)\n        for i, doc in enumerate(parsed_documents, 1):\n            print(f\"\\nüìÑ [{i}/{len(parsed_documents)}] {doc.get('doc_type', 'unknown')}: {doc.get('doc_id', 'unknown')}\")\n            \n            doc_entities = self.process_single_document(doc)\n            \n            # Add unique extraction IDs and additional metadata\n            for entity in doc_entities:\n                entity['extraction_id'] = str(uuid.uuid4())\n                entity['sec_filing_ref'] = f\"SEC_{doc.get('doc_id')}\" if doc.get('doc_type') == 'sec_filing' else f\"{doc.get('doc_type').upper()}_{doc.get('doc_id')}\"\n                entity['extraction_timestamp'] = datetime.now().isoformat()\n            \n            all_entities.extend(doc_entities)\n            print(f\"   ‚úì Total entities from document: {len(doc_entities)}\")\n        \n        # Merge entities at same positions\n        merged_entities = self.merge_position_overlaps(all_entities)\n        \n        # Update statistics\n        processing_time = time.time() - start_time\n        self.processing_stats['documents_processed'] += len(parsed_documents)\n        self.processing_stats['total_entities_found'] += len(merged_entities)\n        self.processing_stats['total_processing_time'] += processing_time\n        \n        print(f\"\\n‚úÖ Batch processing complete!\")\n        print(f\"   üìä Documents: {len(parsed_documents)}\")\n        print(f\"   üîç Raw entities: {len(all_entities)}\")\n        print(f\"   üéØ Merged entities: {len(merged_entities)}\")\n        print(f\"   ‚è±Ô∏è Processing time: {processing_time:.2f} seconds\")\n        \n        return merged_entities\n    \n    def merge_position_overlaps(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Merge entities detected at same position by different models\"\"\"\n        if not entities:\n            return []\n        \n        # Group entities by document and position\n        position_groups = {}\n        \n        for entity in entities:\n            # Create position key: doc_id + character range\n            pos_key = f\"{entity.get('doc_id')}_{entity.get('char_start')}_{entity.get('char_end')}\"\n            \n            if pos_key not in position_groups:\n                position_groups[pos_key] = []\n            position_groups[pos_key].append(entity)\n        \n        merged_entities = []\n        merge_stats = {'single_model': 0, 'multi_model_merged': 0}\n        \n        for pos_key, group in position_groups.items():\n            if len(group) == 1:\n                # Single model detection - keep as is\n                merged_entities.append(group[0])\n                merge_stats['single_model'] += 1\n            else:\n                # Multiple models detected same position - merge\n                merged = self._merge_entity_group(group)\n                merged_entities.append(merged)\n                merge_stats['multi_model_merged'] += 1\n        \n        print(f\"   üîó Merge stats: {merge_stats['single_model']} single, {merge_stats['multi_model_merged']} merged\")\n        return merged_entities\n    \n    def _merge_entity_group(self, entities: List[Dict]) -> Dict:\n        \"\"\"Merge entities from different models at same position\"\"\"\n        # Priority: BioBERT > FinBERT > RoBERTa > BERT-base for biotech domain\n        priority = {'biobert': 4, 'finbert': 3, 'roberta': 2, 'bert_base': 1}\n        \n        # Sort by priority, then by confidence\n        entities.sort(key=lambda x: (priority.get(x['model_source'], 0), x['confidence_score']), reverse=True)\n        \n        # Take best version but track all models\n        best = entities[0].copy()\n        \n        # Add multi-model metadata\n        best['models_detected'] = [e['model_source'] for e in entities]\n        best['all_confidences'] = {e['model_source']: e['confidence_score'] for e in entities}\n        best['primary_model'] = best['model_source']\n        best['entity_variations'] = {e['model_source']: e['entity_text'] for e in entities}\n        best['is_merged'] = True\n        \n        # Use highest confidence score\n        best['confidence_score'] = max(e['confidence_score'] for e in entities)\n        \n        return best\n    \n    def get_processing_summary(self) -> Dict:\n        \"\"\"Get comprehensive processing statistics\"\"\"\n        model_stats = {}\n        for model_name, model in self.models.items():\n            model_stats[model_name] = model.stats.copy()\n            \n        return {\n            'pipeline_stats': self.processing_stats,\n            'model_stats': model_stats,\n            'routing_config': DOCUMENT_MODEL_ROUTING,\n            'confidence_thresholds': EXTRACTION_CONFIG['confidence_thresholds']\n        }\n\n# ========== INITIALIZE MULTI-MODEL PIPELINE ==========\nif 'LOADED_MODELS' in locals():\n    ner_pipeline = MultiModelNERPipeline(LOADED_MODELS, max_workers=4)\n    \n    # ========== PROCESSING FUNCTION ==========\n    def process_documents_end_to_end(documents: List[Dict]) -> List[Dict]:\n        \"\"\"Complete end-to-end processing pipeline\"\"\"\n        print(f\"\\nüîÑ Starting end-to-end processing of {len(documents)} documents...\")\n        \n        # Step 1: Parse documents\n        if hasattr(documents[0], 'get') and documents[0].get('doc_type'):\n            # Already parsed\n            parsed_docs = documents\n            print(f\"‚úì Using pre-parsed documents\")\n        else:\n            # Parse documents\n            parsed_docs = universal_parser.parse_batch(documents)\n            print(f\"‚úì Parsed {len(parsed_docs)} documents\")\n        \n        # Step 2: Extract entities with all applicable models\n        entities = ner_pipeline.process_batch_parallel(parsed_docs)\n        \n        return entities\n    \n    # ========== TEST PROCESSING ==========\n    print(f\"\\nüß™ Testing multi-model processing...\")\n    \n    # Get test documents from database\n    test_docs = universal_parser.get_documents_from_database(limit=1)\n    \n    if test_docs:\n        print(f\"üìÑ Testing with {len(test_docs)} documents\")\n        test_entities = process_documents_end_to_end(test_docs)\n        \n        # Summary by model\n        model_breakdown = {}\n        for entity in test_entities:\n            model = entity.get('primary_model', 'unknown')\n            model_breakdown[model] = model_breakdown.get(model, 0) + 1\n        \n        print(f\"\\nüìä Test Results:\")\n        print(f\"   Total entities: {len(test_entities)}\")\n        for model, count in model_breakdown.items():\n            print(f\"   {model}: {count} entities\")\n            \n        # Show sample entities\n        print(f\"\\nüîç Sample entities:\")\n        for entity in test_entities[:3]:\n            models = entity.get('models_detected', [entity.get('model_source')])\n            print(f\"   ‚Ä¢ {entity['entity_type']}: '{entity['entity_text']}' ({'+'.join(models)}, {entity['confidence_score']:.3f})\")\n            \n    else:\n        print(\"üì≠ No test documents available\")\n    \n    print(f\"\\n‚úÖ Multi-Model NER Pipeline ready for batch processing!\")\n    print(f\"üéØ Usage: entities = process_documents_end_to_end(documents)\")\n    \nelse:\n    print(\"‚ùå LOADED_MODELS not found - ensure Cell 2 was run successfully\")\n    ner_pipeline = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Multi-Model Entity Storage with Position-Based Merging\n\nclass MultiModelEntityStorage:\n    \"\"\"Store entities from multiple models with position-based merging support\"\"\"\n    \n    def __init__(self, db_config):\n        self.db_config = db_config\n        self.storage_stats = {\n            'total_entities_stored': 0,\n            'successful_inserts': 0,\n            'failed_inserts': 0,\n            'duplicate_entities': 0,\n            'merged_entities': 0,\n            'single_model_entities': 0\n        }\n    \n    def prepare_entity_for_storage(self, entity: Dict[str, Any]) -> tuple:\n        \"\"\"Prepare entity data for database insertion with proper type conversion\"\"\"\n        \n        # Core entity data with length limits\n        entity_text = str(entity.get('entity_text', '')).strip()[:1000]\n        company_domain = str(entity.get('company', ''))[:255]\n        entity_category = str(entity.get('entity_type', ''))[:100]\n        sec_filing_ref = str(entity.get('sec_filing_ref', ''))[:255]\n        \n        # Handle surrounding text (extract from original chunk if needed)\n        surrounding_text = str(entity.get('surrounding_text', ''))\n        if not surrounding_text and entity.get('chunk_index') is not None:\n            # Use entity position context\n            surrounding_text = f\"Chunk {entity.get('chunk_index', 0)}: {entity_text}\"\n        \n        # Multi-model specific fields\n        models_detected = entity.get('models_detected', [entity.get('model_source')])\n        if not isinstance(models_detected, list):\n            models_detected = [str(models_detected)]\n        \n        all_confidences = entity.get('all_confidences', {entity.get('model_source', 'unknown'): entity.get('confidence_score', 0.0)})\n        if not isinstance(all_confidences, dict):\n            all_confidences = {entity.get('model_source', 'unknown'): float(entity.get('confidence_score', 0.0))}\n        \n        primary_model = str(entity.get('primary_model', entity.get('model_source', 'unknown')))\n        \n        # Entity variations from different models\n        entity_variations = entity.get('entity_variations', {entity.get('model_source', 'unknown'): entity_text})\n        if not isinstance(entity_variations, dict):\n            entity_variations = {entity.get('model_source', 'unknown'): entity_text}\n        \n        return (\n            entity.get('extraction_id'),\n            company_domain,\n            entity_text,\n            entity_category,\n            float(entity.get('confidence_score', 0.0)),\n            int(entity.get('char_start', 0)),\n            int(entity.get('char_end', 0)),\n            surrounding_text,\n            sec_filing_ref,\n            models_detected,           # ARRAY field\n            json.dumps(all_confidences),   # JSONB field\n            primary_model,\n            json.dumps(entity_variations), # JSONB field\n            entity.get('is_merged', False),\n            entity.get('chunk_index', 0),\n            entity.get('extraction_timestamp'),\n            entity.get('original_label', '')\n        )\n    \n    def create_enhanced_table_if_needed(self):\n        \"\"\"Create enhanced table structure for multi-model storage\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Check if enhanced columns exist\n            cursor.execute(\\\"\\\"\\\"\\n                SELECT column_name \\n                FROM information_schema.columns \\n                WHERE table_schema = 'system_uno' \\n                AND table_name = 'sec_entities_raw'\\n                AND column_name IN ('models_detected', 'all_confidences', 'primary_model')\\n            \\\"\\\"\\\")\\n            \\n            existing_columns = [row[0] for row in cursor.fetchall()]\\n            \\n            # Add missing columns for multi-model support\\n            if 'models_detected' not in existing_columns:\\n                cursor.execute('ALTER TABLE system_uno.sec_entities_raw ADD COLUMN models_detected TEXT[]')\\n                print(\\\"   ‚úì Added models_detected column\\\")\\n            \\n            if 'all_confidences' not in existing_columns:\\n                cursor.execute('ALTER TABLE system_uno.sec_entities_raw ADD COLUMN all_confidences JSONB')\\n                print(\\\"   ‚úì Added all_confidences column\\\")\\n            \\n            if 'primary_model' not in existing_columns:\\n                cursor.execute('ALTER TABLE system_uno.sec_entities_raw ADD COLUMN primary_model TEXT')\\n                print(\\\"   ‚úì Added primary_model column\\\")\\n                \\n            # Add additional helpful columns\\n            cursor.execute(\\\"\\\"\\\"\\n                ALTER TABLE system_uno.sec_entities_raw \\n                ADD COLUMN IF NOT EXISTS entity_variations JSONB,\\n                ADD COLUMN IF NOT EXISTS is_merged BOOLEAN DEFAULT FALSE,\\n                ADD COLUMN IF NOT EXISTS chunk_index INTEGER,\\n                ADD COLUMN IF NOT EXISTS extraction_timestamp TIMESTAMP,\\n                ADD COLUMN IF NOT EXISTS original_label TEXT\\n            \\\"\\\"\\\")\\n            \\n            # Create indexes for efficient querying\\n            cursor.execute(\\\"\\\"\\\"\\n                CREATE INDEX IF NOT EXISTS idx_sec_entities_position \\n                ON system_uno.sec_entities_raw (sec_filing_ref, character_start, character_end)\\n            \\\"\\\"\\\")\\n            \\n            cursor.execute(\\\"\\\"\\\"\\n                CREATE INDEX IF NOT EXISTS idx_sec_entities_models \\n                ON system_uno.sec_entities_raw USING GIN (models_detected)\\n            \\\"\\\"\\\")\\n            \\n            conn.commit()\\n            cursor.close()\\n            conn.close()\\n            print(\\\"   ‚úì Enhanced table structure ready\\\")\\n            \\n        except Exception as e:\\n            print(f\\\"   ‚ö†Ô∏è Table enhancement failed: {e}\\\")\\n    \\n    def store_entities(self, entities: List[Dict[str, Any]]) -> bool:\\n        \\\"\\\"\\\"Store multi-model entities in database\\\"\\\"\\\"        \\n        if not entities:\\n            print(\\\"‚ö†Ô∏è No entities to store\\\")\\n            return True\\n        \\n        # Ensure enhanced table structure\\n        self.create_enhanced_table_if_needed()\\n        \\n        try:\\n            conn = psycopg2.connect(**self.db_config)\\n            cursor = conn.cursor()\\n            \\n            print(f\\\"üíæ Storing {len(entities)} multi-model entities...\\\")\\n            \\n            # Prepare batch insert data\\n            insert_data = []\\n            merged_count = 0\\n            single_count = 0\\n            \\n            for entity in entities:\\n                prepared_data = self.prepare_entity_for_storage(entity)\\n                insert_data.append(prepared_data)\\n                \\n                if entity.get('is_merged', False):\\n                    merged_count += 1\\n                else:\\n                    single_count += 1\\n            \\n            # Enhanced insert query with multi-model fields\\n            insert_query = \\\"\\\"\\\"\\n                INSERT INTO system_uno.sec_entities_raw \\n                (extraction_id, company_domain, entity_text, entity_category, \\n                 confidence_score, character_start, character_end, surrounding_text, \\n                 sec_filing_ref, models_detected, all_confidences, primary_model,\\n                 entity_variations, is_merged, chunk_index, extraction_timestamp, original_label)\\n                VALUES %s\\n                ON CONFLICT (extraction_id) DO UPDATE SET\\n                    models_detected = EXCLUDED.models_detected,\\n                    all_confidences = EXCLUDED.all_confidences,\\n                    primary_model = EXCLUDED.primary_model,\\n                    entity_variations = EXCLUDED.entity_variations,\\n                    is_merged = EXCLUDED.is_merged\\n            \\\"\\\"\\\"\\n            \\n            from psycopg2.extras import execute_values\\n            execute_values(cursor, insert_query, insert_data, page_size=100)\\n            \\n            # Get number of rows actually inserted/updated\\n            rows_affected = cursor.rowcount\\n            \\n            conn.commit()\\n            cursor.close()\\n            conn.close()\\n            \\n            # Update statistics\\n            self.storage_stats['total_entities_stored'] += len(entities)\\n            self.storage_stats['successful_inserts'] += rows_affected\\n            self.storage_stats['merged_entities'] += merged_count\\n            self.storage_stats['single_model_entities'] += single_count\\n            \\n            print(f\\\"   ‚úì Stored {rows_affected} entities\\\")\\n            print(f\\\"   üîó Merged entities: {merged_count}\\\")\\n            print(f\\\"   üéØ Single-model entities: {single_count}\\\")\\n            \\n            return True\\n            \\n        except Exception as e:\\n            print(f\\\"   ‚ùå Storage failed: {e}\\\")\\n            self.storage_stats['failed_inserts'] += len(entities)\\n            return False\\n    \\n    def verify_multi_model_storage(self, sec_filing_ref: str) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Verify multi-model entities were stored correctly\\\"\\\"\\\"        \\n        try:\\n            conn = psycopg2.connect(**self.db_config)\\n            cursor = conn.cursor()\\n            \\n            # Enhanced verification query with multi-model stats\\n            cursor.execute(\\\"\\\"\\\"\\n                SELECT \\n                    entity_category,\\n                    COUNT(*) as count,\\n                    AVG(confidence_score) as avg_confidence,\\n                    COUNT(*) FILTER (WHERE is_merged = true) as merged_entities,\\n                    COUNT(*) FILTER (WHERE is_merged = false) as single_model_entities,\\n                    COUNT(DISTINCT primary_model) as unique_models_used\\n                FROM system_uno.sec_entities_raw\\n                WHERE sec_filing_ref = %s\\n                GROUP BY entity_category\\n                ORDER BY count DESC\\n            \\\"\\\"\\\", (sec_filing_ref,))\\n            \\n            category_results = cursor.fetchall()\\n            \\n            # Model usage statistics\\n            cursor.execute(\\\"\\\"\\\"\\n                SELECT \\n                    primary_model,\\n                    COUNT(*) as entities_count,\\n                    AVG(confidence_score) as avg_confidence\\n                FROM system_uno.sec_entities_raw\\n                WHERE sec_filing_ref = %s\\n                GROUP BY primary_model\\n                ORDER BY entities_count DESC\\n            \\\"\\\"\\\", (sec_filing_ref,))\\n            \\n            model_results = cursor.fetchall()\\n            \\n            # Multi-model detection stats\\n            cursor.execute(\\\"\\\"\\\"\\n                SELECT \\n                    array_length(models_detected, 1) as num_models,\\n                    COUNT(*) as count\\n                FROM system_uno.sec_entities_raw\\n                WHERE sec_filing_ref = %s AND models_detected IS NOT NULL\\n                GROUP BY array_length(models_detected, 1)\\n                ORDER BY num_models\\n            \\\"\\\"\\\", (sec_filing_ref,))\\n            \\n            multi_model_stats = cursor.fetchall()\\n            \\n            cursor.close()\\n            conn.close()\\n            \\n            verification = {\\n                'total_entities': sum(result[1] for result in category_results),\\n                'entity_breakdown': [{\\n                    'category': result[0],\\n                    'count': result[1],\\n                    'avg_confidence': float(result[2]),\\n                    'merged_entities': result[3],\\n                    'single_model_entities': result[4],\\n                    'unique_models': result[5]\\n                } for result in category_results],\\n                'model_usage': [{\\n                    'model': result[0],\\n                    'entities': result[1],\\n                    'avg_confidence': float(result[2])\\n                } for result in model_results],\\n                'multi_model_detection': [{\\n                    'num_models': result[0] or 1,\\n                    'entities': result[1]\\n                } for result in multi_model_stats]\\n            }\\n            \\n            return verification\\n            \\n        except Exception as e:\\n            print(f\\\"   ‚ùå Verification failed: {e}\\\")\\n            return {}\\n    \\n    def get_storage_summary(self) -> Dict[str, Any]:\\n        \\\"\\\"\\\"Get comprehensive storage statistics\\\"\\\"\\\"        \\n        return self.storage_stats.copy()\\n    \\n    def query_entities_by_models(self, models: List[str], limit: int = 10) -> List[Dict]:\\n        \\\"\\\"\\\"Query entities detected by specific models\\\"\\\"\\\"        \\n        try:\\n            conn = psycopg2.connect(**self.db_config)\\n            cursor = conn.cursor()\\n            \\n            # Query entities detected by any of the specified models\\n            cursor.execute(\\\"\\\"\\\"\\n                SELECT \\n                    entity_text, entity_category, confidence_score,\\n                    models_detected, all_confidences, primary_model,\\n                    is_merged, sec_filing_ref\\n                FROM system_uno.sec_entities_raw\\n                WHERE models_detected && %s\\n                ORDER BY confidence_score DESC\\n                LIMIT %s\\n            \\\"\\\"\\\", (models, limit))\\n            \\n            results = cursor.fetchall()\\n            cursor.close()\\n            conn.close()\\n            \\n            return [{\\n                'entity_text': result[0],\\n                'entity_category': result[1],\\n                'confidence_score': result[2],\\n                'models_detected': result[3],\\n                'all_confidences': json.loads(result[4]) if result[4] else {},\\n                'primary_model': result[5],\\n                'is_merged': result[6],\\n                'sec_filing_ref': result[7]\\n            } for result in results]\\n            \\n        except Exception as e:\\n            print(f\\\"‚ùå Query failed: {e}\\\")\\n            return []\\n\\n# ========== INITIALIZE ENHANCED STORAGE ==========\\nstorage = MultiModelEntityStorage(NEON_CONFIG)\\nprint(\\\"‚úì Multi-Model Entity Storage initialized!\\\")\\n\\n# ========== COMPLETE PROCESSING PIPELINE ==========\\ndef process_and_store_documents(documents: List[Dict], store_results: bool = True) -> Dict[str, Any]:\\n    \\\"\\\"\\\"Complete pipeline: Parse ‚Üí Extract ‚Üí Store\\\"\\\"\\\"    \\n    print(f\\\"\\\\nüöÄ Complete processing pipeline for {len(documents)} documents...\\\")\\n    \\n    pipeline_start = time.time()\\n    \\n    # Step 1: Process documents through multi-model pipeline\\n    entities = process_documents_end_to_end(documents)\\n    \\n    if not entities:\\n        return {\\n            'success': False,\\n            'message': 'No entities extracted',\\n            'entities_extracted': 0\\n        }\\n    \\n    # Step 2: Store entities if requested\\n    storage_success = True\\n    if store_results:\\n        storage_success = storage.store_entities(entities)\\n    \\n    # Step 3: Verification and summary\\n    pipeline_time = time.time() - pipeline_start\\n    \\n    # Get verification for first document\\n    verification = {}\\n    if entities and store_results:\\n        first_filing_ref = entities[0].get('sec_filing_ref')\\n        if first_filing_ref:\\n            verification = storage.verify_multi_model_storage(first_filing_ref)\\n    \\n    # Compile results\\n    result = {\\n        'success': storage_success,\\n        'documents_processed': len(documents),\\n        'entities_extracted': len(entities),\\n        'pipeline_time': round(pipeline_time, 2),\\n        'verification': verification,\\n        'entities_sample': entities[:5] if entities else []  # First 5 entities as sample\\n    }\\n    \\n    print(f\\\"\\\\n‚úÖ Pipeline Complete!\\\")\\n    print(f\\\"   üìä Documents: {result['documents_processed']}\\\")\\n    print(f\\\"   üîç Entities: {result['entities_extracted']}\\\")\\n    print(f\\\"   ‚è±Ô∏è Time: {result['pipeline_time']} seconds\\\")\\n    print(f\\\"   üíæ Storage: {'‚úì' if storage_success else '‚ùå'}\\\")\\n    \\n    return result\\n\\n# ========== TEST COMPLETE PIPELINE ==========\\nprint(f\\\"\\\\nüß™ Testing complete pipeline...\\\")\\n\\n# Get test documents\\ntest_documents = universal_parser.get_documents_from_database(limit=1)\\n\\nif test_documents:\\n    print(f\\\"üìÑ Testing with {len(test_documents)} SEC filings\\\")\\n    \\n    # Run complete pipeline\\n    test_results = process_and_store_documents(test_documents, store_results=True)\\n    \\n    if test_results['success']:\\n        print(f\\\"\\\\nüìä Test Results Summary:\\\")\\n        print(f\\\"   Entities extracted: {test_results['entities_extracted']}\\\")\\n        \\n        # Show verification details\\n        if test_results['verification']:\\n            print(f\\\"   Entity categories:\\\")\\n            for cat in test_results['verification']['entity_breakdown'][:3]:\\n                print(f\\\"      ‚Ä¢ {cat['category']}: {cat['count']} entities ({cat['merged_entities']} merged)\\\")\\n            \\n            print(f\\\"   Model usage:\\\")\\n            for model in test_results['verification']['model_usage']:\\n                print(f\\\"      ‚Ä¢ {model['model']}: {model['entities']} entities (avg conf: {model['avg_confidence']:.3f})\\\")\\n        \\n        # Show sample entities\\n        print(f\\\"\\\\nüîç Sample entities:\\\")\\n        for entity in test_results['entities_sample']:\\n            models = '+'.join(entity.get('models_detected', [entity.get('model_source')]))\\n            is_merged = \\\" (merged)\\\" if entity.get('is_merged') else \\\"\\\"\\n            print(f\\\"   ‚Ä¢ {entity['entity_type']}: '{entity['entity_text']}' ({models}{is_merged}, {entity['confidence_score']:.3f})\\\")\\n        \\n    else:\\n        print(f\\\"   ‚ùå Pipeline test failed\\\")\\n        \\nelse:\\n    print(\\\"üì≠ No test documents available\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Multi-Model Entity Storage ready!\\\")\\nprint(f\\\"üéØ Usage: result = process_and_store_documents(documents)\\\")\\nprint(f\\\"üìä Query: entities = storage.query_entities_by_models(['biobert', 'roberta'])\\\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Multi-Model Batch Processing and Advanced Analytics\n\ndef run_multi_model_batch_extraction(batch_size: int = 3, max_filings: int = None) -> Dict[str, Any]:\n    \"\"\"Run batch entity extraction with multi-model pipeline\"\"\"\n    \n    print(f\"üöÄ Starting multi-model batch entity extraction...\")\n    print(f\"   üì¶ Batch size: {batch_size}\")\n    print(f\"   üìä Max filings: {max_filings or 'unlimited'}\")\n    print(f\"   ü§ñ Active models: {list(ner_pipeline.models.keys()) if 'ner_pipeline' in globals() else 'Not loaded'}\")\n    \n    # Get filings to process\n    filings_to_process = universal_parser.get_documents_from_database(limit=max_filings or 50)\n    \n    if not filings_to_process:\n        print(\"üì≠ No unprocessed filings found\")\n        return {'success': False, 'message': 'No filings to process'}\n    \n    print(f\"   üìÑ Found {len(filings_to_process)} filings to process\")\n    \n    # Initialize batch tracking with multi-model stats\n    batch_results = {\n        'total_filings': len(filings_to_process),\n        'successful_filings': 0,\n        'failed_filings': 0,\n        'total_entities_extracted': 0,\n        'merged_entities': 0,\n        'single_model_entities': 0,\n        'model_usage': {},\n        'entity_categories': {},\n        'processing_start_time': datetime.now(),\n        'results': []\n    }\n    \n    # Process filings in batches\n    for i in range(0, len(filings_to_process), batch_size):\n        batch_filings = filings_to_process[i:i + batch_size]\n        batch_number = (i // batch_size) + 1\n        total_batches = (len(filings_to_process) + batch_size - 1) // batch_size\n        \n        print(f\"\\\\nüì¶ Processing batch {batch_number}/{total_batches} ({len(batch_filings)} filings)\")\n        \n        try:\n            # Process batch through complete pipeline\n            batch_result = process_and_store_documents(batch_filings, store_results=True)\n            \n            if batch_result['success']:\n                batch_results['successful_filings'] += batch_result['documents_processed']\n                batch_results['total_entities_extracted'] += batch_result['entities_extracted']\n                \n                # Track multi-model statistics\n                for entity in batch_result.get('entities_sample', []):\n                    # Count model usage\n                    primary_model = entity.get('primary_model', 'unknown')\n                    batch_results['model_usage'][primary_model] = batch_results['model_usage'].get(primary_model, 0) + 1\n                    \n                    # Count entity categories\n                    entity_type = entity.get('entity_type', 'unknown')\n                    batch_results['entity_categories'][entity_type] = batch_results['entity_categories'].get(entity_type, 0) + 1\n                    \n                    # Count merged vs single model\n                    if entity.get('is_merged', False):\n                        batch_results['merged_entities'] += 1\n                    else:\n                        batch_results['single_model_entities'] += 1\n                \n                print(f\"   ‚úÖ Batch {batch_number}: {batch_result['entities_extracted']} entities extracted\")\n            else:\n                batch_results['failed_filings'] += len(batch_filings)\n                print(f\"   ‚ùå Batch {batch_number}: Processing failed\")\n            \n            batch_results['results'].append(batch_result)\n            \n        except Exception as e:\n            batch_results['failed_filings'] += len(batch_filings)\n            error_result = {\n                'success': False,\n                'error': str(e),\n                'documents_processed': len(batch_filings)\n            }\n            batch_results['results'].append(error_result)\n            print(f\"   ‚ùå Batch {batch_number}: Exception: {e}\")\n        \n        # Brief pause between batches to prevent overwhelming\n        if i + batch_size < len(filings_to_process):\n            print(\"   ‚è∏Ô∏è Brief pause between batches...\")\n            time.sleep(3)\n    \n    # Finalize results\n    batch_results['processing_end_time'] = datetime.now()\n    batch_results['total_processing_time'] = str(batch_results['processing_end_time'] - batch_results['processing_start_time'])\n    batch_results['success_rate'] = batch_results['successful_filings'] / batch_results['total_filings'] if batch_results['total_filings'] > 0 else 0\n    \n    return batch_results\n\ndef generate_multi_model_extraction_report(batch_results: Dict[str, Any] = None) -> None:\n    \"\"\"Generate comprehensive multi-model extraction report\"\"\"\n    print(\"\\\\n\" + \"=\"*70)\n    print(\"üìä MULTI-MODEL SEC ENTITY EXTRACTION REPORT\")\n    print(\"=\"*70)\n    \n    # Current database status with multi-model analysis\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Enhanced database statistics\n        cursor.execute('''\\n            SELECT \\n                COUNT(*) as total_entities,\\n                COUNT(DISTINCT company_domain) as companies_processed,\\n                COUNT(DISTINCT sec_filing_ref) as filings_processed,\\n                COUNT(DISTINCT entity_category) as entity_types,\\n                AVG(confidence_score) as avg_confidence,\\n                COUNT(*) FILTER (WHERE is_merged = true) as merged_entities,\\n                COUNT(*) FILTER (WHERE is_merged = false) as single_model_entities\\n            FROM system_uno.sec_entities_raw\\n            WHERE models_detected IS NOT NULL\\n        ''')\\n        \\n        db_stats = cursor.fetchone()\\n        \\n        # Model usage breakdown\\n        cursor.execute('''\\n            SELECT primary_model, COUNT(*) as count, AVG(confidence_score) as avg_conf\\n            FROM system_uno.sec_entities_raw\\n            WHERE primary_model IS NOT NULL\\n            GROUP BY primary_model\\n            ORDER BY count DESC\\n        ''')\\n        \\n        model_breakdown = cursor.fetchall()\\n        \\n        # Entity category breakdown\\n        cursor.execute('''\\n            SELECT entity_category, COUNT(*) as count, AVG(confidence_score) as avg_conf,\\n                   COUNT(*) FILTER (WHERE is_merged = true) as merged_count\\n            FROM system_uno.sec_entities_raw\\n            GROUP BY entity_category\\n            ORDER BY count DESC\\n            LIMIT 10\\n        ''')\\n        \\n        entity_breakdown = cursor.fetchall()\\n        \\n        # Multi-model detection statistics\\n        cursor.execute('''\\n            SELECT \\n                array_length(models_detected, 1) as num_models,\\n                COUNT(*) as count\\n            FROM system_uno.sec_entities_raw\\n            WHERE models_detected IS NOT NULL\\n            GROUP BY array_length(models_detected, 1)\\n            ORDER BY num_models\\n        ''')\\n        \\n        multi_model_stats = cursor.fetchall()\\n        \\n        cursor.close()\\n        conn.close()\\n        \\n        print(f\\\"\\\\nüìà DATABASE STATISTICS:\\\")\\n        if db_stats and db_stats[0]:\\n            print(f\\\"   Total Entities Extracted: {db_stats[0]:,}\\\")\\n            print(f\\\"   Companies Processed: {db_stats[1]}\\\")\\n            print(f\\\"   SEC Filings Processed: {db_stats[2]}\\\")\\n            print(f\\\"   Entity Types Found: {db_stats[3]}\\\")\\n            print(f\\\"   Average Confidence: {db_stats[4]:.3f}\\\")\\n            print(f\\\"   Merged Entities: {db_stats[5]:,} ({(db_stats[5]/db_stats[0]*100):.1f}%)\\\")\\n            print(f\\\"   Single-Model Entities: {db_stats[6]:,} ({(db_stats[6]/db_stats[0]*100):.1f}%)\\\")\\n        \\n        if model_breakdown:\\n            print(f\\\"\\\\nü§ñ MODEL USAGE BREAKDOWN:\\\")\\n            for model_data in model_breakdown:\\n                print(f\\\"   {model_data[0]}: {model_data[1]:,} entities (avg conf: {model_data[2]:.3f})\\\")\\n        \\n        if entity_breakdown:\\n            print(f\\\"\\\\nüè∑Ô∏è ENTITY CATEGORY BREAKDOWN:\\\")\\n            for entity_data in entity_breakdown:\\n                merged_pct = (entity_data[3] / entity_data[1] * 100) if entity_data[1] > 0 else 0\\n                print(f\\\"   {entity_data[0]}: {entity_data[1]:,} entities (avg conf: {entity_data[2]:.3f}, {merged_pct:.1f}% merged)\\\")\\n        \\n        if multi_model_stats:\\n            print(f\\\"\\\\nüîó MULTI-MODEL DETECTION STATS:\\\")\\n            for stat in multi_model_stats:\\n                num_models = stat[0] or 1\\n                print(f\\\"   {num_models} model(s): {stat[1]:,} entities\\\")\\n                \\n    except Exception as e:\\n        print(f\\\"   ‚ùå Could not retrieve database statistics: {e}\\\")\\n    \\n    # Batch processing results\\n    if batch_results:\\n        print(f\\\"\\\\n‚ö° BATCH PROCESSING RESULTS:\\\")\\n        print(f\\\"   Total Filings Processed: {batch_results['total_filings']}\\\")\\n        print(f\\\"   Successful: {batch_results['successful_filings']} ({batch_results['success_rate']*100:.1f}%)\\\")\\n        print(f\\\"   Failed: {batch_results['failed_filings']}\\\")\\n        print(f\\\"   Total Entities Extracted: {batch_results['total_entities_extracted']:,}\\\")\\n        print(f\\\"   Merged Entities: {batch_results.get('merged_entities', 0):,}\\\")\\n        print(f\\\"   Single-Model Entities: {batch_results.get('single_model_entities', 0):,}\\\")\\n        print(f\\\"   Processing Time: {batch_results['total_processing_time']}\\\")\\n        \\n        if batch_results.get('model_usage'):\\n            print(f\\\"\\\\nüéØ BATCH MODEL USAGE:\\\")\\n            for model, count in batch_results['model_usage'].items():\\n                print(f\\\"   {model}: {count} entities\\\")\\n        \\n        if batch_results.get('entity_categories'):\\n            print(f\\\"\\\\nüìã BATCH ENTITY CATEGORIES:\\\")\\n            sorted_categories = sorted(batch_results['entity_categories'].items(), key=lambda x: x[1], reverse=True)\\n            for category, count in sorted_categories[:5]:\\n                print(f\\\"   {category}: {count} entities\\\")\\n    \\n    # Pipeline statistics\\n    if 'ner_pipeline' in globals() and ner_pipeline:\\n        summary = ner_pipeline.get_processing_summary()\\n        print(f\\\"\\\\nüîç PIPELINE STATISTICS:\\\")\\n        pipeline_stats = summary['pipeline_stats']\\n        print(f\\\"   Documents Processed: {pipeline_stats['documents_processed']:,}\\\")\\n        print(f\\\"   Total Entities Found: {pipeline_stats['total_entities_found']:,}\\\")\\n        print(f\\\"   Total Processing Time: {pipeline_stats['total_processing_time']:.2f} seconds\\\")\\n        print(f\\\"   Active Models: {', '.join(pipeline_stats['models_used'])}\\\")\\n        \\n        print(f\\\"\\\\nüìä INDIVIDUAL MODEL STATISTICS:\\\")\\n        for model_name, stats in summary['model_stats'].items():\\n            print(f\\\"   {model_name}:\\\")\\n            print(f\\\"      Chunks processed: {stats['chunks_processed']:,}\\\")\\n            print(f\\\"      Entities found: {stats['entities_found']:,}\\\")\\n            print(f\\\"      Processing time: {stats['processing_time']:.2f}s\\\")\\n    \\n    # Storage statistics\\n    if 'storage' in globals():\\n        storage_stats = storage.get_storage_summary()\\n        print(f\\\"\\\\nüíæ STORAGE STATISTICS:\\\")\\n        print(f\\\"   Entities Stored: {storage_stats['successful_inserts']:,}\\\")\\n        print(f\\\"   Merged Entities: {storage_stats['merged_entities']:,}\\\")\\n        print(f\\\"   Single-Model Entities: {storage_stats['single_model_entities']:,}\\\")\\n        print(f\\\"   Failed Inserts: {storage_stats['failed_inserts']:,}\\\")\\n    \\n    print(\\\"\\\\n\\\" + \\\"=\\\"*70)\\n    print(\\\"‚úÖ Multi-Model Extraction Report Complete!\\\")\\n    print(\\\"=\\\"*70)\\n\\n# ========== ADVANCED ANALYTICS FUNCTIONS ==========\\n\\ndef analyze_model_agreement(filing_ref: str = None, limit: int = 100) -> Dict:\\n    \\\"\\\"\\\"Analyze agreement between different models on entity detection\\\"\\\"\\\"\\n    try:\\n        conn = psycopg2.connect(**NEON_CONFIG)\\n        cursor = conn.cursor()\\n        \\n        base_query = \\\"\\\"\\\"\\n            SELECT entity_text, models_detected, all_confidences, is_merged\\n            FROM system_uno.sec_entities_raw\\n            WHERE models_detected IS NOT NULL\\n        \\\"\\\"\\\"\\n        \\n        params = []\\n        if filing_ref:\\n            base_query += \\\" AND sec_filing_ref = %s\\\"\\n            params.append(filing_ref)\\n        \\n        base_query += f\\\" ORDER BY confidence_score DESC LIMIT {limit}\\\"\\n        \\n        cursor.execute(base_query, params)\\n        results = cursor.fetchall()\\n        cursor.close()\\n        conn.close()\\n        \\n        analysis = {\\n            'total_entities': len(results),\\n            'agreement_stats': {},\\n            'model_pairs': {},\\n            'high_agreement_entities': [],\\n            'disagreement_entities': []\\n        }\\n        \\n        for entity_text, models, confidences_json, is_merged in results:\\n            num_models = len(models) if models else 1\\n            \\n            if num_models not in analysis['agreement_stats']:\\n                analysis['agreement_stats'][num_models] = 0\\n            analysis['agreement_stats'][num_models] += 1\\n            \\n            # Parse confidences\\n            try:\\n                confidences = json.loads(confidences_json) if confidences_json else {}\\n            except:\\n                confidences = {}\\n            \\n            # High agreement: multiple models with similar confidence\\n            if num_models > 1 and confidences:\\n                conf_values = list(confidences.values())\\n                if len(conf_values) > 1:\\n                    conf_std = np.std(conf_values)\\n                    if conf_std < 0.1:  # Low standard deviation = high agreement\\n                        analysis['high_agreement_entities'].append({\\n                            'entity': entity_text,\\n                            'models': models,\\n                            'confidences': confidences,\\n                            'std_dev': conf_std\\n                        })\\n                    else:\\n                        analysis['disagreement_entities'].append({\\n                            'entity': entity_text,\\n                            'models': models,\\n                            'confidences': confidences,\\n                            'std_dev': conf_std\\n                        })\\n        \\n        return analysis\\n        \\n    except Exception as e:\\n        print(f\\\"‚ùå Model agreement analysis failed: {e}\\\")\\n        return {}\\n\\ndef get_top_entities_by_model(model_name: str, limit: int = 10) -> List[Dict]:\\n    \\\"\\\"\\\"Get top entities detected by a specific model\\\"\\\"\\\"\\n    if 'storage' in globals():\\n        return storage.query_entities_by_models([model_name], limit)\\n    return []\\n\\n# ========== GENERATE INITIAL REPORT ==========\\ngenerate_multi_model_extraction_report()\\n\\n# ========== READY FOR BATCH PROCESSING ==========\\nprint(f\\\"\\\\nüéØ MULTI-MODEL BATCH PROCESSING COMMANDS:\\\")\\nprint(f\\\"   ‚Ä¢ Small batch: batch_results = run_multi_model_batch_extraction(batch_size=2, max_filings=5)\\\")\\nprint(f\\\"   ‚Ä¢ Medium batch: batch_results = run_multi_model_batch_extraction(batch_size=3, max_filings=10)\\\")\\nprint(f\\\"   ‚Ä¢ Analysis: agreement = analyze_model_agreement(limit=50)\\\")\\nprint(f\\\"   ‚Ä¢ Model query: entities = get_top_entities_by_model('biobert', 10)\\\")\\nprint(f\\\"   ‚Ä¢ New report: generate_multi_model_extraction_report(batch_results)\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Multi-Model SEC Entity Extraction Engine fully operational!\\\")\\nprint(f\\\"üöÄ Ready to process biotech SEC filings with 4 parallel NER models!\\\")\\nprint(f\\\"üîß Features: EdgarTools parsing, position-based merging, multi-model analytics\\\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}