{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 0: Package Installation and Consolidated Imports\n# \n# Purpose: Install packages and consolidate all imports following PEP 8 order\n# Initialize basic logging - all other setup in Cell 1\n\n# ============================================================================\n# PACKAGE INSTALLATION\n# ============================================================================\n\nprint(\"🔧 Installing required packages...\")\n!pip install edgartools transformers torch accelerate huggingface_hub requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n!pip install -U bitsandbytes --quiet\n!pip install psycopg2-binary --quiet\n!pip install accelerate --quiet\n\nprint(\"✅ All packages installed successfully\")\n\n# ============================================================================\n# CONSOLIDATED IMPORTS - ALL IMPORTS FOR THE NOTEBOOK\n# ============================================================================\n\n# Standard library imports (alphabetical order)\nimport importlib\nimport importlib.util\nimport json\nimport os\nimport pickle\nimport re\nimport signal\nimport sys\nimport time\nimport traceback\nimport uuid\nimport warnings\n\n# Standard library from imports (alphabetical order)\nfrom collections import OrderedDict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom functools import wraps\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Set, Tuple\n\n# Third-party imports (alphabetical order)\nimport edgar\nimport numpy as np\nimport psycopg2\nimport requests\nimport torch\n\n# Third-party from imports (alphabetical order by module)\nfrom bs4 import BeautifulSoup\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nfrom huggingface_hub import login\nfrom IPython import get_ipython\nfrom ipykernel.iostream import OutStream\nfrom psycopg2 import pool\nfrom psycopg2.extras import execute_values\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoModelForTokenClassification, \n    pipeline,\n    BitsAndBytesConfig\n)\n\n# Environment imports\nfrom kaggle_secrets import UserSecretsClient\n\n# ============================================================================\n# AUTO-LOGGER BOOTSTRAP (USING CONSOLIDATED IMPORTS)\n# ============================================================================\n\n# Get GitHub token for logger access\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n\nprint(\"🔧 Setting up consolidated imports and logger bootstrap...\")\n\n# Clone/update repo for logger access\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\nif os.path.exists(LOCAL_PATH):\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\nelse:\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n\n# Add to path\nif f'{LOCAL_PATH}/BizIntel' not in sys.path:\n    sys.path.insert(0, f'{LOCAL_PATH}/BizIntel')\n\n# Initialize logger with minimal setup\nlogger_path = f\"{LOCAL_PATH}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\nif os.path.exists(logger_path):\n    spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_path)\n    auto_logger = importlib.util.module_from_spec(spec)\n    sys.modules[\"auto_logger\"] = auto_logger\n    spec.loader.exec_module(auto_logger)\n    \n    # Simple logger setup - database manager will be provided by Cell 1\n    logger = None  # Will be properly initialized after Cell 1 runs\n    print(\"✅ Auto-logger module loaded\")\nelse:\n    logger = None\n    print(\"⚠️  Logger module not found - continuing without logging\")\n\nprint(\"✅ Cell 0: Package installation + all imports consolidated (33+ imports) + bootstrap complete\")\nprint(\"   📦 Standard library: importlib, json, os, pickle, re, signal, sys, time, etc.\")\nprint(\"   🔗 Third-party: edgar, numpy, psycopg2, requests, torch, transformers, bs4\")\nprint(\"   🌐 Environment: kaggle_secrets\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 1: GitHub Setup and Simplified Configuration\n\n# ============================================================================\n# GITHUB SETUP AND PATH CONFIGURATION\n# ============================================================================\n\nprint(\"🔄 Setting up GitHub repository...\")\n\n# GitHub configuration\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\n# Clone or update the repository\nif os.path.exists(LOCAL_PATH):\n    print(\"   📂 Repository exists, pulling latest changes...\")\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n    print(\"   ✅ Repository updated\")\nelse:\n    print(\"   📥 Cloning repository...\")\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n    print(\"   ✅ Repository cloned\")\n\n# Add paths for module imports\nbizintel_path = f'{LOCAL_PATH}/BizIntel'\nscripts_path = f'{LOCAL_PATH}/BizIntel/Scripts'\n\nif bizintel_path not in sys.path:\n    sys.path.insert(0, bizintel_path)\nif scripts_path not in sys.path:\n    sys.path.insert(0, scripts_path)\n\nprint(f\"   ✅ Added {bizintel_path} to Python path\")\nprint(f\"   ✅ Added {scripts_path} to Python path\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    SEC_FILINGS_PROMPT,\n    SizeLimitedLRUCache,\n    log_error,\n    log_warning, \n    log_info,\n    get_db_connection\n)\n\nprint(\"✅ Imported modular EntityExtractionEngine components\")\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION\n# ============================================================================\n\n# Neon database configuration (from secrets)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"), \n    'user': user_secrets.get_secret(\"NEON_USER\"),\n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'port': 5432,\n    'sslmode': 'require'\n}\n\n# Complete centralized configuration\nCONFIG = {\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': 'https://github.com/amiralpert/SmartReach.git',\n        'local_path': '/kaggle/working/SmartReach',\n        'branch': 'main'\n    },\n    'database': {\n        'connection_pool_size': 5,\n        'max_connections': 10,\n        'connection_timeout': 30,\n        'query_timeout': 60,\n        'retry_attempts': 3,\n        'batch_size': 100\n    },\n    'models': {\n        'confidence_threshold': 0.75,\n        'warm_up_enabled': True,\n        'warm_up_text': 'Test entity extraction with biotechnology company.',\n        'device_preference': 'auto',  # 'auto', 'cuda', 'cpu'\n        'model_timeout': 30\n    },\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 512,\n        'ttl_hours': 24,\n        'cleanup_interval': 3600\n    },\n    'processing': {\n        'filing_batch_size': 3,\n        'entity_batch_size': 50,\n        'max_section_length': 50000,\n        'enable_parallel': True,\n        'max_workers': 4,\n        'section_validation': True,\n        'filing_query_limit': 10,\n        'enable_relationships': True,\n        'relationship_batch_size': 15,\n        'context_window_chars': 400\n    },\n    'llama': {\n        'enabled': True,\n        'model_name': 'meta-llama/Llama-3.1-8B-Instruct',\n        'batch_size': 15,\n        'max_new_tokens': 50,\n        'context_window': 400,\n        'temperature': 0.3,\n        'entity_context_window': 400,\n        'test_max_tokens': 50,\n        'min_confidence_filter': 0.8,\n        'timeout_seconds': 30,\n        'SEC_FilingsPrompt': SEC_FILINGS_PROMPT,  # Now imported from module\n    },\n    'edgar': {\n        'identity': 'SmartReach BizIntel amir.alpert@gmail.com',\n        'rate_limit_delay': 0.1,\n        'max_retries': 3,\n        'timeout_seconds': 30\n    }\n}\n\n# Error checking for required secrets\nrequired_secrets = ['NEON_HOST', 'NEON_DATABASE', 'NEON_USER', 'NEON_PASSWORD', 'GITHUB_TOKEN']\nmissing_secrets = []\n\nfor secret in required_secrets:\n    try:\n        value = user_secrets.get_secret(secret)\n        if not value:\n            missing_secrets.append(secret)\n    except Exception as e:\n        missing_secrets.append(secret)\n\nif missing_secrets:\n    print(f\"❌ Missing required secrets: {missing_secrets}\")\n    print(\"   Please add these secrets in Kaggle's Settings > Secrets\")\n    raise ValueError(\"Missing required secrets\")\n\nprint(\"✅ All required secrets validated\")\n\n# Configuration validation and display\nprint(\"\\n🔧 Configuration Summary:\")\nprint(f\"   • Database: {NEON_CONFIG['host']} / {NEON_CONFIG['database']}\")\nprint(f\"   • Models: {len(['biobert', 'bert', 'roberta', 'finbert'])} NER models + Llama 3.1-8B\")\nprint(f\"   • Processing: {CONFIG['processing']['filing_batch_size']} filings/batch\")\nprint(f\"   • Cache: {CONFIG['cache']['max_size_mb']}MB limit\")\nprint(f\"   • Relationships: {'Enabled' if CONFIG['processing']['enable_relationships'] else 'Disabled'}\")\n\n# ============================================================================\n# SIMPLIFIED REAL-TIME CONSOLE LOGGING SYSTEM\n# ============================================================================\n\nclass RealTimeKaggleLogger:\n    def __init__(self, cell_number):\n        self.cell_number = cell_number\n        self.original_stdout = sys.stdout\n        self.original_stderr = sys.stderr\n        \n    def write(self, text):\n        # Write to original console immediately\n        self.original_stdout.write(text)\n        self.original_stdout.flush()\n        \n        # Save to simplified database table (non-blocking)\n        if text.strip():  # Only log non-empty lines\n            try:\n                with get_db_connection(NEON_CONFIG) as conn:\n                    with conn.cursor() as cursor:\n                        cursor.execute(\"\"\"\n                            INSERT INTO core.console_logs (cell_number, console_output) \n                            VALUES (%s, %s)\n                        \"\"\", (self.cell_number, text.strip()))\n                        conn.commit()\n            except:\n                pass  # Don't let logging errors break execution\n                \n    def flush(self):\n        self.original_stdout.flush()\n\ndef start_cell_logging(cell_number):\n    \"\"\"Start real-time console logging for a cell\"\"\"\n    logger = RealTimeKaggleLogger(cell_number)\n    sys.stdout = logger\n    sys.stderr = logger\n    \n    # Log cell start\n    print(f\"=== CELL {cell_number} START ===\")\n\n# Make it globally available\nglobals()['start_cell_logging'] = start_cell_logging\n\nprint(\"✅ Simplified real-time console logging system ready\")\n\n# ============================================================================\n# INITIALIZE COMPONENTS\n# ============================================================================\n\n# Initialize global cache for section extraction using imported class\nSECTION_CACHE = SizeLimitedLRUCache(max_size_mb=CONFIG['cache']['max_size_mb'])\n\n# Create database connection function with NEON_CONFIG\ndef get_db_connection_configured():\n    \"\"\"Database connection using our configuration\"\"\"\n    return get_db_connection(NEON_CONFIG)\n\n# ============================================================================\n# MODULE CLEARING AND EDGARTOOLS SETUP\n# ============================================================================\n\nprint(\"\\n🧹 Clearing modules and setting up EdgarTools...\")\n\n# Clear any existing modules to ensure fresh imports\nmodules_to_clear = [mod for mod in sys.modules.keys() if 'SmartReach' in mod]\nfor module in modules_to_clear:\n    del sys.modules[module]\n\n# Configure EdgarTools identity\nset_identity(CONFIG['edgar']['identity'])\nprint(f\"   ✅ EdgarTools identity set: {CONFIG['edgar']['identity']}\")\n\n# ============================================================================\n# FINAL INITIALIZATION MESSAGES\n# ============================================================================\n\n# Start logging for Cell 1\nstart_cell_logging(1)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎉 CELL 1 INITIALIZATION COMPLETE\")\nprint(\"=\"*80)\n\nprint(f\"✅ GitHub repository ready at: {LOCAL_PATH}\")\nprint(f\"✅ Database connection configured: {NEON_CONFIG['host']}\")\nprint(f\"✅ Configuration loaded with {len(CONFIG)} main sections\")\nprint(f\"✅ Modular components imported from EntityExtractionEngine\")\nprint(f\"✅ Size-limited cache initialized: {CONFIG['cache']['max_size_mb']}MB limit\")\nprint(f\"✅ EdgarTools identity configured\")\nprint(f\"✅ Logging functions available: log_error, log_warning, log_info\")\nprint(f\"✅ Database context manager available: get_db_connection_configured()\")\nprint(f\"✅ Llama 3.1-8B relationship extraction prompt configured\")\nprint(f\"✅ Simplified console logging initialized - using core.console_logs table\")\n\nprint(f\"\\n🚀 Ready to proceed to Cell 2 for EdgarTools section extraction!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:10:05.069606Z",
     "iopub.status.busy": "2025-09-13T18:10:05.069316Z",
     "iopub.status.idle": "2025-09-13T18:10:12.336615Z",
     "shell.execute_reply": "2025-09-13T18:10:12.336382Z",
     "shell.execute_reply.started": "2025-09-13T18:10:05.069581Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 2: Database Functions and ORM-like Models with Batching - SIMPLIFIED\n\n# Start real-time console logging for this cell\nstart_cell_logging(2)\n\n# Basic startup check - restart kernel if issues persist\nprint(\"Starting Cell 2 - EdgarTools section extraction\")\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    TimeoutError,\n    get_filing_sections,\n    route_sections_to_models, \n    process_sec_filing_with_sections,\n    get_unprocessed_filings\n)\n\nprint(\"✅ Imported EdgarTools processing components from EntityExtractionEngine\")\n\n# ============================================================================\n# WRAPPER FUNCTIONS FOR CONFIGURED COMPONENTS\n# ============================================================================\n\ndef get_filing_sections_configured(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get filing sections using global configuration and cache\"\"\"\n    return get_filing_sections(accession_number, filing_type, SECTION_CACHE, CONFIG)\n\ndef process_sec_filing_configured(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing using global configuration and cache\"\"\"\n    return process_sec_filing_with_sections(filing_data, SECTION_CACHE, CONFIG)\n\ndef get_unprocessed_filings_configured(limit: int = 5) -> List[Dict]:\n    \"\"\"Get unprocessed filings using configured database connection\"\"\"\n    return get_unprocessed_filings(get_db_connection_configured, limit)\n\n# ============================================================================\n# TESTING AND VALIDATION\n# ============================================================================\n\n# Test the simplified extraction with timeout protection\nlog_info(\"Test\", \"Starting section extraction test with timeout protection\")\n\ntest_filings = get_unprocessed_filings_configured(limit=1)\n\nif test_filings:\n    print(f\"\\n🧪 Testing with filing: {test_filings[0]['company_domain']} - {test_filings[0]['filing_type']}\")\n    print(f\"   Accession: {test_filings[0]['accession_number']}\")\n    \n    test_result = process_sec_filing_configured(test_filings[0])\n    \n    if test_result['processing_status'] == 'success':\n        log_info(\"Test\", f\"✅ Successfully extracted {test_result['total_sections']} sections\")\n    elif test_result['processing_status'] == 'timeout':\n        log_warning(\"Test\", f\"⏱️ Processing timed out - filing may be too large or slow\")\n    elif test_result['processing_status'] == 'skipped':\n        log_info(\"Test\", f\"⏭️ Skipped problematic filing\")\n    else:\n        log_error(\"Test\", f\"❌ Section extraction failed: {test_result.get('error')}\")\nelse:\n    log_info(\"Test\", \"No test filings available (all may be processed or problematic)\")\n\nprint(\"✅ Cell 2 complete - EdgarTools section extraction with timeout protection ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:10:12.338823Z",
     "iopub.status.busy": "2025-09-13T18:10:12.338593Z",
     "iopub.status.idle": "2025-09-13T18:10:58.440614Z",
     "shell.execute_reply": "2025-09-13T18:10:58.440277Z",
     "shell.execute_reply.started": "2025-09-13T18:10:12.338803Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 3: Optimized Entity Extraction Pipeline - Uses Cell 2's Pre-Extracted Sections\n\n# Start real-time console logging for this cell\nstart_cell_logging(3)\n\nprint(\"🚀 Loading Optimized EntityExtractionPipeline (Handler Classes Eliminated)...\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import EntityExtractionPipeline\n\nprint(\"✅ Imported EntityExtractionPipeline from EntityExtractionEngine\")\n\n# ============================================================================\n# INITIALIZE PIPELINE\n# ============================================================================\n\n# Initialize the entity extraction pipeline\nentity_pipeline = EntityExtractionPipeline(CONFIG)\n\nprint(f\"✅ EntityExtractionPipeline initialized:\")\nstats = entity_pipeline.get_extraction_stats()\nfor key, value in stats.items():\n    print(f\"   • {key}: {value}\")\n\n# ============================================================================\n# WRAPPER FUNCTION FOR CONFIGURED PROCESSING\n# ============================================================================\n\ndef process_filing_entities_configured(filing_data: Dict) -> List[Dict]:\n    \"\"\"Process filing entities using configured pipeline and Cell 2 functions\"\"\"\n    return entity_pipeline.process_filing_entities(filing_data, process_sec_filing_configured)\n\nprint(\"✅ Cell 3 complete - Optimized entity extraction ready (handler classes eliminated)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 4: Relationship Extractor with Local Llama 3.1-8B - MODULARIZED\n\n# Start real-time console logging for this cell\nstart_cell_logging(4)\n\nprint(\"🦙 Loading Relationship Extractor with Local Llama 3.1-8B...\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    RelationshipExtractor,\n    SemanticRelationshipStorage,\n    PipelineEntityStorage,\n    process_filings_batch,\n    generate_pipeline_analytics_report\n)\n\nprint(\"✅ Imported relationship processing components from EntityExtractionEngine\")\n\n# ============================================================================\n# INITIALIZE GLOBAL OBJECTS\n# ============================================================================\n\nprint(\"🔧 Initializing pipeline components...\")\n\n# Initialize relationship extraction and storage components\nrelationship_extractor = RelationshipExtractor(CONFIG)\nsemantic_storage = SemanticRelationshipStorage(CONFIG['database'])\npipeline_storage = PipelineEntityStorage(CONFIG['database'])\n\nprint(\"✅ Pipeline components initialized:\")\nprint(f\"   🦙 Llama model status: {'✅ Loaded' if relationship_extractor.model else '❌ Failed'}\")\nprint(f\"   💾 Storage systems: ✅ Entity & ✅ Relationship storage initialized\")\n\n# ============================================================================\n# WRAPPER FUNCTIONS FOR CONFIGURED PROCESSING\n# ============================================================================\n\ndef process_filings_batch_configured(limit: int = None) -> Dict:\n    \"\"\"Process multiple SEC filings using configured pipeline components\"\"\"\n    return process_filings_batch(\n        entity_pipeline, relationship_extractor, pipeline_storage, \n        semantic_storage, CONFIG, limit\n    )\n\nprint(\"✅ Cell 4 complete - Relationship extraction and storage ready\")\nprint(f\"   🎯 Batch processing: process_filings_batch_configured() function ready\")\nprint(f\"   📊 Analytics: generate_pipeline_analytics_report() function ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:14:18.239404Z",
     "iopub.status.busy": "2025-09-13T18:14:18.239084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 5: Main Processing Pipeline with Relationship Extraction - MODULARIZED\n\n# Start real-time console logging for this cell\nstart_cell_logging(5)\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import execute_main_pipeline\n\nprint(\"✅ Imported main pipeline orchestrator from EntityExtractionEngine\")\n\n# ============================================================================\n# EXECUTE MAIN PIPELINE\n# ============================================================================\n\n# Execute the complete SEC filing processing pipeline\nresults = execute_main_pipeline(\n    entity_pipeline, \n    relationship_extractor, \n    pipeline_storage, \n    semantic_storage, \n    CONFIG\n)\n\nprint(\"✅ Cell 5 complete - Main pipeline execution finished\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}