{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0: Minimal Auto-Logger Bootstrap\n# \n# Purpose: Initialize basic logging ONLY - all other setup in Cell 1\n# This cell runs first to enable logging for subsequent cells\n\nimport os\nimport sys\nimport importlib.util\n\n# Get GitHub token for logger access\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n\nprint(\"ğŸ”§ Setting up minimal logger bootstrap...\")\n\n# Clone/update repo for logger access\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\nif os.path.exists(LOCAL_PATH):\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\nelse:\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n\n# Add to path\nif f'{LOCAL_PATH}/BizIntel' not in sys.path:\n    sys.path.insert(0, f'{LOCAL_PATH}/BizIntel')\n\n# Initialize logger with minimal setup\nlogger_path = f\"{LOCAL_PATH}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\nif os.path.exists(logger_path):\n    spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_path)\n    auto_logger = importlib.util.module_from_spec(spec)\n    sys.modules[\"auto_logger\"] = auto_logger\n    spec.loader.exec_module(auto_logger)\n    \n    # Simple logger setup - database manager will be provided by Cell 1\n    logger = None  # Will be properly initialized after Cell 1 runs\n    print(\"âœ… Auto-logger module loaded\")\nelse:\n    logger = None\n    print(\"âš ï¸  Logger module not found - continuing without logging\")\n\nprint(\"âœ… Cell 0 bootstrap complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T17:30:43.958699Z","iopub.execute_input":"2025-09-14T17:30:43.958999Z","iopub.status.idle":"2025-09-14T17:30:44.380822Z","shell.execute_reply.started":"2025-09-14T17:30:43.958972Z","shell.execute_reply":"2025-09-14T17:30:44.379948Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Setting up minimal logger bootstrap...\nâœ… Auto-logger module loaded - database setup in Cell 1\nâœ… Cell 0 bootstrap complete - run Cell 1 for full setup\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 1: GitHub Setup and Enhanced Import Configuration\n\n# Install required packages first\n!pip install edgartools transformers torch accelerate huggingface_hub requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n!pip install -U bitsandbytes --quiet\n\nimport os\nimport sys\nimport importlib\nimport importlib.util\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom psycopg2 import pool\nimport time\nimport json\nimport pickle\nimport traceback\nfrom pathlib import Path\nfrom functools import wraps\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime\nfrom edgar import set_identity\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION - All settings in one place\n# ============================================================================\n\n# Use Kaggle secrets for all sensitive credentials\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n# Database Configuration (using Kaggle secrets for security)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"),\n    'user': user_secrets.get_secret(\"NEON_USER\"), \n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'sslmode': 'require'\n}\n\n# Master Configuration Dictionary - PHASE 3 ENHANCED\nCONFIG = {\n    # GitHub Settings\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': f\"https://{user_secrets.get_secret('GITHUB_TOKEN')}@github.com/amiralpert/SmartReach.git\",\n        'local_path': \"/kaggle/working/SmartReach\"\n    },\n    \n    # Database Settings\n    'database': NEON_CONFIG,\n    \n    # Connection Pool Settings - PHASE 3\n    'pool': {\n        'min_connections': 2,\n        'max_connections': 10,\n        'keepalives': 1,\n        'keepalives_idle': 30,\n        'keepalives_interval': 10,\n        'keepalives_count': 5\n    },\n    \n    # Connection Retry Settings\n    'retry': {\n        'max_attempts': 3,\n        'initial_delay': 1,  # seconds\n        'exponential_base': 2,\n        'max_delay': 30  # seconds\n    },\n    \n    # Model Configuration - PHASE 3 ENHANCED\n    'models': {\n        'confidence_threshold': 0.8, #confidence NER model needs to store an entity \n        'batch_size': 16, # number of text chunks to process at one time through NER\n        'max_length': 512, #max chunk size to process through NER 512 token limit for BERT models \n        'chunk_overlap': 0.1,  # 10% overlap between chunks for complete entity extraction\n        'warm_up_enabled': True,  # PHASE 3: Model warm-up\n        'warm_up_text': 'Pfizer announced FDA approval for new cancer drug targeting BRCA mutations.'\n    },\n    \n    # Cache Settings - PHASE 2 ENHANCED\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 100,  # Maximum cache size in MB\n        'ttl': 3600,  # seconds\n        'eviction_policy': 'LRU'  # Least Recently Used\n    },\n    \n    # Processing Settings - PHASE 3 ENHANCED  \n    'processing': {\n        'filing_batch_size': 1,\n        'filing_query_limit': 1,       # Explicit limit for get_unprocessed_filings()\n        'enable_relationships': True,   # Enable/disable relationship extraction\n        'entity_batch_size': 10000,    # Max entities per database insert\n        'section_validation': True,    # Enforce section name validation\n        'debug_mode': False,\n        'max_insert_batch': 50000,     # Maximum batch for database inserts\n        'deprecation_warnings': True,  # Show warnings for deprecated functions\n        'checkpoint_enabled': True,    # PHASE 3: Enable checkpointing\n        'checkpoint_dir': '/kaggle/working/checkpoints',  # PHASE 3: Checkpoint directory\n        'deduplication_threshold': 0.85  # PHASE 3: Similarity threshold for dedup\n    },\n    \n    # Llama 3.1 Configuration - CENTRALIZED FOR OPTIMIZATION\n    'llama': {\n        'enabled': True,\n        'model_name': 'meta-llama/Llama-3.1-8B-Instruct',\n        'batch_size': 15,              # Entities per Llama call (for future batching)\n        'max_new_tokens': 50,          # Reduced from 200 for speed\n        'context_window': 400,         # Reduced from 1000 chars for speed  \n        'temperature': 0.3,            # Sampling temperature\n        'entity_context_window': 400,  # Reduced from 500 chars for entity context\n        'test_max_tokens': 50,         # For model testing\n        'min_confidence_filter': 0.8,  # Entity filtering threshold\n        'timeout_seconds': 30,         # Timeout for model calls\n    },\n    \n        # EdgarTools Settings\n    'edgar': {\n        'identity': \"SmartReach BizIntel amir@leanbio.consulting\"\n    }\n}\n\nif not CONFIG['github']['token']:\n    raise ValueError(\"âŒ GITHUB_TOKEN is required in Kaggle secrets\")\n\nif not CONFIG['database']['password']:\n    raise ValueError(\"âŒ NEON_PASSWORD is required in Kaggle secrets\")\n\nprint(\"âœ… Configuration loaded from Kaggle secrets\")\nprint(f\"   Database: {CONFIG['database']['host']}\")\nprint(f\"   Processing: Filing batch={CONFIG['processing']['filing_batch_size']}, Query limit={CONFIG['processing']['filing_query_limit']}\")\nprint(f\"   Llama 3.1: Enabled={CONFIG['llama']['enabled']}, Tokens={CONFIG['llama']['max_new_tokens']}, Context={CONFIG['llama']['context_window']}\")\nprint(f\"   Cache: Max size={CONFIG['cache']['max_size_mb']}MB, TTL={CONFIG['cache']['ttl']}s\")\nprint(f\"   Relationships: {'Enabled' if CONFIG['processing']['enable_relationships'] else 'Disabled'}\")\n\n# ============================================================================\n# CONNECTION RETRY DECORATOR\n# ============================================================================\n\ndef retry_on_connection_error(func):\n    \"\"\"Decorator to retry database operations on connection errors\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        max_attempts = CONFIG['retry']['max_attempts']\n        delay = CONFIG['retry']['initial_delay']\n        \n        for attempt in range(max_attempts):\n            try:\n                return func(*args, **kwargs)\n            except (psycopg2.OperationalError, psycopg2.InterfaceError) as e:\n                if attempt == max_attempts - 1:\n                    error_msg = f\"ERROR [ConnectionRetry]: Failed after {max_attempts} attempts - {type(e).__name__}: {str(e)}\"\n                    print(error_msg)\n                if 'logger' in globals() and logger:\n                    logger.log(error_msg)\n                raise\n                \n                wait_time = min(delay * (CONFIG['retry']['exponential_base'] ** attempt), CONFIG['retry']['max_delay'])\n                print(f\"WARNING [ConnectionRetry]: Attempt {attempt + 1}/{max_attempts} failed. Retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n        \n        return None\n    return wrapper\n\n# ============================================================================\n# PHASE 3: ENHANCED ERROR LOGGING WITH STACK TRACES\n# ============================================================================\n\ndef log_error(component: str, message: str, exception: Exception = None, context: dict = None):\n    \"\"\"Enhanced error logging with stack traces\"\"\"\n    if exception:\n        error_msg = f\"ERROR [{component}]: {message} - {type(exception).__name__}: {str(exception)}\"\n        # Add stack trace for debugging\n        if CONFIG['processing'].get('debug_mode', False):\n            error_msg += f\"\\nStack trace:\\n{traceback.format_exc()}\"\n    else:\n        error_msg = f\"ERROR [{component}]: {message}\"\n    \n    if context:\n        error_msg += f\" | Context: {context}\"\n    \n    print(error_msg)  # Auto-logger captures this\n    return error_msg\n\ndef log_warning(component: str, message: str, context: dict = None):\n    \"\"\"Standardized warning logging\"\"\"\n    warning_msg = f\"WARNING [{component}]: {message}\"\n    if context:\n        warning_msg += f\" | Context: {context}\"\n    print(warning_msg)\n    return warning_msg\n\ndef log_info(component: str, message: str):\n    \"\"\"Standardized info logging\"\"\"\n    info_msg = f\"INFO [{component}]: {message}\"\n    print(info_msg)\n    return info_msg\n\n# ============================================================================\n# PHASE 3: ENHANCED DATABASE MANAGER WITH CONNECTION POOLING\n# ============================================================================\n\nclass DatabaseManager:\n    \"\"\"Singleton database manager with connection pooling\"\"\"\n    \n    _instance = None\n    _pool = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(DatabaseManager, cls).__new__(cls)\n        return cls._instance\n    \n    def __init__(self):\n        if DatabaseManager._pool is None:\n            self._initialize_pool()\n    \n    def _initialize_pool(self):\n        \"\"\"Initialize connection pool\"\"\"\n        try:\n            DatabaseManager._pool = psycopg2.pool.ThreadedConnectionPool(\n                CONFIG['pool']['min_connections'],\n                CONFIG['pool']['max_connections'],\n                **CONFIG['database'],\n                keepalives=CONFIG['pool']['keepalives'],\n                keepalives_idle=CONFIG['pool']['keepalives_idle'],\n                keepalives_interval=CONFIG['pool']['keepalives_interval'],\n                keepalives_count=CONFIG['pool']['keepalives_count']\n            )\n            log_info(\"DatabaseManager\", f\"Connection pool initialized with {CONFIG['pool']['max_connections']} max connections\")\n        except Exception as e:\n            log_error(\"DatabaseManager\", \"Failed to initialize connection pool\", e)\n            raise\n    \n    @contextmanager\n    def get_connection(self):\n        \"\"\"Get connection from pool with automatic return\"\"\"\n        conn = None\n        try:\n            conn = DatabaseManager._pool.getconn()\n            yield conn\n            conn.commit()\n        except Exception as e:\n            if conn:\n                conn.rollback()\n            raise e\n        finally:\n            if conn:\n                DatabaseManager._pool.putconn(conn)\n    \n    def close_all(self):\n        \"\"\"Close all connections in pool\"\"\"\n        if DatabaseManager._pool:\n            DatabaseManager._pool.closeall()\n            log_info(\"DatabaseManager\", \"All connections closed\")\n    \n    def get_pool_status(self) -> Dict:\n        \"\"\"Get current pool status\"\"\"\n        if DatabaseManager._pool:\n            return {\n                'minconn': DatabaseManager._pool.minconn,\n                'maxconn': DatabaseManager._pool.maxconn,\n                'closed': DatabaseManager._pool.closed\n            }\n        return {'status': 'not initialized'}\n\n# Initialize global database manager\nDB_MANAGER = DatabaseManager()\n\n# PHASE 2: Keep backward compatibility\n@contextmanager\ndef get_db_connection():\n    \"\"\"Legacy context manager - now uses DatabaseManager\"\"\"\n    with DB_MANAGER.get_connection() as conn:\n        yield conn\n\n# ============================================================================\n# PHASE 3: CHECKPOINT MANAGER FOR FAILURE RECOVERY\n# ============================================================================\n\nclass CheckpointManager:\n    \"\"\"Manage pipeline checkpoints for failure recovery\"\"\"\n    \n    def __init__(self, checkpoint_dir: str = None):\n        self.checkpoint_dir = Path(checkpoint_dir or CONFIG['processing']['checkpoint_dir'])\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.current_checkpoint = None\n        log_info(\"CheckpointManager\", f\"Initialized with directory: {self.checkpoint_dir}\")\n    \n    def save_checkpoint(self, state: Dict, checkpoint_name: str = None) -> str:\n        \"\"\"Save pipeline state to checkpoint\"\"\"\n        try:\n            if not checkpoint_name:\n                checkpoint_name = f\"checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            \n            checkpoint_path = self.checkpoint_dir / f\"{checkpoint_name}.pkl\"\n            \n            # Add metadata\n            state['checkpoint_metadata'] = {\n                'created_at': datetime.now().isoformat(),\n                'pipeline_version': '3.0',\n                'config_hash': hash(str(CONFIG))\n            }\n            \n            with open(checkpoint_path, 'wb') as f:\n                pickle.dump(state, f)\n            \n            self.current_checkpoint = checkpoint_path\n            log_info(\"CheckpointManager\", f\"Saved checkpoint: {checkpoint_name}\")\n            return str(checkpoint_path)\n            \n        except Exception as e:\n            log_error(\"CheckpointManager\", \"Failed to save checkpoint\", e)\n            return None\n    \n    def load_checkpoint(self, checkpoint_path: str = None) -> Optional[Dict]:\n        \"\"\"Load pipeline state from checkpoint\"\"\"\n        try:\n            if not checkpoint_path:\n                checkpoint_path = self.current_checkpoint\n            \n            if not checkpoint_path:\n                # Find latest checkpoint\n                checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n                if not checkpoints:\n                    log_warning(\"CheckpointManager\", \"No checkpoints found\")\n                    return None\n                checkpoint_path = max(checkpoints, key=lambda p: p.stat().st_mtime)\n            \n            with open(checkpoint_path, 'rb') as f:\n                state = pickle.load(f)\n            \n            log_info(\"CheckpointManager\", f\"Loaded checkpoint: {Path(checkpoint_path).name}\")\n            return state\n            \n        except Exception as e:\n            log_error(\"CheckpointManager\", \"Failed to load checkpoint\", e)\n            return None\n    \n    def list_checkpoints(self) -> List[Dict]:\n        \"\"\"List available checkpoints\"\"\"\n        checkpoints = []\n        for cp_file in self.checkpoint_dir.glob(\"checkpoint_*.pkl\"):\n            try:\n                stats = cp_file.stat()\n                checkpoints.append({\n                    'name': cp_file.stem,\n                    'path': str(cp_file),\n                    'size_mb': stats.st_size / (1024 * 1024),\n                    'modified': datetime.fromtimestamp(stats.st_mtime).isoformat()\n                })\n            except:\n                continue\n        return sorted(checkpoints, key=lambda x: x['modified'], reverse=True)\n    \n    def cleanup_old_checkpoints(self, keep_last: int = 5):\n        \"\"\"Clean up old checkpoints\"\"\"\n        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n        checkpoints.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n        \n        for cp_file in checkpoints[keep_last:]:\n            try:\n                cp_file.unlink()\n                log_info(\"CheckpointManager\", f\"Deleted old checkpoint: {cp_file.name}\")\n            except:\n                continue\n\n# Initialize global checkpoint manager\nCHECKPOINT_MANAGER = CheckpointManager()\n\n# ============================================================================\n# PHASE 2: SIZE-LIMITED LRU CACHE\n# ============================================================================\n\nclass SizeLimitedLRUCache:\n    \"\"\"LRU cache with size limit in MB\"\"\"\n    \n    def __init__(self, max_size_mb: int):\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.cache = OrderedDict()\n        self.current_size = 0\n        self.hits = 0\n        self.misses = 0\n    \n    def _estimate_size(self, value: str) -> int:\n        \"\"\"Estimate size of cached value in bytes\"\"\"\n        return len(value.encode('utf-8')) if isinstance(value, str) else sys.getsizeof(value)\n    \n    def get(self, key: str):\n        \"\"\"Get item from cache\"\"\"\n        if key in self.cache:\n            self.hits += 1\n            # Move to end (most recently used)\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        self.misses += 1\n        return None\n    \n    def put(self, key: str, value, size: int = None):\n        \"\"\"Put item in cache with LRU eviction\"\"\"\n        if size is None:\n            size = self._estimate_size(value)\n        \n        # Remove old entries if needed\n        while self.current_size + size > self.max_size_bytes and self.cache:\n            evicted_key, evicted_value = self.cache.popitem(last=False)\n            self.current_size -= self._estimate_size(evicted_value)\n            log_info(\"Cache\", f\"Evicted {evicted_key} to maintain size limit\")\n        \n        # Add new entry\n        if key in self.cache:\n            self.current_size -= self._estimate_size(self.cache[key])\n        \n        self.cache[key] = value\n        self.current_size += size\n        self.cache.move_to_end(key)\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get cache statistics\"\"\"\n        hit_rate = (self.hits / (self.hits + self.misses) * 100) if (self.hits + self.misses) > 0 else 0\n        return {\n            'entries': len(self.cache),\n            'size_mb': self.current_size / (1024 * 1024),\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': hit_rate\n        }\n\n# Initialize global cache for EdgarTools sections\nSECTION_CACHE = SizeLimitedLRUCache(CONFIG['cache']['max_size_mb'])\n\n# ============================================================================\n# PHASE 2: DEPRECATION WARNING SYSTEM\n# ============================================================================\n\ndef deprecated(replacement_func: str = None):\n    \"\"\"Decorator to mark functions as deprecated\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if CONFIG['processing']['deprecation_warnings']:\n                msg = f\"Function '{func.__name__}' is deprecated and will be removed.\"\n                if replacement_func:\n                    msg += f\" Use '{replacement_func}' instead.\"\n                log_warning(\"Deprecation\", msg)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# ============================================================================\n# GITHUB SETUP\n# ============================================================================\n\nprint(\"\\nğŸ“¦ Setting up GitHub repository...\")\nlocal_path = CONFIG['github']['local_path']\nrepo_url = CONFIG['github']['repo_url']\n\n# Clone or update repo with force pull\nif os.path.exists(local_path):\n    log_info(\"GitHub\", f\"Repository exists at {local_path}\")\n    log_info(\"GitHub\", \"Force updating from main branch\")\n    !cd {local_path} && git fetch origin\n    !cd {local_path} && git reset --hard origin/main\n    !cd {local_path} && git pull origin main\n    log_info(\"GitHub\", \"Repository updated successfully\")\n    \n    # Show current commit\n    !cd {local_path} && echo \"Current commit:\" && git log --oneline -1\nelse:\n    log_info(\"GitHub\", f\"Cloning repository to {local_path}\")\n    !git clone {repo_url} {local_path}\n    log_info(\"GitHub\", \"Repository cloned successfully\")\n\n# Clear any cached modules from previous runs\nmodules_to_clear = [key for key in sys.modules.keys() if 'auto_logger' in key.lower() or 'clean' in key.lower()]\nfor mod in modules_to_clear:\n    del sys.modules[mod]\n    log_info(\"ModuleCache\", f\"Cleared cached module: {mod}\")\n\n# Add to Python path for regular imports\nif f'{local_path}/BizIntel' in sys.path:\n    sys.path.remove(f'{local_path}/BizIntel')\nsys.path.insert(0, f'{local_path}/BizIntel')\n\nlog_info(\"Setup\", \"Python path configured for SEC entity extraction\")\n\n# Configure EdgarTools authentication - REQUIRED by SEC\nset_identity(CONFIG['edgar']['identity'])\nlog_info(\"EdgarTools\", f\"Identity configured: {CONFIG['edgar']['identity']}\")\n\nprint(\"ğŸš€ SEC ENTITY EXTRACTION ENGINE INITIALIZED - PHASE 3 PRODUCTION-READY\")\nprint(\"=\"*80)\nprint(f\"âœ… GitHub: Repository at {CONFIG['github']['local_path']}\")\nprint(f\"âœ… Database: Connected to {CONFIG['database']['host']}\")\nprint(f\"âœ… EdgarTools: Configured as '{CONFIG['edgar']['identity']}'\")\nprint(f\"âœ… Logging: {'Enabled' if logger else 'Disabled'}\")\nprint(f\"âœ… Section Validation: {'ENFORCED' if CONFIG['processing']['section_validation'] else 'Disabled'}\")\nprint(f\"âœ… PHASE 2 Enhancements:\")\nprint(f\"   â€¢ Database context manager: get_db_connection()\")\nprint(f\"   â€¢ Size-limited LRU cache: {CONFIG['cache']['max_size_mb']}MB\")\nprint(f\"   â€¢ Batch size limits: Max {CONFIG['processing']['max_insert_batch']} per insert\")\nprint(f\"   â€¢ Deprecation warnings: {'Enabled' if CONFIG['processing']['deprecation_warnings'] else 'Disabled'}\")\nprint(f\"âœ… PHASE 3 PRODUCTION FEATURES:\")\nprint(f\"   â€¢ Connection Pool: {CONFIG['pool']['max_connections']} max connections\")\nprint(f\"   â€¢ Checkpoint System: Recovery from failures enabled\")\nprint(f\"   â€¢ Enhanced Error Logging: Stack traces included\")\nprint(f\"   â€¢ Model Warm-up: Enabled for faster first inference\")\nprint(f\"   â€¢ Entity Deduplication: {CONFIG['processing']['deduplication_threshold']} threshold\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:08:25.345299Z","iopub.execute_input":"2025-09-13T18:08:25.345519Z","iopub.status.idle":"2025-09-13T18:10:05.068123Z","shell.execute_reply.started":"2025-09-13T18:08:25.345500Z","shell.execute_reply":"2025-09-13T18:10:05.067761Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hâœ… Configuration loaded from Kaggle secrets\n   Database: ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech\n   Processing: Batch size=1, Section validation=True\n   Cache: Max size=100MB, TTL=3600s\n   Database batching: Max insert batch=50000\n   Checkpointing: Enabled\nINFO [DatabaseManager]: Connection pool initialized with 10 max connections\nINFO [CheckpointManager]: Initialized with directory: /kaggle/working/checkpoints\n\nğŸ“¦ Setting up GitHub repository...\nINFO [GitHub]: Repository exists at /kaggle/working/SmartReach\nINFO [GitHub]: Force updating from main branch\nHEAD is now at 84c1ff3 Fix cell completion blocking - remove threading from capture\nFrom https://github.com/amiralpert/SmartReach\n * branch            main       -> FETCH_HEAD\nAlready up to date.\nINFO [GitHub]: Repository updated successfully\nCurrent commit:\n\u001b[33m84c1ff3\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m Fix cell completion blocking - remove threading from capture\nINFO [ModuleCache]: Cleared cached module: auto_logger\nINFO [Setup]: Python path configured for SEC entity extraction\nINFO [EdgarTools]: Identity configured: SmartReach BizIntel amir@leanbio.consulting\nğŸš€ SEC ENTITY EXTRACTION ENGINE INITIALIZED - PHASE 3 PRODUCTION-READY\n================================================================================\nâœ… GitHub: Repository at /kaggle/working/SmartReach\nâœ… Database: Connected to ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech\nâœ… EdgarTools: Configured as 'SmartReach BizIntel amir@leanbio.consulting'\nâœ… Logging: Enabled\nâœ… Section Validation: ENFORCED\nâœ… PHASE 2 Enhancements:\n   â€¢ Database context manager: get_db_connection()\n   â€¢ Size-limited LRU cache: 100MB\n   â€¢ Batch size limits: Max 50000 per insert\n   â€¢ Deprecation warnings: Enabled\nâœ… PHASE 3 PRODUCTION FEATURES:\n   â€¢ Connection Pool: 10 max connections\n   â€¢ Checkpoint System: Recovery from failures enabled\n   â€¢ Enhanced Error Logging: Stack traces included\n   â€¢ Model Warm-up: Enabled for faster first inference\n   â€¢ Entity Deduplication: 0.85 threshold\n================================================================================\nStarting cell 2\nINFO [Test]: Starting section extraction test with timeout protection\nINFO [DatabaseQuery]: Retrieved 1 unprocessed filings (excluded 1 problematic)\n\nğŸ§ª Testing with filing: grail.com - 8-K\n   Accession: 0001699031-25-000120\nINFO [FilingProcessor]: Processing 8-K for grail.com\n   ğŸ“„ Filing ID: 436\n   ğŸ“‘ Accession: 0001699031-25-000120\n[18:10:05] Starting EdgarTools find() for 0001699031-25-000120\n[18:10:12] find() completed successfully\nINFO [EdgarTools]: Found 8-K for GRAIL, Inc.\n[18:10:12] Starting html() fetch...\n[18:10:12] html() completed, size: 34,498 bytes\n[18:10:12] Starting HTML parsing...\n[18:10:12] HTML parsing completed\nINFO [EdgarTools]: SectionExtractor found 0 sections\nWARNING [EdgarTools]: No structured sections found, using full document fallback\nINFO [EdgarTools]: Using full document: 5,371 chars\nINFO [Cache]: Cached sections for 0001699031-25-000120 (1 sections)\nINFO [FilingProcessor]: Extracted 1 sections\n   ğŸ¯ Model routing: ['biobert: 1 sections', 'bert_base: 1 sections', 'roberta: 1 sections', 'finbert: 1 sections']\nINFO [Test]: âœ… Successfully extracted 1 sections\nâœ… Cell 2 complete - EdgarTools section extraction with timeout protection ready\n","output_type":"stream"},{"name":"stderr","text":"2025-09-13 18:10:26.380699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757787026.586030      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757787026.646647      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ğŸš€ Loading Enhanced EntityExtractionPipeline with Quality Control...\nğŸ”§ Enhanced EntityExtractionPipeline initialized with quality control\nğŸ“¦ Loading 3 NER models with handlers...\n   ğŸ§  Loading biobert: alvaroalon2/biobert_diseases_ner\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"      âœ“ biobert loaded with BioBERTHandler\n   ğŸ§  Loading bert_base: dslim/bert-base-NER\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"      âœ“ bert_base loaded with BERTHandler\n   ğŸ§  Loading roberta: Jean-Baptiste/roberta-large-ner-english\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"      âœ“ roberta loaded with RoBERTaHandler\n   âœ… Successfully loaded: 3 models\n\nâœ… Enhanced EntityExtractionPipeline ready!\n   ğŸ¯ Loaded models: ['biobert', 'bert_base', 'roberta']\n   ğŸ” Quality control: Enabled\n   ğŸ“Š Consensus requirement: 2 models\n   ğŸ’» Device: GPU\nğŸš€ Loading In-Memory Pipeline with Storage and Local Llama 3.1-8B...\n   âœ… Database tables verified/created\n   ğŸ” Logging in to HuggingFace...\n   âœ… Logged in to HuggingFace\n   âš™ï¸ Configuring 4-bit quantization...\n   ğŸ“¥ Loading Llama 3.1-8B-Instruct (this may take a minute)...\n   âœ… Llama 3.1-8B loaded successfully (4-bit quantized)\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2479: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"   ğŸ§ª Test response: system\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\nuser\n\nWhat is a partnership? ...\n\nâœ… In-Memory Pipeline Components Ready!\n   ğŸ¯ RelationshipExtractor with Llama 3.1\n   ğŸ’¾ Atomic storage for entities + relationships\n   ğŸš€ In-memory processing (no DB round-trips)\n   ğŸ“Š Usage: batch_results = process_filings_batch(limit=5)\n   ğŸ§ª Test: test_result = test_pipeline()\n================================================================================\nğŸš€ STARTING SEC FILING PROCESSING PIPELINE\n================================================================================\nğŸ“ Relationship extraction enabled with local Llama 3.1-8B\n   â„¹ï¸ Using local Llama 3.1-8B for relationship extraction\n\nğŸ“Š Checking for unprocessed filings...\nINFO [DatabaseQuery]: Retrieved 10 unprocessed filings (excluded 1 problematic)\n   Found 10 unprocessed filings\n\nğŸ“‹ Available filings to process:\n   1. grail.com - 8-K (2025-05-16)\n   2. grail.com - 10-Q (2025-05-14)\n   3. grail.com - 8-K (2025-05-13)\n   4. exactsciences.com - 10-Q (2025-05-01)\n   5. exactsciences.com - 8-K (2025-05-01)\n\nğŸ”„ Processing 3 filings...\n------------------------------------------------------------\n\nğŸš€ Processing batch of 3 SEC filings with in-memory pipeline...\nINFO [DatabaseQuery]: Retrieved 3 unprocessed filings (excluded 1 problematic)\nğŸ“Š Found 3 filings to process\n\n[1/3] Processing 8-K for grail.com\n\nğŸ“„ Processing 8-K for grail.com\nINFO [FilingProcessor]: Processing 8-K for grail.com\n   ğŸ“„ Filing ID: 436\n   ğŸ“‘ Accession: 0001699031-25-000120\nINFO [Cache]: Cache hit for 0001699031-25-000120\nINFO [FilingProcessor]: Extracted 1 sections\n   ğŸ¯ Model routing: ['biobert: 1 sections', 'bert_base: 1 sections', 'roberta: 1 sections', 'finbert: 1 sections']\n   ğŸ“Š Cache: 50.0% hit rate, 0.0MB used\nğŸ” Processing 1 sections with quality control\n   ğŸ“‘ Processing 'full_document'\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"      â€¢ biobert: 4 quality entities\n      â€¢ bert_base: 47 quality entities\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/pipelines/token_classification.py:398: UserWarning: Tokenizer does not support real words, using fallback heuristic\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"      â€¢ roberta: 33 quality entities\n   âœ… Extracted 48 high-quality entities\n   ğŸ“Š Filtered out 0 low-quality entities\n   ğŸ” Extracted 48 entities\n   ğŸ’¾ Storing 48 entities to database...\n      âœ… Entities stored: 48 entities\n         â€¢ Merged entities: 0, Single-model: 48\n   ğŸ¤– Starting Llama 3.1 relationship extraction (this may take several minutes)...\n   ğŸ” Analyzing relationships for grail.com\n      ğŸ¯ Filtered: 48 â†’ 44 entities (removed 4 non-business entities)\n      ğŸ¯ Total entities to analyze: 44\n      ğŸ“‘ Processing 44 entities in 'full_document'\n         ğŸ”¬ Analyzing: death\n         ğŸ”¬ Analyzing: D. C\n         â³ Progress: 10/44 entities (22%)\n         ğŸ”¬ Analyzing: Act\n         ğŸ”¬ Analyzing: Compensation Committee\n         â³ Progress: 20/44 entities (45%)\n         ğŸ”¬ Analyzing: Company\n         â±ï¸  Estimated time remaining: 3.0 minutes\n         ğŸ”¬ Analyzing: Company\n         â³ Progress: 30/44 entities (68%)\n         ğŸ”¬ Analyzing: Company\n         ğŸ”¬ Analyzing: Inc.\n         â³ Progress: 40/44 entities (90%)\n         ğŸ”¬ Analyzing: NEO\n   âœ… Found 6 relationships from 44 entities\n   âŒ Failed: 'PipelineEntityStorage' object has no attribute 'store_relationships'\n\n[2/3] Processing 10-Q for grail.com\n\nğŸ“„ Processing 10-Q for grail.com\nINFO [FilingProcessor]: Processing 10-Q for grail.com\n   ğŸ“„ Filing ID: 426\n   ğŸ“‘ Accession: 0001699031-25-000114\n[18:21:48] Starting EdgarTools find() for 0001699031-25-000114\n[18:21:48] find() completed successfully\nINFO [EdgarTools]: Found 10-Q for GRAIL, Inc.\n[18:21:48] Starting html() fetch...\n[18:21:49] html() completed, size: 1,095,462 bytes\n[18:21:49] Starting HTML parsing...\n[18:21:50] HTML parsing completed\nINFO [EdgarTools]: SectionExtractor found 6 sections\n      â€¢ financial_statements: 35,496 chars\n      â€¢ mda: 61,408 chars\n      â€¢ market_risk: 2,112 chars\n      â€¢ controls_procedures: 1,608 chars\n      â€¢ legal_proceedings: 226 chars\n      â€¢ risk_factors: 37,453 chars\nINFO [Cache]: Cached sections for 0001699031-25-000114 (6 sections)\nINFO [FilingProcessor]: Extracted 6 sections\n   ğŸ¯ Model routing: ['biobert: 5 sections', 'bert_base: 5 sections', 'roberta: 5 sections', 'finbert: 1 sections']\n   ğŸ“Š Cache: 33.3% hit rate, 0.0MB used\nğŸ” Processing 6 sections with quality control\n   ğŸ“‘ Processing 'financial_statements'\n      â€¢ biobert: 3 quality entities\n      â€¢ bert_base: 277 quality entities\n      â€¢ roberta: 163 quality entities\n   ğŸ“‘ Processing 'mda'\n      â€¢ biobert: 24 quality entities\n      â€¢ bert_base: 287 quality entities\n      â€¢ roberta: 265 quality entities\n   ğŸ“‘ Processing 'market_risk'\n      â€¢ bert_base: 7 quality entities\n      â€¢ roberta: 6 quality entities\n   ğŸ“‘ Processing 'controls_procedures'\n      â€¢ bert_base: 3 quality entities\n      â€¢ roberta: 4 quality entities\n   ğŸ“‘ Processing 'legal_proceedings'\n   ğŸ“‘ Processing 'risk_factors'\n      â€¢ biobert: 7 quality entities\n      â€¢ bert_base: 199 quality entities\n      â€¢ roberta: 217 quality entities\n   âœ… Extracted 364 high-quality entities\n   ğŸ“Š Filtered out 557 low-quality entities\n   ğŸ” Extracted 364 entities\n   ğŸ’¾ Storing 364 entities to database...\n      âœ… Entities stored: 64 entities\n         â€¢ Merged entities: 0, Single-model: 364\n   ğŸ¤– Starting Llama 3.1 relationship extraction (this may take several minutes)...\n   ğŸ” Analyzing relationships for grail.com\n      ğŸ¯ Filtered: 364 â†’ 296 entities (removed 68 non-business entities)\n      ğŸ¯ Total entities to analyze: 296\n      ğŸ“‘ Processing 88 entities in 'financial_statements'\n         ğŸ”¬ Analyzing: of Contents GRAIL, Inc\n         ğŸ”¬ Analyzing: Nasdaq Stock Exchange\n         â³ Progress: 10/296 entities (3%)\n         ğŸ”¬ Analyzing: Matters Agreement\n         ğŸ”¬ Analyzing: Internal Revenue\n         â³ Progress: 20/296 entities (6%)\n         ğŸ”¬ Analyzing: Award Plan\n         â±ï¸  Estimated time remaining: 44.2 minutes\n         ğŸ”¬ Analyzing: Nasdaq Listing Rule\n         â³ Progress: 30/296 entities (10%)\n         ğŸ”¬ Analyzing: GRAIL,\n         ğŸ”¬ Analyzing: United States District Court\n         â³ Progress: 40/296 entities (13%)\n         ğŸ”¬ Analyzing: Illumina, Inc\n         ğŸ”¬ Analyzing: Universal - Investment - Gesellschaft\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Database Functions and ORM-like Models with Batching - FIXED HANGING ISSUE\n\n# ============================================================================\n# FIX STDOUT/STDERR AFTER CELL CANCELLATION\n# ============================================================================\n# This fixes the \"I/O operation on closed file\" error when restarting after cancel\n\nimport sys\ntry:\n    # Test if stdout is working\n    sys.stdout.write('')\n    sys.stdout.flush()\nexcept (ValueError, AttributeError, OSError):\n    # stdout is broken, need to restore it\n    try:\n        from IPython import get_ipython\n        from ipykernel.iostream import OutStream\n        \n        ip = get_ipython()\n        if ip and hasattr(ip, 'kernel'):\n            # Restore stdout and stderr using kernel's output streams\n            sys.stdout = OutStream(ip.kernel.session, ip.kernel.iopub_socket, 'stdout')\n            sys.stderr = OutStream(ip.kernel.session, ip.kernel.iopub_socket, 'stderr')\n            print(\"âœ… Restored stdout/stderr after cancellation\")\n    except Exception as e:\n        import warnings\n        warnings.warn(f\"Could not restore output streams: {e}. You may need to restart the kernel.\")\n\n# Now safe to continue with Cell 2\nimport edgar\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nimport requests\nimport re\nimport signal\nfrom functools import wraps\nfrom typing import Dict, List, Optional, Tuple\nfrom bs4 import BeautifulSoup\n\nprint(\"Starting cell 2\")\n\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# ============================================================================\n# TIMEOUT HANDLER FOR EDGARTOOLS API CALLS - FIX FOR HANGING\n# ============================================================================\n\nclass TimeoutError(Exception):\n    \"\"\"Custom timeout exception\"\"\"\n    pass\n\ndef timeout_handler(signum, frame):\n    \"\"\"Signal handler for timeout\"\"\"\n    raise TimeoutError(\"EdgarTools API call timed out\")\n\ndef with_timeout(seconds=30):\n    \"\"\"Decorator to add timeout to functions\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Set the signal alarm\n            signal.signal(signal.SIGALRM, timeout_handler)\n            signal.alarm(seconds)\n            try:\n                result = func(*args, **kwargs)\n            finally:\n                # Disable the alarm\n                signal.alarm(0)\n            return result\n        return wrapper\n    return decorator\n\n# ============================================================================\n# PROBLEMATIC FILINGS TO SKIP\n# ============================================================================\n\n# Known problematic filings that cause indefinite hangs\nPROBLEMATIC_FILINGS = [\n    '0001699031-25-000166',  # Grail 10-Q that caused 11+ hour hang\n]\n\n# ============================================================================\n# TIMEOUT-WRAPPED EDGARTOOLS CALLS\n# ============================================================================\n\n@with_timeout(30)  # 30 second timeout\ndef find_filing_with_timeout(accession_number: str):\n    \"\"\"Find filing with timeout protection\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting EdgarTools find() for {accession_number}\")\n    filing = find(accession_number)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] find() completed successfully\")\n    return filing\n\n@with_timeout(60)  # 60 second timeout for HTML download\ndef get_html_with_timeout(filing):\n    \"\"\"Get HTML content with timeout protection\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting html() fetch...\")\n    html_content = filing.html()\n    if html_content:\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] html() completed, size: {len(html_content):,} bytes\")\n    else:\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] html() returned empty content\")\n    return html_content\n\n@with_timeout(30)  # 30 second timeout for parsing\ndef parse_html_with_timeout(html_content):\n    \"\"\"Parse HTML with timeout protection\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting HTML parsing...\")\n    document = parse_html(html_content)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] HTML parsing completed\")\n    return document\n\ndef get_filing_sections(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get structured sections from SEC filing using accession number\n    \n    ENHANCED: With timeouts, progress monitoring, and problematic filing skipping\n    \"\"\"\n    # Skip known problematic filings\n    if accession_number in PROBLEMATIC_FILINGS:\n        log_warning(\"EdgarTools\", f\"Skipping known problematic filing: {accession_number}\")\n        return {}\n    \n    # Check cache first\n    cache_key = f\"{accession_number}#{filing_type or 'UNKNOWN'}\"\n    cached_sections = SECTION_CACHE.get(cache_key)\n    if cached_sections:\n        log_info(\"Cache\", f\"Cache hit for {accession_number}\")\n        return cached_sections\n    \n    try:\n        # Find filing with timeout protection\n        try:\n            filing = find_filing_with_timeout(accession_number)\n        except TimeoutError:\n            log_error(\"EdgarTools\", f\"Timeout finding filing {accession_number} (30s exceeded)\")\n            return {}\n        \n        if not filing:\n            raise ValueError(f\"Filing not found for accession: {accession_number}\")\n            \n        # Auto-detect filing type if not provided\n        if not filing_type:\n            filing_type = getattr(filing, 'form', '10-K')\n        \n        log_info(\"EdgarTools\", f\"Found {filing_type} for {getattr(filing, 'company', 'Unknown Company')}\")\n        \n        # Get structured HTML content with timeout\n        try:\n            html_content = get_html_with_timeout(filing)\n        except TimeoutError:\n            log_error(\"EdgarTools\", f\"Timeout fetching HTML for {accession_number} (60s exceeded)\")\n            return {}\n        \n        if not html_content:\n            raise ValueError(\"No HTML content available\")\n        \n        # Limit HTML size to prevent memory issues\n        MAX_HTML_SIZE = 10 * 1024 * 1024  # 10MB limit\n        if len(html_content) > MAX_HTML_SIZE:\n            log_warning(\"EdgarTools\", f\"HTML too large ({len(html_content):,} bytes), truncating to {MAX_HTML_SIZE:,}\")\n            html_content = html_content[:MAX_HTML_SIZE]\n        \n        # Parse HTML to Document object with timeout\n        try:\n            document = parse_html_with_timeout(html_content)\n        except TimeoutError:\n            log_error(\"EdgarTools\", f\"Timeout parsing HTML for {accession_number} (30s exceeded)\")\n            return {}\n        \n        # Extract sections using SectionExtractor\n        extractor = SectionExtractor(filing_type=filing_type)\n        sections = extractor.extract(document)\n        \n        log_info(\"EdgarTools\", f\"SectionExtractor found {len(sections)} sections\")\n        \n        # Convert sections to text dictionary\n        section_texts = {}\n        for section_name, section in sections.items():\n            try:\n                if hasattr(section, 'text'):\n                    text = section.text() if callable(section.text) else section.text\n                    if isinstance(text, str) and text.strip():\n                        section_texts[section_name] = text.strip()\n                        print(f\"      â€¢ {section_name}: {len(text):,} chars\")\n                elif hasattr(section, '__str__'):\n                    text = str(section).strip()\n                    if text:\n                        section_texts[section_name] = text\n                        print(f\"      â€¢ {section_name}: {len(text):,} chars (via str)\")\n            except Exception as section_e:\n                log_warning(\"EdgarTools\", f\"Could not extract section {section_name}\", {\"error\": str(section_e)})\n                continue\n        \n        # If SectionExtractor returns no sections, fall back to full document text\n        if not section_texts:\n            log_warning(\"EdgarTools\", \"No structured sections found, using full document fallback\")\n            full_text = document.text() if hasattr(document, 'text') and callable(document.text) else str(document)\n            if full_text and len(full_text.strip()) > 100:  # Only use if substantial content\n                # Limit full document size\n                if len(full_text) > MAX_HTML_SIZE:\n                    log_warning(\"EdgarTools\", f\"Full document too large ({len(full_text):,} chars), truncating\")\n                    full_text = full_text[:MAX_HTML_SIZE]\n                section_texts['full_document'] = full_text.strip()\n                log_info(\"EdgarTools\", f\"Using full document: {len(full_text):,} chars\")\n        \n        # Cache the result\n        if section_texts and CONFIG['cache']['enabled']:\n            SECTION_CACHE.put(cache_key, section_texts)\n            log_info(\"Cache\", f\"Cached sections for {accession_number} ({len(section_texts)} sections)\")\n        \n        return section_texts\n        \n    except Exception as e:\n        log_error(\"EdgarTools\", f\"Failed to fetch filing {accession_number}\", e)\n        return {}  # Return empty dict on network/API failure\n\ndef route_sections_to_models(sections: Dict[str, str], filing_type: str) -> Dict[str, List[str]]:\n    \"\"\"Route sections to appropriate NER models based on filing type\"\"\"\n    routing = {\n        'biobert': [],\n        'bert_base': [],\n        'roberta': [],\n        'finbert': []\n    }\n    \n    if filing_type.upper() in ['10-K', '10-Q']:\n        for section_name, section_text in sections.items():\n            # FinBERT gets financial statements exclusively\n            if 'financial' in section_name.lower() or 'statement' in section_name.lower():\n                routing['finbert'].append(section_name)\n            else:\n                # All other sections go to BERT/RoBERTa/BioBERT\n                routing['bert_base'].append(section_name)\n                routing['roberta'].append(section_name)\n                routing['biobert'].append(section_name)\n    \n    elif filing_type.upper() == '8-K':\n        # 8-K: all item sections go to all four models\n        for section_name in sections.keys():\n            routing['biobert'].append(section_name)\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['finbert'].append(section_name)\n    \n    else:\n        # Default routing for other filing types\n        for section_name in sections.keys():\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['biobert'].append(section_name)\n    \n    # Remove empty routing\n    routing = {model: sections_list for model, sections_list in routing.items() if sections_list}\n    \n    return routing\n\ndef process_sec_filing_with_sections(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing with section-based extraction\n    \n    ENHANCED: With timeout protection and progress monitoring\n    \"\"\"\n    try:\n        filing_id = filing_data.get('id')\n        accession_number = filing_data.get('accession_number')  # DIRECT FROM DATABASE\n        filing_type = filing_data.get('filing_type', '10-K')\n        company_domain = filing_data.get('company_domain', 'Unknown')\n        filing_url = filing_data.get('url')  # Still keep for reference\n        \n        log_info(\"FilingProcessor\", f\"Processing {filing_type} for {company_domain}\")\n        print(f\"   ğŸ“„ Filing ID: {filing_id}\")\n        print(f\"   ğŸ“‘ Accession: {accession_number}\")\n        \n        # Validate accession number\n        if not accession_number:\n            raise ValueError(f\"Missing accession number for filing {filing_id}\")\n        \n        # Check if this is a problematic filing\n        if accession_number in PROBLEMATIC_FILINGS:\n            log_warning(\"FilingProcessor\", f\"Skipping problematic filing: {accession_number}\")\n            return {\n                'filing_id': filing_id,\n                'company_domain': company_domain,\n                'filing_type': filing_type,\n                'accession_number': accession_number,\n                'error': 'Skipped - known problematic filing',\n                'processing_status': 'skipped'\n            }\n        \n        # Get structured sections using accession directly\n        sections = get_filing_sections(accession_number, filing_type)\n        if not sections:\n            raise ValueError(\"No sections extracted\")\n        \n        log_info(\"FilingProcessor\", f\"Extracted {len(sections)} sections\")\n        \n        # Route sections to models\n        model_routing = route_sections_to_models(sections, filing_type)\n        print(f\"   ğŸ¯ Model routing: {[f'{model}: {len(secs)} sections' for model, secs in model_routing.items()]}\")\n        \n        # Validate section names if configured\n        if CONFIG['processing']['section_validation']:\n            missing_sections = [name for name in sections.keys() if not name]\n            if missing_sections:\n                log_warning(\"FilingProcessor\", f\"Found {len(missing_sections)} sections without names\")\n        \n        # Show cache statistics\n        cache_stats = SECTION_CACHE.get_stats()\n        if cache_stats['hits'] > 0:\n            print(f\"   ğŸ“Š Cache: {cache_stats['hit_rate']:.1f}% hit rate, {cache_stats['size_mb']:.1f}MB used\")\n        \n        return {\n            'filing_id': filing_id,\n            'company_domain': company_domain,\n            'filing_type': filing_type,\n            'accession_number': accession_number,\n            'url': filing_url,\n            'sections': sections,\n            'model_routing': model_routing,\n            'total_sections': len(sections),\n            'processing_status': 'success'\n        }\n        \n    except TimeoutError as e:\n        log_error(\"FilingProcessor\", \"Filing processing timed out\", e, \n                 {\"filing_id\": filing_data.get('id'), \"accession\": filing_data.get('accession_number')})\n        return {\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain', 'Unknown'),\n            'filing_type': filing_data.get('filing_type', 'Unknown'),\n            'accession_number': filing_data.get('accession_number'),\n            'error': 'Processing timeout',\n            'processing_status': 'timeout'\n        }\n    except Exception as e:\n        log_error(\"FilingProcessor\", \"Filing processing failed\", e, \n                 {\"filing_id\": filing_data.get('id'), \"accession\": filing_data.get('accession_number')})\n        return {\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain', 'Unknown'),\n            'filing_type': filing_data.get('filing_type', 'Unknown'),\n            'accession_number': filing_data.get('accession_number'),\n            'error': str(e),\n            'processing_status': 'failed'\n        }\n\n@retry_on_connection_error\ndef get_unprocessed_filings(limit: int = 5) -> List[Dict]:\n    \"\"\"Get SEC filings that haven't been processed yet\n    \n    ENHANCED: Skip known problematic filings\n    \"\"\"\n    with get_db_connection() as conn:  # PHASE 2: Using context manager\n        cursor = conn.cursor()\n        \n        # Build exclusion list for SQL\n        exclusion_list = \"', '\".join(PROBLEMATIC_FILINGS)\n        exclusion_clause = f\"AND sf.accession_number NOT IN ('{exclusion_list}')\" if PROBLEMATIC_FILINGS else \"\"\n        \n        cursor.execute(f\"\"\"\n            SELECT \n                sf.id, \n                sf.company_domain, \n                sf.filing_type, \n                sf.accession_number,\n                sf.url, \n                sf.filing_date, \n                sf.title\n            FROM raw_data.sec_filings sf\n            LEFT JOIN system_uno.sec_entities_raw ser \n                ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n            WHERE sf.accession_number IS NOT NULL  -- Must have accession\n                AND ser.sec_filing_ref IS NULL     -- Not yet processed\n                {exclusion_clause}                 -- Skip problematic filings\n            ORDER BY sf.filing_date DESC\n            LIMIT %s\n        \"\"\", (limit,))\n        \n        filings = cursor.fetchall()\n        cursor.close()\n        \n        log_info(\"DatabaseQuery\", f\"Retrieved {len(filings)} unprocessed filings (excluded {len(PROBLEMATIC_FILINGS)} problematic)\")\n        \n        return [{\n            'id': filing[0],\n            'company_domain': filing[1],\n            'filing_type': filing[2],\n            'accession_number': filing[3],\n            'url': filing[4],\n            'filing_date': filing[5],\n            'title': filing[6]\n        } for filing in filings]\n\n# Test the simplified extraction with timeout protection\nlog_info(\"Test\", \"Starting section extraction test with timeout protection\")\n\ntest_filings = get_unprocessed_filings(limit=1)\n\nif test_filings:\n    print(f\"\\nğŸ§ª Testing with filing: {test_filings[0]['company_domain']} - {test_filings[0]['filing_type']}\")\n    print(f\"   Accession: {test_filings[0]['accession_number']}\")\n    \n    test_result = process_sec_filing_with_sections(test_filings[0])\n    \n    if test_result['processing_status'] == 'success':\n        log_info(\"Test\", f\"âœ… Successfully extracted {test_result['total_sections']} sections\")\n    elif test_result['processing_status'] == 'timeout':\n        log_warning(\"Test\", f\"â±ï¸ Processing timed out - filing may be too large or slow\")\n    elif test_result['processing_status'] == 'skipped':\n        log_info(\"Test\", f\"â­ï¸ Skipped problematic filing\")\n    else:\n        log_error(\"Test\", f\"âŒ Section extraction failed: {test_result.get('error')}\")\nelse:\n    log_info(\"Test\", \"No test filings available (all may be processed or problematic)\")\n\nprint(\"âœ… Cell 2 complete - EdgarTools section extraction with timeout protection ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:10:05.069316Z","iopub.execute_input":"2025-09-13T18:10:05.069606Z","iopub.status.idle":"2025-09-13T18:10:12.336615Z","shell.execute_reply.started":"2025-09-13T18:10:05.069581Z","shell.execute_reply":"2025-09-13T18:10:12.336382Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Cell 3: Enhanced Entity Extraction Pipeline with Quality Control\n\nimport torch\nimport time\nimport uuid\nimport json\nimport re\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional, Set, Tuple\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport numpy as np\n\nprint(\"ğŸš€ Loading Enhanced EntityExtractionPipeline with Quality Control...\")\n\nclass ModelHandler:\n    \"\"\"Base class for model-specific entity handlers\"\"\"\n    \n    def __init__(self, model_name: str, pipeline_obj):\n        self.model_name = model_name\n        self.pipeline = pipeline_obj\n        self.stats = {\n            'texts_processed': 0,\n            'entities_found': 0,\n            'entities_filtered': 0,\n            'processing_time': 0\n        }\n    \n    def process_chunk(self, text: str, chunk_start: int = 0) -> List[Dict]:\n        \"\"\"Process a text chunk and return raw entities\"\"\"\n        raw_entities = self.pipeline(text)\n        \n        # Adjust positions to full text coordinates\n        for entity in raw_entities:\n            entity[\"start\"] = entity.get(\"start\", 0) + chunk_start\n            entity[\"end\"] = entity.get(\"end\", 0) + chunk_start\n        \n        return raw_entities\n    \n    def filter_entities(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Apply model-specific filtering\"\"\"\n        return entities\n    \n    def score_entity(self, entity: Dict) -> float:\n        \"\"\"Calculate quality score for an entity\"\"\"\n        score = entity.get('score', 0.5)\n        \n        # Penalize very short entities\n        text_len = len(entity.get('word', ''))\n        if text_len < 3:\n            score *= 0.5\n        elif text_len > 100:\n            score *= 0.7  # Likely a phrase/sentence, not an entity\n        \n        return score\n\n\nclass BioBERTHandler(ModelHandler):\n    \"\"\"Handler for BioBERT with medical entity focus\"\"\"\n    \n    def filter_entities(self, entities: List[Dict]) -> List[Dict]:\n        filtered = []\n        for entity in entities:\n            # Skip '0' category (non-medical text misclassified)\n            if entity.get('entity_group') == '0':\n                self.stats['entities_filtered'] += 1\n                continue\n            \n            # Skip if it looks like a full sentence\n            text = entity.get('word', '')\n            if len(text) > 150 or text.count('.') > 1:\n                self.stats['entities_filtered'] += 1\n                continue\n            \n            # Keep valid medical entities\n            filtered.append(entity)\n        \n        return filtered\n    \n    def score_entity(self, entity: Dict) -> float:\n        score = super().score_entity(entity)\n        \n        # Boost score for known medical patterns\n        text = entity.get('word', '').lower()\n        medical_keywords = ['disease', 'syndrome', 'cancer', 'therapy', 'drug', \n                          'treatment', 'clinical', 'trial', 'fda', 'phase']\n        \n        if any(keyword in text for keyword in medical_keywords):\n            score *= 1.2\n        \n        # Check for medical acronyms (3-5 uppercase letters)\n        if re.match(r'^[A-Z]{3,5}$', entity.get('word', '')):\n            score *= 1.1\n        \n        return min(score, 1.0)\n\n\nclass BERTHandler(ModelHandler):\n    \"\"\"Handler for BERT-base NER\"\"\"\n    \n    def filter_entities(self, entities: List[Dict]) -> List[Dict]:\n        filtered = []\n        for entity in entities:\n            # Filter out MISC entities that are too vague\n            if entity.get('entity_group') == 'MISC':\n                text = entity.get('word', '')\n                # Keep only if it's a clear entity (capitalized, reasonable length)\n                if len(text) < 3 or len(text) > 50 or not text[0].isupper():\n                    self.stats['entities_filtered'] += 1\n                    continue\n            \n            filtered.append(entity)\n        \n        return filtered\n    \n    def score_entity(self, entity: Dict) -> float:\n        score = super().score_entity(entity)\n        \n        # Boost score for clear entity types\n        entity_type = entity.get('entity_group', '')\n        if entity_type in ['PER', 'ORG', 'LOC']:\n            score *= 1.1\n        elif entity_type == 'MISC':\n            score *= 0.9\n        \n        return min(score, 1.0)\n\n\nclass RoBERTaHandler(ModelHandler):\n    \"\"\"Handler for RoBERTa with high precision focus\"\"\"\n    \n    def filter_entities(self, entities: List[Dict]) -> List[Dict]:\n        # RoBERTa is generally high precision, minimal filtering needed\n        return [e for e in entities if len(e.get('word', '')) >= 2]\n    \n    def score_entity(self, entity: Dict) -> float:\n        # RoBERTa typically has higher confidence, boost scores slightly\n        score = super().score_entity(entity) * 1.05\n        return min(score, 1.0)\n\n\nclass EntityExtractionPipeline:\n    \"\"\"Enhanced pipeline with quality control and model-specific handlers\"\"\"\n    \n    def __init__(self, db_config: Dict, model_config: Dict = None):\n        self.db_config = db_config\n        self.model_config = model_config or self._get_default_model_config()\n        self.model_handlers = {}\n        self.pipeline_stats = {\n            'documents_processed': 0,\n            'total_entities_extracted': 0,\n            'entities_filtered': 0,\n            'processing_time_total': 0\n        }\n        \n        print(f\"ğŸ”§ Enhanced EntityExtractionPipeline initialized with quality control\")\n    \n    def _get_default_model_config(self) -> Dict:\n        \"\"\"Model configuration with quality thresholds\"\"\"\n        return {\n            'models': {\n                'biobert': {\n                    'model_name': 'alvaroalon2/biobert_diseases_ner',\n                    'confidence_threshold': 0.6,  # Raised threshold\n                    'handler_class': BioBERTHandler,\n                    'description': 'Medical entities (diseases, treatments)'\n                },\n                'bert_base': {\n                    'model_name': 'dslim/bert-base-NER',\n                    'confidence_threshold': 0.5,\n                    'handler_class': BERTHandler,\n                    'description': 'General entities (persons, organizations)'\n                },\n                'roberta': {\n                    'model_name': 'Jean-Baptiste/roberta-large-ner-english',\n                    'confidence_threshold': 0.6,\n                    'handler_class': RoBERTaHandler,\n                    'description': 'High-precision general entities'\n                }\n            },\n            'quality_thresholds': {\n                'min_entity_length': 2,\n                'max_entity_length': 100,\n                'min_consensus_models': 2,  # Require 2+ models to agree\n                'min_quality_score': 0.5\n            },\n            'entity_type_mapping': {\n                # BioBERT mappings\n                'Disease': 'MEDICAL_CONDITION',\n                'Chemical': 'MEDICATION',\n                'DISEASE': 'MEDICAL_CONDITION',\n                'DRUG': 'MEDICATION',\n                \n                # BERT mappings\n                'PER': 'PERSON',\n                'ORG': 'ORGANIZATION',\n                'LOC': 'LOCATION',\n                'MISC': 'MISCELLANEOUS'\n            }\n        }\n    \n    def load_models(self) -> Dict:\n        \"\"\"Load NER models with specific handlers\"\"\"\n        print(f\"ğŸ“¦ Loading {len(self.model_config['models'])} NER models with handlers...\")\n        \n        for model_key, model_info in self.model_config['models'].items():\n            try:\n                model_name = model_info['model_name']\n                print(f\"   ğŸ§  Loading {model_key}: {model_name}\")\n                \n                # Skip FinBERT - it's for sentiment, not NER\n                if 'finbert' in model_key.lower():\n                    print(f\"      â­ï¸ Skipping FinBERT (sentiment model, not NER)\")\n                    continue\n                \n                tokenizer = AutoTokenizer.from_pretrained(model_name)\n                model = AutoModelForTokenClassification.from_pretrained(model_name)\n                ner_pipeline = pipeline(\n                    \"ner\", \n                    model=model, \n                    tokenizer=tokenizer,\n                    aggregation_strategy=\"average\",\n                    device=0 if torch.cuda.is_available() else -1\n                )\n                \n                # Create model-specific handler\n                handler_class = model_info.get('handler_class', ModelHandler)\n                handler = handler_class(model_key, ner_pipeline)\n                handler.confidence_threshold = model_info['confidence_threshold']\n                \n                self.model_handlers[model_key] = handler\n                \n                print(f\"      âœ“ {model_key} loaded with {handler_class.__name__}\")\n                \n            except Exception as e:\n                print(f\"      âŒ {model_key} failed: {e}\")\n        \n        print(f\"   âœ… Successfully loaded: {len(self.model_handlers)} models\")\n        return self.model_handlers\n    \n    def extract_entities_from_text(self, text: str, model_name: str) -> List[Dict]:\n        \"\"\"Extract entities using specific model handler\"\"\"\n        if model_name not in self.model_handlers or not text.strip():\n            return []\n        \n        try:\n            handler = self.model_handlers[model_name]\n            start_time = time.time()\n            \n            # Process in chunks\n            max_length = 512\n            overlap = 50\n            stride = max_length - overlap\n            \n            all_entities = []\n            for chunk_start in range(0, len(text), stride):\n                chunk_end = min(chunk_start + max_length, len(text))\n                chunk = text[chunk_start:chunk_end]\n                \n                # Get raw entities from chunk\n                chunk_entities = handler.process_chunk(chunk, chunk_start)\n                \n                # Apply model-specific filtering\n                filtered_entities = handler.filter_entities(chunk_entities)\n                \n                all_entities.extend(filtered_entities)\n                \n                if chunk_end >= len(text):\n                    break\n            \n            # Deduplicate entities\n            unique_entities = self._deduplicate_entities(all_entities)\n            \n            # Filter by confidence and quality\n            final_entities = []\n            for entity in unique_entities:\n                if entity['score'] >= handler.confidence_threshold:\n                    # Calculate quality score\n                    quality_score = handler.score_entity(entity)\n                    \n                    if quality_score >= self.model_config['quality_thresholds']['min_quality_score']:\n                        processed = {\n                            'extraction_id': str(uuid.uuid4()),\n                            'entity_text': entity['word'].strip(),\n                            'entity_type': self.model_config['entity_type_mapping'].get(\n                                entity['entity_group'], entity['entity_group']\n                            ),\n                            'confidence_score': float(entity['score']),\n                            'quality_score': quality_score,\n                            'char_start': entity['start'],\n                            'char_end': entity['end'],\n                            'model_source': model_name,\n                            'extraction_timestamp': datetime.now().isoformat()\n                        }\n                        final_entities.append(processed)\n            \n            # Update stats\n            handler.stats['texts_processed'] += 1\n            handler.stats['entities_found'] += len(final_entities)\n            handler.stats['processing_time'] += time.time() - start_time\n            \n            return final_entities\n            \n        except Exception as e:\n            print(f\"   âŒ {model_name} extraction failed: {e}\")\n            return []\n    \n    def _deduplicate_entities(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Remove duplicate entities at same positions\"\"\"\n        seen = set()\n        unique = []\n        \n        for entity in entities:\n            key = (entity.get('start'), entity.get('end'), entity.get('word'))\n            if key not in seen:\n                seen.add(key)\n                unique.append(entity)\n        \n        return unique\n    \n    def process_sec_filing_sections(self, filing_result: Dict) -> List[Dict]:\n        \"\"\"Process SEC sections with multi-model consensus\"\"\"\n        if filing_result.get('processing_status') != 'success':\n            return []\n        \n        sections = filing_result['sections']\n        print(f\"ğŸ” Processing {len(sections)} sections with quality control\")\n        \n        all_entities = []\n        \n        for section_name, section_text in sections.items():\n            if not section_text.strip():\n                continue\n            \n            print(f\"   ğŸ“‘ Processing '{section_name}'\")\n            \n            # Extract with all applicable models\n            section_entities_by_model = {}\n            for model_name in self.model_handlers:\n                entities = self.extract_entities_from_text(section_text, model_name)\n                if entities:\n                    section_entities_by_model[model_name] = entities\n                    print(f\"      â€¢ {model_name}: {len(entities)} quality entities\")\n            \n            # Apply consensus merging\n            consensus_entities = self._apply_consensus_merge(\n                section_entities_by_model,\n                section_name,\n                filing_result\n            )\n            \n            all_entities.extend(consensus_entities)\n        \n        # Final quality filtering\n        high_quality_entities = self._final_quality_filter(all_entities)\n        \n        print(f\"   âœ… Extracted {len(high_quality_entities)} high-quality entities\")\n        print(f\"   ğŸ“Š Filtered out {len(all_entities) - len(high_quality_entities)} low-quality entities\")\n        \n        self.pipeline_stats['total_entities_extracted'] += len(high_quality_entities)\n        self.pipeline_stats['entities_filtered'] += len(all_entities) - len(high_quality_entities)\n        \n        return high_quality_entities\n    \n    def _apply_consensus_merge(self, entities_by_model: Dict[str, List[Dict]], \n                              section_name: str, filing_result: Dict) -> List[Dict]:\n        \"\"\"Merge entities with consensus scoring\"\"\"\n        if not entities_by_model:\n            return []\n        \n        # Group entities by position\n        position_groups = {}\n        \n        for model_name, entities in entities_by_model.items():\n            for entity in entities:\n                # Create position key with tolerance\n                start = entity['char_start']\n                end = entity['char_end']\n                \n                # Find matching position group (within 5 char tolerance)\n                matched = False\n                for pos_key in list(position_groups.keys()):\n                    key_start, key_end = map(int, pos_key.split('_'))\n                    if abs(start - key_start) <= 5 and abs(end - key_end) <= 5:\n                        position_groups[pos_key].append((model_name, entity))\n                        matched = True\n                        break\n                \n                if not matched:\n                    pos_key = f\"{start}_{end}\"\n                    position_groups[pos_key] = [(model_name, entity)]\n        \n        # Create consensus entities\n        consensus_entities = []\n        min_models = self.model_config['quality_thresholds']['min_consensus_models']\n        \n        for pos_key, model_entities in position_groups.items():\n            # Single model detection - apply stricter quality threshold\n            if len(model_entities) == 1:\n                model_name, entity = model_entities[0]\n                if entity.get('quality_score', 0) >= 0.7:  # Higher threshold for single model\n                    entity['extraction_id'] = entity.get('extraction_id', str(uuid.uuid4()))\n                    entity['consensus_count'] = 1\n                    entity['detecting_models'] = [model_name]\n                    entity['section_name'] = section_name\n                    entity['filing_id'] = filing_result['filing_id']\n                    entity['company_domain'] = filing_result['company_domain']\n                    entity['sec_filing_ref'] = f\"SEC_{filing_result['filing_id']}\"\n                    consensus_entities.append(entity)\n            \n            # Multi-model consensus - more confident\n            else:\n                # Take highest quality entity as base\n                best_entity = max(model_entities, key=lambda x: x[1].get('quality_score', 0))[1].copy()\n                \n                # Add consensus metadata\n                best_entity['extraction_id'] = str(uuid.uuid4())  # New ID for merged entity\n                best_entity['consensus_count'] = len(model_entities)\n                best_entity['detecting_models'] = [m[0] for m in model_entities]\n                best_entity['consensus_score'] = np.mean([e[1].get('quality_score', 0) \n                                                         for e in model_entities])\n                \n                # Boost quality score for consensus\n                best_entity['quality_score'] = min(best_entity['quality_score'] * 1.2, 1.0)\n                \n                # Add filing metadata\n                best_entity['section_name'] = section_name\n                best_entity['filing_id'] = filing_result['filing_id']\n                best_entity['company_domain'] = filing_result['company_domain']\n                best_entity['sec_filing_ref'] = f\"SEC_{filing_result['filing_id']}\"\n                \n                consensus_entities.append(best_entity)\n        \n        return consensus_entities\n    \n    def _final_quality_filter(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Apply final quality filters\"\"\"\n        filtered = []\n        \n        # Track entity text frequency (common boilerplate detection)\n        text_frequency = {}\n        for entity in entities:\n            text = entity['entity_text'].lower()\n            text_frequency[text] = text_frequency.get(text, 0) + 1\n        \n        for entity in entities:\n            text = entity['entity_text']\n            \n            # Skip if too short or too long\n            if len(text) < self.model_config['quality_thresholds']['min_entity_length']:\n                continue\n            if len(text) > self.model_config['quality_thresholds']['max_entity_length']:\n                continue\n            \n            # Skip common boilerplate (appears too frequently)\n            if text_frequency.get(text.lower(), 0) > 10:\n                continue\n            \n            # Skip if looks like a sentence\n            if text.count(' ') > 5 or text.count('.') > 1:\n                continue\n            \n            # Skip common non-entities\n            skip_patterns = [\n                r'^\\d+$',  # Just numbers\n                r'^[A-Z]$',  # Single letters\n                r'^(the|and|or|of|in|to|for|with|on|at|from|by|about)$',  # Common words\n                r'^\\W+$',  # Just punctuation\n            ]\n            \n            if any(re.match(pattern, text, re.IGNORECASE) for pattern in skip_patterns):\n                continue\n            \n            filtered.append(entity)\n        \n        return filtered\n    \n    def get_pipeline_statistics(self) -> Dict:\n        \"\"\"Get comprehensive statistics\"\"\"\n        model_stats = {}\n        for name, handler in self.model_handlers.items():\n            model_stats[name] = handler.stats.copy()\n        \n        return {\n            'pipeline_stats': self.pipeline_stats.copy(),\n            'model_stats': model_stats,\n            'loaded_models': list(self.model_handlers.keys()),\n            'quality_settings': self.model_config['quality_thresholds']\n        }\n\n\n# Initialize the enhanced pipeline\nentity_pipeline = EntityExtractionPipeline(NEON_CONFIG)\n\n# Load models with handlers\nloaded_models = entity_pipeline.load_models()\n\nprint(f\"\\nâœ… Enhanced EntityExtractionPipeline ready!\")\nprint(f\"   ğŸ¯ Loaded models: {list(loaded_models.keys())}\")\nprint(f\"   ğŸ” Quality control: Enabled\")\nprint(f\"   ğŸ“Š Consensus requirement: {entity_pipeline.model_config['quality_thresholds']['min_consensus_models']} models\")\nprint(f\"   ğŸ’» Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:10:12.338593Z","iopub.execute_input":"2025-09-13T18:10:12.338823Z","iopub.status.idle":"2025-09-13T18:10:58.440614Z","shell.execute_reply.started":"2025-09-13T18:10:12.338803Z","shell.execute_reply":"2025-09-13T18:10:58.440277Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9afceb8ac093465fa7804b2f13dcaf7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e87cc3d20d884a1fb6b9e75bf92da944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c35950bbf848308f2255df61ca6115"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d32db8006c428ba7ec45d81486530e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e028934672c4ce0ac55e918d42a9584"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77311d364cc4cfdba3b743ebb20a4da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884d1b630cd04b1cbfabef23836e7ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9472022ee145435e8e043f95a2a00b85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6220deff5ab34450a6b2209d796bb87d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"661d6212ae734cde9d7481ef787b3537"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abcd69cd97e54e33bec5faff604b7490"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5447371cdea94679bbfa179ea84c6732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c7932347b074e088124a922514765fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/849 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33bf711ae41c4f9fa16cf7c68c34e6d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7557098c3eb84bdf86f705636f1546fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21850786d7d048fb8b85dfa483416953"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfab6c4f8c954b83bd5ec0c9f1cfc45c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6136281c14f4b509e2c8c83de976876"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Cell 4: Relationship Extractor with Local Llama 3.1-8B\n\nimport psycopg2\nfrom psycopg2.extras import execute_values\nimport numpy as np\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nimport re\nfrom typing import List, Dict, Tuple, Optional\nimport json\nimport uuid\nimport time\nimport os\nfrom datetime import datetime\n\nprint(\"ğŸš€ Loading In-Memory Pipeline with Storage and Local Llama 3.1-8B...\")\n\n# ================================================================================\n# RELATIONSHIP EXTRACTOR WITH LOCAL LLAMA 3.1-8B\n# ================================================================================\n\nclass RelationshipExtractor:\n    \"\"\"Extract company-centric relationships using local Llama 3.1-8B\"\"\"\n    \n    def __init__(self, llama_config: Dict = None):\n        \"\"\"Initialize with local Llama 3.1-8B model\"\"\"\n        self.config = llama_config or CONFIG.get('llama', {})\n        self.model = None\n        self.tokenizer = None\n        self.stats = {\n            'entities_processed': 0,\n            'relationships_found': 0,\n            'llama_calls': 0,\n            'processing_time': 0\n        }\n        \n                \n        # Verify CONFIG is available\n        if not CONFIG.get('llama', {}).get('enabled', False):\n            print(\"   âš ï¸ Llama configuration disabled in CONFIG\")\n            return\n        \n        try:\n            # Auto-login to HuggingFace using Kaggle secret\n            print(\"   ğŸ” Logging in to HuggingFace...\")\n            user_secrets = UserSecretsClient()\n            hf_token = user_secrets.get_secret('HUGGINGFACE_TOKEN')\n            \n            if hf_token:\n                login(token=hf_token, add_to_git_credential=False)\n                print(\"   âœ… Logged in to HuggingFace\")\n            else:\n                print(\"   âš ï¸ No HUGGINGFACE_TOKEN found in Kaggle secrets\")\n                return\n            \n            # Configure 4-bit quantization for memory efficiency\n            print(\"   âš™ï¸ Configuring 4-bit quantization...\")\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16\n            )\n            \n            # Load Llama 3.1-8B model\n            print(\"   ğŸ“¥ Loading Llama 3.1-8B-Instruct (this may take a minute)...\")\n            model_name = CONFIG[\"llama\"][\"model_name\"]\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n            \n            print(\"   âœ… Llama 3.1-8B loaded successfully (4-bit quantized)\")\n            \n            # Test the model\n            test_messages = [\n                {\"role\": \"user\", \"content\": \"What is a partnership? Answer in one sentence.\"}\n            ]\n            test_input = self.tokenizer.apply_chat_template(test_messages, return_tensors=\"pt\", tokenize=True)\n            \n            with torch.no_grad():\n                outputs = self.model.generate(test_input, max_new_tokens=CONFIG[\"llama\"][\"test_max_tokens\"], temperature=CONFIG[\"llama\"][\"temperature\"])\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            print(f\"   ğŸ§ª Test response: {response[:100]}...\")\n            \n        except Exception as e:\n            print(f\"   âŒ Failed to load Llama 3.1-8B: {e}\")\n            print(\"   âš ï¸ Relationship extraction will be disabled\")\n            self.model = None\n            self.tokenizer = None\n    \n    def _filter_entities(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Filter out non-business entities like SEC section headers and document structure\"\"\"\n        filtered = []\n        \n        # Patterns to exclude (SEC filing structure elements)\n        exclude_patterns = [\n            r'^Item \\d+[A-Za-z]?\\.?\\s*',  # Item 1, Item 2A, etc.\n            r'^Part [IVX]+\\.?\\s*',  # Part I, Part II, etc.\n            r'^Section \\d+\\.?\\s*',  # Section 1, Section 2, etc.\n            r'^Exhibit \\d+\\.?\\s*',  # Exhibit 1, etc.\n            r'^Schedule [A-Z]\\.?\\s*',  # Schedule A, etc.\n            r'^Note \\d+\\.?\\s*',  # Note 1, Note 2, etc.\n            r'^Table of Contents',\n            r'^Index to',\n            r'^Forward.?Looking Statements',\n            r'^Risk Factors$',\n            r'^Legal Proceedings$',\n            r'^Management.s Discussion',\n            r'^Quantitative and Qualitative Disclosures',\n            r'^Controls and Procedures',\n            r'^Financial Statements',\n            r'^Signatures$',\n            r'^SIGNATURES$'\n        ]\n        \n        for entity in entities:\n            entity_text = entity.get('entity_text', '').strip()\n            entity_type = entity.get('entity_category', entity.get('entity_type', ''))\n            \n            # Skip empty or very short entities\n            if not entity_text or len(entity_text) < 3:\n                continue\n            \n            # Skip entities that are just numbers\n            if entity_text.replace('.', '').replace(',', '').isdigit():\n                continue\n            \n            # Skip if entity matches SEC structure patterns\n            is_structure = False\n            for pattern in exclude_patterns:\n                if re.match(pattern, entity_text, re.IGNORECASE):\n                    is_structure = True\n                    break\n            \n            if is_structure:\n                continue\n            \n            # Skip entities with category '0' that look like SEC headers\n            if entity.get('entity_category') == '0' or entity_type == '0':\n                # Check if it starts with common SEC filing patterns\n                if any(entity_text.startswith(prefix) for prefix in ['Item ', 'Part ', 'Section ', 'Note ']):\n                    continue\n            \n            # Additional filtering for low-quality entities\n            if entity_type in ['MISCELLANEOUS', '0']:\n                # For miscellaneous entities, apply stricter filtering\n                confidence = entity.get('confidence_score', entity.get('confidence', 0))\n                if confidence < CONFIG[\"llama\"][\"min_confidence_filter\"]:\n                    continue\n                \n                # Check if it looks like a real company/organization name\n                words = entity_text.split()\n                if len(words) > 0:\n                    has_capital = any(word[0].isupper() for word in words if word)\n                    all_caps = entity_text.isupper()\n                    all_lower = entity_text.islower()\n                    \n                    if not has_capital or all_caps or all_lower:\n                        continue\n            \n            # Entity passed all filters\n            filtered.append(entity)\n        \n        print(f\"      ğŸ¯ Filtered: {len(entities)} â†’ {len(filtered)} entities (removed {len(entities) - len(filtered)} non-business entities)\")\n        return filtered\n    \n    def extract_company_relationships(self, \n                                     entities: List[Dict], \n                                     sections: Dict[str, str],\n                                     company_domain: str) -> List[Dict]:\n        \"\"\"Extract relationships between company and all found entities\"\"\"\n        if not self.model or not self.tokenizer or not entities:\n            return []\n        \n        print(f\"   ğŸ” Analyzing relationships for {company_domain}\")\n        \n        # Filter out non-business entities first\n        entities = self._filter_entities(entities)\n        if not entities:\n            print(\"      âš ï¸ No valid business entities after filtering\")\n            return []\n        \n        relationships = []\n        \n        # Group entities by section for context efficiency\n        entities_by_section = {}\n        for entity in entities:\n            section = entity.get('section_name', 'unknown')\n            if section not in entities_by_section:\n                entities_by_section[section] = []\n            entities_by_section[section].append(entity)\n        \n        # Process each section's entities\n        # Progress tracking initialization\n        total_entities_to_process = sum(len(ents) for ents in entities_by_section.values())\n        entities_processed_count = 0\n        start_time = time.time()\n        print(f\"      ğŸ¯ Total entities to analyze: {total_entities_to_process}\")\n\n        for section_name, section_entities in entities_by_section.items():\n            if section_name not in sections:\n                continue\n                \n            section_text = sections[section_name]\n            section_relationships_count = 0  # Initialize counter for this section\n            print(f\"      ğŸ“‘ Processing {len(section_entities)} entities in '{section_name}'\")\n            \n            # Analyze each entity's relationship to the company\n            for entity_idx, entity in enumerate(section_entities, 1):\n                # Skip self-references\n                entities_processed_count += 1\n                \n                # Show progress every 10 entities\n                if entities_processed_count % 10 == 0:\n                    progress_pct = (entities_processed_count * 100) // total_entities_to_process\n                    print(f\"         â³ Progress: {entities_processed_count}/{total_entities_to_process} entities ({progress_pct}%)\")\n                \n                # Show time estimate every 25 entities\n                if entities_processed_count % 25 == 0:\n                    elapsed = time.time() - start_time\n                    if entities_processed_count > 0:\n                        rate = entities_processed_count / elapsed\n                        remaining = (total_entities_to_process - entities_processed_count) / rate\n                        print(f\"         â±ï¸  Estimated time remaining: {remaining/60:.1f} minutes\")\n                \n                # Show current entity being processed (every 5th)\n                if entity_idx % 5 == 1:\n                    print(f\"         ğŸ”¬ Analyzing: {entity['entity_text'][:50]}...\" if len(entity.get('entity_text', '')) > 50 else f\"         ğŸ”¬ Analyzing: {entity.get('entity_text', 'Unknown')}\")\n\n                company_name = company_domain.replace('.com', '').replace('tx', '')\n                if entity['entity_text'].lower() == company_name.lower():\n                    continue\n                \n                # Get context around entity\n                context = self._get_entity_context(entity, section_text)\n                \n                # Build and send prompt to Llama\n                relationship = self._analyze_relationship(\n                    company_domain, entity, context, section_name\n                )\n                \n                if relationship:\n                    relationships.append(relationship)\n                    section_relationships_count += 1\n                    self.stats['relationships_found'] += 1\n                \n                self.stats['entities_processed'] += 1\n        \n        print(f\"   âœ… Found {len(relationships)} relationships from {len(entities)} entities\")\n        return relationships\n    \n    def _get_entity_context(self, entity: Dict, section_text: str, window: int = None) -> str:\n        \"\"\"Get context around an entity\"\"\"\n        if window is None:\n            window = CONFIG[\"llama\"][\"entity_context_window\"]\n        start = max(0, entity.get('character_start', entity.get('char_start', 0)) - window)\n        end = min(len(section_text), entity.get('character_end', entity.get('char_end', 0)) + window)\n        return section_text[start:end]\n    \n    def _analyze_relationship(self, \n                            company_domain: str,\n                            entity: Dict,\n                            context: str,\n                            section_name: str) -> Optional[Dict]:\n        \"\"\"Analyze a single entity's relationship to the company using local Llama\"\"\"\n        if not self.model or not self.tokenizer:\n            return None\n        \n        try:\n            # Build prompt\n            prompt = f\"\"\"Analyze the business relationship between the filing company and the mentioned entity.\n\nFiling Company: {company_domain}\nEntity Found: {entity['entity_text']} (Type: {entity.get('entity_category', entity.get('entity_type', 'UNKNOWN'))})\nSection: {section_name}\n\nContext:\n{context[:CONFIG[\"llama\"][\"context_window\"]]}\n\nBased on the context, determine:\n1. Relationship type (choose ONE from: PARTNERSHIP, COMPETITOR, REGULATORY, CLINICAL_TRIAL, SUPPLIER, CUSTOMER, INVESTOR, ACQUISITION, LICENSING, RESEARCH, NONE)\n2. Relationship direction (company_to_entity or entity_to_company)\n3. Business impact (positive, negative, or neutral)\n4. Confidence level (high, medium, or low)\n5. Brief summary (one sentence)\n\nFormat your response EXACTLY as:\nTYPE: <relationship_type>\nDIRECTION: <direction>\nIMPACT: <impact>\nCONFIDENCE: <confidence>\nSUMMARY: <one_sentence_summary>\"\"\"\n            \n            # Create messages for chat format\n            messages = [\n                {\"role\": \"system\", \"content\": \"You are an expert at analyzing business relationships from SEC filings. Always respond in the exact format requested.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n            \n            # Apply chat template\n            inputs = self.tokenizer.apply_chat_template(\n                messages,\n                return_tensors=\"pt\",\n                tokenize=True\n            )\n            \n            # Generate response with local model\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs,\n                    max_new_tokens=CONFIG[\"llama\"][\"max_new_tokens\"],\n                    temperature=CONFIG[\"llama\"][\"temperature\"],\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode response\n            llama_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Extract just the assistant's response\n            if \"assistant\" in llama_response:\n                llama_response = llama_response.split(\"assistant\")[-1].strip()\n            \n            self.stats['llama_calls'] += 1\n            \n            # Parse response\n            parsed = self._parse_llama_response(llama_response)\n            \n            if parsed and parsed['type'] != 'NONE':\n                return {\n                    'company_domain': company_domain,\n                    'entity_text': entity['entity_text'],\n                    'entity_type': entity.get('entity_category', entity.get('entity_type', 'UNKNOWN')),\n                    'entity_id': entity.get('extraction_id', str(uuid.uuid4())),\n                    'relationship_type': parsed['type'],\n                    'relationship_direction': parsed['direction'],\n                    'business_impact': parsed['impact'],\n                    'confidence_level': parsed['confidence'],\n                    'summary': parsed['summary'],\n                    'section_name': section_name,\n                    'context_used': context[:CONFIG[\"llama\"][\"context_window\"]],  # Store first 1000 chars\n                    'llama_response': llama_response[:500],  # Store trimmed response\n                    'extraction_timestamp': datetime.now().isoformat()\n                }\n            \n            return None\n            \n        except Exception as e:\n            print(f\"         âš ï¸ Llama analysis failed for {entity['entity_text']}: {e}\")\n            return None\n    \n    def _parse_llama_response(self, response: str) -> Optional[Dict]:\n        \"\"\"Parse structured response from Llama\"\"\"\n        try:\n            lines = response.strip().split('\\n')\n            parsed = {}\n            \n            for line in lines:\n                if ':' in line:\n                    key, value = line.split(':', 1)\n                    key = key.strip().upper()\n                    value = value.strip()\n                    \n                    if key == 'TYPE':\n                        parsed['type'] = value\n                    elif key == 'DIRECTION':\n                        parsed['direction'] = value\n                    elif key == 'IMPACT':\n                        parsed['impact'] = value.lower()\n                    elif key == 'CONFIDENCE':\n                        parsed['confidence'] = value.lower()\n                    elif key == 'SUMMARY':\n                        parsed['summary'] = value\n            \n            # Validate required fields\n            required = ['type', 'direction', 'impact', 'confidence', 'summary']\n            if all(field in parsed for field in required):\n                return parsed\n            \n            return None\n            \n        except Exception:\n            return None\n\n\n# ================================================================================\n# ENHANCED STORAGE WITH ATOMIC TRANSACTIONS\n# ================================================================================\n\nclass PipelineEntityStorage:\n    \"\"\"Enhanced storage system with atomic entity+relationship storage\"\"\"\n    \n    def __init__(self, db_config: Dict):\n        self.db_config = db_config\n        self.storage_stats = {\n            'total_entities_stored': 0,\n            'total_relationships_stored': 0,\n            'filings_processed': 0,\n            'transactions_completed': 0,\n            'transactions_failed': 0,\n            'merged_entities': 0,\n            'single_model_entities': 0,\n            'failed_inserts': 0\n        }\n        \n        # Ensure table structures\n        self._ensure_table_structures()\n    \n    def _ensure_table_structures(self):\n        \"\"\"Ensure both entity and relationship tables exist with proper schema\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Ensure entity table columns (same as before)\n            self._ensure_entity_table(cursor)\n            \n            # Create relationship table if not exists\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS system_uno.entity_relationships (\n                    relationship_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n                    company_domain TEXT NOT NULL,\n                    entity_id UUID REFERENCES system_uno.sec_entities_raw(extraction_id),\n                    entity_text TEXT NOT NULL,\n                    entity_type TEXT,\n                    relationship_type TEXT NOT NULL,\n                    relationship_direction TEXT,\n                    business_impact TEXT,\n                    confidence_level TEXT,\n                    summary TEXT,\n                    section_name TEXT,\n                    context_used TEXT,\n                    llama_response TEXT,\n                    filing_ref TEXT,\n                    extraction_timestamp TIMESTAMP,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            \n            # Create indexes for relationships\n            cursor.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_relationships_company \n                ON system_uno.entity_relationships(company_domain)\n            \"\"\")\n            cursor.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_relationships_type \n                ON system_uno.entity_relationships(relationship_type)\n            \"\"\")\n            cursor.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_relationships_impact \n                ON system_uno.entity_relationships(business_impact)\n            \"\"\")\n            \n            conn.commit()\n            cursor.close()\n            conn.close()\n            print(\"   âœ… Database tables verified/created\")\n            \n        except Exception as e:\n            print(f\"   âš ï¸ Table structure setup failed: {e}\")\n    \n    def _ensure_entity_table(self, cursor):\n        \"\"\"Ensure entity table has all required columns\"\"\"\n        # Check existing columns\n        cursor.execute(\"\"\"\n            SELECT column_name \n            FROM information_schema.columns \n            WHERE table_schema = 'system_uno' \n            AND table_name = 'sec_entities_raw'\n        \"\"\")\n        \n        existing_columns = {row[0] for row in cursor.fetchall()}\n        \n        # Required columns for pipeline\n        required_columns = {\n            'models_detected': 'TEXT[]',\n            'all_confidences': 'JSONB',\n            'primary_model': 'TEXT',\n            'entity_variations': 'JSONB',\n            'is_merged': 'BOOLEAN DEFAULT FALSE',\n            'section_name': 'TEXT',\n            'data_source': 'TEXT DEFAULT \\'sec_filings\\'',\n            'extraction_timestamp': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',\n            'original_label': 'TEXT',\n            'quality_score': 'REAL DEFAULT 0.0',\n            'consensus_count': 'INTEGER DEFAULT 1',\n            'detecting_models': 'JSONB',\n            'consensus_score': 'REAL DEFAULT 0.0',\n        }\n        \n        # Add missing columns\n        for col_name, col_type in required_columns.items():\n            if col_name not in existing_columns:\n                cursor.execute(f'ALTER TABLE system_uno.sec_entities_raw ADD COLUMN {col_name} {col_type}')\n                print(f\"      âœ“ Added column: {col_name}\")\n    \n    def store_entities(self, entities: List[Dict], filing_ref: str) -> bool:\n        \"\"\"Store only entities (without relationships) - used before Llama processing\"\"\"\n        if not entities:\n            return True\n        \n        conn = None\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Start transaction\n            conn.autocommit = False\n            \n            print(f\"   ğŸ’¾ Storing {len(entities)} entities to database...\")\n            \n            # Count merged vs single-model entities BEFORE storage\n            merged_count = sum(1 for e in entities if e.get('is_merged', False))\n            single_model_count = len(entities) - merged_count\n            \n            # Store entities\n            entity_data = []\n            for entity in entities:\n                # Prepare entity data\n                models_detected = entity.get('models_detected', [entity.get('model_source', 'unknown')])\n                if not isinstance(models_detected, list):\n                    models_detected = [str(models_detected)]\n                \n                all_confidences = entity.get('all_confidences', \n                    {entity.get('model_source', 'unknown'): entity.get('confidence_score', 0.0)})\n                entity_variations = entity.get('entity_variations', \n                    {entity.get('model_source', 'unknown'): entity.get('entity_text', '')})\n                \n                entity_tuple = (\n                    entity.get('extraction_id', str(uuid.uuid4())),\n                    entity.get('company_domain', ''),\n                    entity.get('entity_text', '').strip()[:1000],\n                    entity.get('entity_type', 'UNKNOWN'),\n                    float(entity.get('confidence_score', 0.0)),\n                    int(entity.get('char_start', 0)),\n                    int(entity.get('char_end', 0)),\n                    entity.get('surrounding_text', '')[:2000] if entity.get('surrounding_text') else '',\n                    entity.get('sec_filing_ref', filing_ref),\n                    models_detected,\n                    json.dumps(all_confidences),\n                    entity.get('primary_model', entity.get('model_source', 'unknown')),\n                    json.dumps(entity_variations),\n                    entity.get('is_merged', False),\n                    entity.get('section_name', ''),\n                    entity.get('data_source', 'sec_filings'),\n                    entity.get('extraction_timestamp'),\n                    entity.get('original_label', '')\n                )\n                entity_data.append(entity_tuple)\n            \n            # Batch insert entities\n            entity_query = \"\"\"\n                INSERT INTO system_uno.sec_entities_raw \n                (extraction_id, company_domain, entity_text, entity_category, \n                 confidence_score, character_start, character_end, surrounding_text, \n                 sec_filing_ref, models_detected, all_confidences, primary_model,\n                 entity_variations, is_merged, section_name, data_source,\n                 extraction_timestamp, original_label)\n                VALUES %s\n                ON CONFLICT (extraction_id) DO UPDATE SET\n                    models_detected = EXCLUDED.models_detected,\n                    all_confidences = EXCLUDED.all_confidences,\n                    is_merged = EXCLUDED.is_merged\n            \"\"\"\n            \n            execute_values(cursor, entity_query, entity_data, page_size=100)\n            entities_stored = cursor.rowcount\n            \n            # Commit transaction\n            conn.commit()\n            \n            # Update statistics\n            self.storage_stats['total_entities_stored'] += entities_stored\n            self.storage_stats['transactions_completed'] += 1\n            self.storage_stats['merged_entities'] += merged_count\n            self.storage_stats['single_model_entities'] += single_model_count\n            \n            print(f\"      âœ… Entities stored: {entities_stored} entities\")\n            print(f\"         â€¢ Merged entities: {merged_count}, Single-model: {single_model_count}\")\n            \n            cursor.close()\n            conn.close()\n            return True\n            \n        except Exception as e:\n            print(f\"      âŒ Entity storage failed: {e}\")\n            if conn:\n                conn.rollback()\n                conn.close()\n            self.storage_stats['transactions_failed'] += 1\n            self.storage_stats['failed_inserts'] += len(entities)\n            return False\n\n\n    def store_relationships(self, relationships: List[Dict], filing_ref: str) -> bool:\n        \"\"\"Store relationship data using correct db_config pattern\"\"\"\n        if not relationships:\n            log_warning(\"RelationshipStorage\", \"No relationships to store\")\n            return True\n        \n        conn = None\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            conn.autocommit = False\n            \n            # Prepare batch data for relationships\n            relationship_data = []\n            for rel in relationships:\n                relationship_data.append((\n                    rel.get('company_domain', ''),\n                    rel.get('entity_id', str(uuid.uuid4())),\n                    rel.get('entity_text', ''),\n                    rel.get('entity_type', 'UNKNOWN'),\n                    rel.get('relationship_type', 'UNKNOWN'),\n                    rel.get('relationship_direction', ''),\n                    rel.get('business_impact', ''),\n                    rel.get('confidence_level', ''),\n                    rel.get('summary', ''),\n                    rel.get('section_name', ''),\n                    rel.get('context_used', '')[:2000],  # Limit context size\n                    rel.get('llama_response', '')[:1000],  # Limit response size\n                    filing_ref,  # Add filing_ref parameter\n                    rel.get('extraction_timestamp', datetime.now().isoformat())\n                ))\n            \n            # Use execute_values for efficient batch insert\n            execute_values(\n                cursor,\n                \"\"\"INSERT INTO system_uno.entity_relationships \n                   (company_domain, entity_id, entity_text, entity_type, relationship_type,\n                    relationship_direction, business_impact, confidence_level, summary, \n                    section_name, context_used, llama_response, filing_ref, extraction_timestamp)\n                   VALUES %s\"\"\",\n                relationship_data,\n                template=None,\n                page_size=CONFIG['processing']['max_insert_batch']\n            )\n            \n            conn.commit()\n            cursor.close()\n            \n            # Update statistics\n            self.storage_stats['total_relationships_stored'] += len(relationships)\n            self.storage_stats['transactions_completed'] += 1\n            \n            log_info(\"RelationshipStorage\", f\"Stored {len(relationships)} relationships for {filing_ref}\")\n            return True\n            \n        except Exception as e:\n            if conn:\n                conn.rollback()\n            self.storage_stats['transactions_failed'] += 1\n            log_error(\"RelationshipStorage\", f\"Failed to store {len(relationships)} relationships\", e)\n            return False\n        finally:\n            if conn:\n                conn.close()\n\n    def get_storage_verification(self, filing_ref: str) -> Dict:\n        \"\"\"Verify stored entities and relationships for a filing\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Get entity statistics\n            cursor.execute(\"\"\"\n                SELECT \n                    COUNT(*) as total_entities,\n                    COUNT(DISTINCT entity_category) as unique_categories,\n                    COUNT(DISTINCT section_name) as sections_processed,\n                    AVG(confidence_score) as avg_confidence\n                FROM system_uno.sec_entities_raw\n                WHERE sec_filing_ref = %s\n            \"\"\", (filing_ref,))\n            \n            entity_stats = cursor.fetchone()\n            \n            # Get relationship statistics\n            cursor.execute(\"\"\"\n                SELECT \n                    COUNT(*) as total_relationships,\n                    COUNT(DISTINCT relationship_type) as relationship_types,\n                    COUNT(CASE WHEN business_impact = 'positive' THEN 1 END) as positive_impact,\n                    COUNT(CASE WHEN business_impact = 'negative' THEN 1 END) as negative_impact,\n                    COUNT(CASE WHEN business_impact = 'neutral' THEN 1 END) as neutral_impact\n                FROM system_uno.entity_relationships\n                WHERE filing_ref = %s\n            \"\"\", (filing_ref,))\n            \n            rel_stats = cursor.fetchone()\n            \n            cursor.close()\n            conn.close()\n            \n            return {\n                'filing_ref': filing_ref,\n                'entities': {\n                    'total': entity_stats[0] if entity_stats else 0,\n                    'categories': entity_stats[1] if entity_stats else 0,\n                    'sections': entity_stats[2] if entity_stats else 0,\n                    'avg_confidence': float(entity_stats[3]) if entity_stats and entity_stats[3] else 0\n                },\n                'relationships': {\n                    'total': rel_stats[0] if rel_stats else 0,\n                    'types': rel_stats[1] if rel_stats else 0,\n                    'positive': rel_stats[2] if rel_stats else 0,\n                    'negative': rel_stats[3] if rel_stats else 0,\n                    'neutral': rel_stats[4] if rel_stats else 0\n                }\n            }\n            \n        except Exception as e:\n            print(f\"   âŒ Verification failed: {e}\")\n            return {}\n\n# ================================================================================\n# REFACTORED MAIN PIPELINE WITH IN-MEMORY PROCESSING\n# ================================================================================\n\n# Initialize components\npipeline_storage = PipelineEntityStorage(NEON_CONFIG)\nrelationship_extractor = RelationshipExtractor()\n\ndef process_filing_with_pipeline(filing_data: Dict) -> Dict:\n    \"\"\"Process filing with in-memory entity and relationship extraction\"\"\"\n    try:\n        start_time = time.time()\n        \n        # Step 1: Extract sections (Cell 2 function)\n        print(f\"\\nğŸ“„ Processing {filing_data['filing_type']} for {filing_data['company_domain']}\")\n        section_result = process_sec_filing_with_sections(filing_data)\n        \n        if section_result['processing_status'] != 'success':\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': section_result.get('error', 'Section extraction failed'),\n                'processing_time': time.time() - start_time\n            }\n        \n        # Keep sections in memory for context retrieval\n        sections_dict = section_result['sections']\n        \n        # Step 2: Extract entities (Cell 3 function) - keep in memory\n        entities = entity_pipeline.process_sec_filing_sections(section_result)\n        \n        if not entities:\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': 'No entities extracted',\n                'processing_time': time.time() - start_time\n            }\n        \n        print(f\"   ğŸ” Extracted {len(entities)} entities\")\n\n        # Debug step removed\n        # debug_entities call removed - function not defined\n\n        # Step 3: Store entities IMMEDIATELY before Llama processing\n        filing_ref = f\"SEC_{filing_data.get('id')}\"\n        entity_storage_success = pipeline_storage.store_entities(entities, filing_ref)\n\n        if not entity_storage_success:\n            print(f\"   âš ï¸ Failed to store entities, but continuing with relationship extraction...\")\n\n        # Step 4: Extract relationships using in-memory entities and sections (LONG PROCESS)\n        print(f\"   ğŸ¤– Starting Llama 3.1 relationship extraction (this may take several minutes)...\")\n        \n        # (relationships extraction continues below...)\n        relationships = relationship_extractor.extract_company_relationships(\n            entities, \n            sections_dict,\n            filing_data['company_domain']\n        )\n        \n        # Step 5: Store relationships separately\n        filing_ref = f\"SEC_{filing_data.get('id')}\"\n        if relationships:\n            relationship_storage_success = pipeline_storage.store_relationships(relationships, filing_ref)\n        else:\n            relationship_storage_success = True\n            print(f\"   â„¹ï¸ No relationships found to store\")\n        \n        # Step 6: Verify storage\n        verification = pipeline_storage.get_storage_verification(filing_ref)\n        \n        processing_time = time.time() - start_time\n        \n        # Calculate overall storage success\n        storage_success = entity_storage_success and relationship_storage_success\n        \n        return {\n            'success': storage_success,\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain'),\n            'filing_type': filing_data.get('filing_type'),\n            'sections_processed': len(sections_dict),\n            'entities_extracted': len(entities),\n            'relationships_found': len(relationships),\n            'entities_stored': verification.get('entities', {}).get('total', 0),\n            'relationships_stored': verification.get('relationships', {}).get('total', 0),\n            'processing_time': round(processing_time, 2),\n            'verification': verification,\n            'sample_entities': entities[:3],\n            'sample_relationships': relationships[:3]\n        }\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'filing_id': filing_data.get('id'),\n            'error': str(e),\n            'processing_time': time.time() - start_time\n        }\n\ndef process_filings_batch(limit: int = 3) -> Dict:\n    \"\"\"Process multiple filings with complete in-memory pipeline\"\"\"\n    print(f\"\\nğŸš€ Processing batch of {limit} SEC filings with in-memory pipeline...\")\n    \n    batch_start = time.time()\n    \n    # Get unprocessed filings\n    filings = get_unprocessed_filings(limit)\n    \n    if not filings:\n        return {'success': False, 'message': 'No filings to process'}\n    \n    print(f\"ğŸ“Š Found {len(filings)} filings to process\")\n    \n    # Process each filing\n    results = []\n    successful = 0\n    total_entities = 0\n    total_relationships = 0\n    \n    for i, filing in enumerate(filings, 1):\n        print(f\"\\n[{i}/{len(filings)}] Processing {filing['filing_type']} for {filing['company_domain']}\")\n        \n        result = process_filing_with_pipeline(filing)\n        results.append(result)\n        \n        if result['success']:\n            successful += 1\n            total_entities += result.get('entities_extracted', 0)\n            total_relationships += result.get('relationships_found', 0)\n            \n            print(f\"   âœ… Success: {result['entities_extracted']} entities, {result['relationships_found']} relationships\")\n            print(f\"   â±ï¸ Processing time: {result['processing_time']}s\")\n            \n            # Show sample relationships\n            for rel in result.get('sample_relationships', [])[:2]:\n                print(f\"      â€¢ {rel['entity_text']} â†’ {rel['relationship_type']} ({rel['business_impact']})\")\n        else:\n            print(f\"   âŒ Failed: {result.get('error', 'Unknown error')}\")\n        \n        # Brief pause between filings\n        if i < len(filings):\n            time.sleep(1)\n    \n    batch_time = time.time() - batch_start\n    \n    # Update pipeline statistics\n    entity_pipeline.pipeline_stats['documents_processed'] += successful\n    entity_pipeline.pipeline_stats['total_entities_extracted'] += total_entities\n    pipeline_storage.storage_stats['filings_processed'] += successful\n    \n    return {\n        'success': successful > 0,\n        'filings_processed': len(filings),\n        'successful_filings': successful,\n        'failed_filings': len(filings) - successful,\n        'total_entities_extracted': total_entities,\n        'total_relationships_found': total_relationships,\n        'batch_processing_time': round(batch_time, 2),\n        'avg_time_per_filing': round(batch_time / len(filings), 2) if filings else 0,\n        'results': results\n    }\n\n# ================================================================================\n# QUICK ACCESS FUNCTIONS\n# ================================================================================\n\ndef test_pipeline(company_domain: str = None):\n    \"\"\"Test the pipeline with a single filing\"\"\"\n    print(\"\\nğŸ§ª Testing in-memory pipeline...\")\n    \n    # Get one filing\n    if company_domain:\n        # Modify get_unprocessed_filings to accept company filter\n        # For now, just get any filing\n        filings = get_unprocessed_filings(limit=1)\n    else:\n        filings = get_unprocessed_filings(limit=1)\n    \n    if not filings:\n        print(\"âŒ No test filings available\")\n        return None\n    \n    result = process_filing_with_pipeline(filings[0])\n    \n    if result['success']:\n        print(f\"\\nâœ… Pipeline test successful!\")\n        print(f\"   ğŸ“Š Sections: {result['sections_processed']}\")\n        print(f\"   ğŸ” Entities: {result['entities_extracted']}\")\n        print(f\"   ğŸ”— Relationships: {result['relationships_found']}\")\n        print(f\"   ğŸ’¾ Stored: {result['entities_stored']} entities, {result['relationships_stored']} relationships\")\n        print(f\"   â±ï¸ Time: {result['processing_time']}s\")\n    else:\n        print(f\"\\nâŒ Pipeline test failed: {result.get('error')}\")\n    \n    return result\n\nprint(\"\\nâœ… In-Memory Pipeline Components Ready!\")\nprint(\"   ğŸ¯ RelationshipExtractor with Llama 3.1\")\nprint(\"   ğŸ’¾ Atomic storage for entities + relationships\")\nprint(\"   ğŸš€ In-memory processing (no DB round-trips)\")\nprint(\"   ğŸ“Š Usage: batch_results = process_filings_batch(limit=5)\")\nprint(\"   ğŸ§ª Test: test_result = test_pipeline()\")\n\ndef generate_pipeline_analytics_report() -> None:\n    \"\"\"Generate comprehensive analytics report for the pipeline\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“Š ENTITYEXTRACTIONPIPELINE ANALYTICS DASHBOARD\")\n    print(\"=\"*80)\n    \n    # Database overview with enhanced metrics\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Enhanced database statistics\n        cursor.execute(\"\"\"\n            SELECT \n                COUNT(*) as total_entities,\n                COUNT(DISTINCT company_domain) as companies,\n                COUNT(DISTINCT sec_filing_ref) as filings,\n                COUNT(DISTINCT entity_category) as entity_types,\n                AVG(confidence_score) as avg_confidence,\n                COUNT(*) FILTER (WHERE is_merged = true) as merged_entities,\n                COUNT(DISTINCT primary_model) as active_models,\n                COUNT(DISTINCT section_name) as sections_processed,\n                COUNT(*) FILTER (WHERE section_name IS NOT NULL AND section_name != '') as entities_with_sections,\n                MAX(extraction_timestamp) as last_extraction\n            FROM system_uno.sec_entities_raw\n            WHERE data_source = 'sec_filings'\n        \"\"\")\n        \n        db_overview = cursor.fetchone()\n        \n        if db_overview and db_overview[0] > 0:\n            total_entities = db_overview[0]\n            entities_with_sections = db_overview[8]\n            section_success_rate = (entities_with_sections / total_entities * 100) if total_entities > 0 else 0\n            \n            print(f\"\\nğŸ“ˆ DATABASE OVERVIEW:\")\n            print(f\"   Total Entities Extracted: {db_overview[0]:,}\")\n            print(f\"   Companies Processed: {db_overview[1]:,}\")\n            print(f\"   SEC Filings Analyzed: {db_overview[2]:,}\")\n            print(f\"   Entity Categories Found: {db_overview[3]:,}\")\n            print(f\"   Average Confidence Score: {db_overview[4]:.3f}\")\n            print(f\"   Multi-Model Entities: {db_overview[5]:,} ({db_overview[5]/db_overview[0]*100:.1f}%)\")\n            print(f\"   Active Models: {db_overview[6]:,}\")\n            print(f\"   Unique Sections Found: {db_overview[7]:,}\")\n            print(f\"   ğŸ¯ SECTION SUCCESS RATE: {entities_with_sections:,}/{total_entities:,} ({section_success_rate:.1f}%)\")\n            print(f\"   Last Extraction: {db_overview[9] or 'Never'}\")\n            \n            # Alert if section success rate is low\n            if section_success_rate < 90 and total_entities > 10:\n                print(f\"   ğŸš¨ WARNING: Section success rate is {section_success_rate:.1f}% - Pipeline routing issue!\")\n            elif section_success_rate >= 90:\n                print(f\"   âœ… EXCELLENT: Section success rate is {section_success_rate:.1f}% - Pipeline working correctly!\")\n        else:\n            print(f\"\\nğŸ“ˆ DATABASE OVERVIEW: No entities found - database is clean for testing\")\n        \n        cursor.close()\n        conn.close()\n                \n    except Exception as e:\n        print(f\"   âŒ Could not retrieve analytics: {e}\")\n    \n    # Pipeline statistics\n    try:\n        pipeline_stats = entity_pipeline.get_pipeline_statistics()\n        \n        print(f\"\\nğŸ”§ PIPELINE STATISTICS:\")\n        print(f\"   Documents Processed: {pipeline_stats['pipeline_stats']['documents_processed']:,}\")\n        print(f\"   Total Entities Found: {pipeline_stats['pipeline_stats']['total_entities_extracted']:,}\")\n        print(f\"   Processing Time: {pipeline_stats['pipeline_stats']['processing_time_total']:.2f}s\")\n        print(f\"   Device: {pipeline_stats['device']}\")\n        print(f\"   Loaded Models: {', '.join(pipeline_stats['loaded_models'])}\")\n        print(f\"   Supported Sources: {', '.join(pipeline_stats['supported_data_sources'])}\")\n        \n        # Individual model statistics\n        print(f\"\\nğŸ“Š INDIVIDUAL MODEL PERFORMANCE:\")\n        for model_name, stats in pipeline_stats['model_stats'].items():\n            if stats['texts_processed'] > 0:\n                entities_per_text = stats['entities_found'] / stats['texts_processed']\n                avg_time = stats['processing_time'] / stats['texts_processed']\n                print(f\"   {model_name:>12}: {stats['texts_processed']:>4} texts | {stats['entities_found']:>5} entities | {entities_per_text:>4.1f} avg/text | {avg_time:>4.2f}s avg\")\n        \n        # Storage statistics\n        storage_stats = pipeline_storage.storage_stats\n        if storage_stats['total_entities_stored'] > 0:\n            print(f\"\\nğŸ’¾ STORAGE STATISTICS:\")\n            print(f\"   Entities Stored: {storage_stats['total_entities_stored']:,}\")\n            print(f\"   Filings Processed: {storage_stats['filings_processed']:,}\")\n            print(f\"   Merged Entities: {storage_stats['merged_entities']:,}\")\n            print(f\"   Single-Model Entities: {storage_stats['single_model_entities']:,}\")\n            print(f\"   Failed Inserts: {storage_stats['failed_inserts']:,}\")\n            \n            merge_rate = (storage_stats['merged_entities'] / storage_stats['total_entities_stored'] * 100) if storage_stats['total_entities_stored'] > 0 else 0\n            print(f\"   Multi-Model Detection Rate: {merge_rate:.1f}%\")\n    \n    except Exception as e:\n        print(f\"\\nğŸ”§ Pipeline statistics unavailable: {e}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"âœ… EntityExtractionPipeline Analytics Complete!\")\n    print(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:10:58.442080Z","iopub.execute_input":"2025-09-13T18:10:58.443039Z","iopub.status.idle":"2025-09-13T18:14:18.237962Z","shell.execute_reply.started":"2025-09-13T18:10:58.443014Z","shell.execute_reply":"2025-09-13T18:14:18.237655Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad42f991e34438b95adff84f43cad5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f5b69db2134668b76f3215e4c07c4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eafb55ed734e475e8ed5b711e08453a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68022832067b4f8088155b24ea3abe30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af17271b224a436fba9c23941bbf812e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b73d672749e4baabe9489d27cc7eb18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8dbb3cae6344d0987eaeecfeb45d52e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7a25cd020ac416f9aa41b4d8e3e5726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcff44915e5846629249ea619fc3f9f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244d853240e1483c9f054ce968f5f189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac313820af34987b8d4288f80e24388"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69395b394ac34b20a7c19df61fc79eb8"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Cell 5: Main Processing Pipeline with Relationship Extraction\n\nprint(\"=\"*80)\nprint(\"ğŸš€ STARTING SEC FILING PROCESSING PIPELINE\")\nprint(\"=\"*80)\n\n# Configure processing parameters\n# CONFIG[\"processing\"][\"filing_batch_size\"] = 3  # REMOVED: Use CONFIG[\"processing\"][\"filing_batch_size\"] instead  # Number of filings to process in this run\n# CONFIG[\"processing\"][\"enable_relationships\"] = True  # REMOVED: Use CONFIG[\"processing\"][\"enable_relationships\"] instead  # Set to False to skip relationship extraction\n\n# Llama 3.1-8B is loaded locally, no API key needed\nprint(\"ğŸ“ Relationship extraction enabled with local Llama 3.1-8B\")\n\n# # Check if we have a Groq API key for relationship extraction\nprint(\"   â„¹ï¸ Using local Llama 3.1-8B for relationship extraction\")\n#     print(\"   To enable relationships, add GROQ_API_KEY to Kaggle secrets\")\n\n# Check for available unprocessed filings\nprint(\"\\nğŸ“Š Checking for unprocessed filings...\")\navailable_filings = get_unprocessed_filings(limit=CONFIG[\"processing\"][\"filing_query_limit\"])\nprint(f\"   Found {len(available_filings)} unprocessed filings\")\n\nif available_filings:\n    print(\"\\nğŸ“‹ Available filings to process:\")\n    for i, filing in enumerate(available_filings[:5], 1):\n        print(f\"   {i}. {filing['company_domain']} - {filing['filing_type']} ({filing['filing_date']})\")\n    \n    # Process the batch\n    print(f\"\\nğŸ”„ Processing {min(CONFIG[\"processing\"][\"filing_batch_size\"], len(available_filings))} filings...\")\n    print(\"-\"*60)\n    \n    # Run the pipeline\n    batch_results = process_filings_batch(limit=CONFIG[\"processing\"][\"filing_batch_size\"])\n    \n    # Display results summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"ğŸ“Š PROCESSING SUMMARY\")\n    print(\"=\"*80)\n    \n    if batch_results['success']:\n        print(f\"âœ… Successfully processed {batch_results['successful_filings']}/{batch_results['filings_processed']} filings\")\n        print(f\"   â€¢ Total entities extracted: {batch_results['total_entities_extracted']:,}\")\n        print(f\"   â€¢ Total relationships found: {batch_results['total_relationships_found']:,}\")\n        print(f\"   â€¢ Total processing time: {batch_results['batch_processing_time']:.1f}s\")\n        print(f\"   â€¢ Average time per filing: {batch_results['avg_time_per_filing']:.1f}s\")\n        \n        # Show detailed results for each filing\n        print(f\"\\nğŸ“ˆ Detailed Results:\")\n        for i, result in enumerate(batch_results['results'], 1):\n            if result['success']:\n                print(f\"\\n   Filing {i}: {result['company_domain']} - {result['filing_type']}\")\n                print(f\"      âœ“ Sections: {result['sections_processed']}\")\n                print(f\"      âœ“ Entities: {result['entities_extracted']}\")\n                print(f\"      âœ“ Relationships: {result['relationships_found']}\")\n                print(f\"      âœ“ Time: {result['processing_time']:.1f}s\")\n            else:\n                print(f\"\\n   Filing {i}: FAILED - {result.get('error', 'Unknown error')}\")\n        \n        # Show pipeline statistics\n        print(f\"\\nğŸ“Š Pipeline Statistics:\")\n        print(f\"   â€¢ Documents processed (total): {entity_pipeline.pipeline_stats['documents_processed']}\")\n        print(f\"   â€¢ Entities extracted (total): {entity_pipeline.pipeline_stats['total_entities_extracted']}\")\n        print(f\"   â€¢ Storage transactions: {pipeline_storage.storage_stats['transactions_completed']} successful, {pipeline_storage.storage_stats['transactions_failed']} failed\")\n        print(f\"   â€¢ Merged entities: {pipeline_storage.storage_stats['merged_entities']}\")\n        print(f\"   â€¢ Single-model entities: {pipeline_storage.storage_stats['single_model_entities']}\")\n        \n    else:\n        print(f\"âŒ Processing failed: {batch_results.get('message', 'Unknown error')}\")\n    \n    # Generate analytics report\n    print(\"\\n\" + \"=\"*80)\n    generate_pipeline_analytics_report()\n    \nelse:\n    print(\"\\nâš ï¸ No unprocessed filings found in raw_data.sec_filings\")\n    print(\"   All available filings have already been processed\")\n    print(\"\\nğŸ’¡ To add new filings:\")\n    print(\"   1. Insert new records into raw_data.sec_filings with accession_number\")\n    print(\"   2. Make sure the accession_number is valid (20 characters)\")\n    print(\"   3. Run this cell again to process them\")\n\nprint(\"\\nâœ… Pipeline execution complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T18:14:18.239084Z","iopub.execute_input":"2025-09-13T18:14:18.239404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Execute Pipeline\n\nprint(f\"\\nğŸ¯ PRODUCTION COMMANDS:\")\nprint(f\"   â€¢ Process new filings: batch_results = process_filings_batch(limit=5)\")\nprint(f\"   â€¢ Check results:       generate_pipeline_analytics_report()\")\nprint(f\"   â€¢ View statistics:     context_retriever.get_retrieval_statistics()\")\n\nprint(f\"\\nâœ… EntityExtractionPipeline Production Interface Ready!\")\nprint(f\"ğŸ”§ SINGLE ENTRY POINT: process_filings_batch() - ensures all extractions use section-based pipeline\")\nprint(f\"ğŸ“Š Database cleared - ready for fresh testing with guaranteed section extraction!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: TEST ENTITY STORAGE - Quick Verification Without Full Pipeline\n# This cell tests ONLY the storage functionality with mock entities\n\nprint(\"=\"*80)\nprint(\"ğŸ§ª ENTITY STORAGE TEST - Verify Storage Works Without 90-Minute Run\")\nprint(\"=\"*80)\n\nimport uuid\nfrom datetime import datetime\n\n# Create mock entities that match the expected structure\nprint(\"\\nğŸ“ Creating mock entities for storage test...\")\n\n# Create test entities with ALL required fields\ntest_entities = [\n    {\n        'extraction_id': str(uuid.uuid4()),\n        'company_domain': 'test.com',\n        'entity_text': 'Test Company Inc',\n        'entity_type': 'ORGANIZATION',\n        'entity_category': 'ORGANIZATION',  # Some code uses entity_category\n        'confidence_score': 0.95,\n        'char_start': 100,\n        'char_end': 117,\n        'character_start': 100,  # Some code uses character_start\n        'character_end': 117,\n        'surrounding_text': 'This is Test Company Inc in the context',\n        'models_detected': ['bert_base', 'roberta'],\n        'all_confidences': {'bert_base': 0.94, 'roberta': 0.96},\n        'primary_model': 'roberta',\n        'entity_variations': {'bert_base': 'Test Company Inc', 'roberta': 'Test Company Inc.'},\n        'is_merged': True,\n        'section_name': 'test_section',\n        'data_source': 'sec_filings',\n        'extraction_timestamp': datetime.now().isoformat(),\n        'original_label': 'ORG',\n        'model_source': 'roberta',  # Fallback field\n        'quality_score': 0.92,\n        'consensus_count': 2,\n        'detecting_models': ['bert_base', 'roberta'],\n        'consensus_score': 0.95,\n        'filing_id': 999999,\n        'sec_filing_ref': 'SEC_999999'\n    },\n    {\n        'extraction_id': str(uuid.uuid4()),\n        'company_domain': 'test.com',\n        'entity_text': 'John Smith',\n        'entity_type': 'PERSON',\n        'entity_category': 'PERSON',\n        'confidence_score': 0.88,\n        'char_start': 200,\n        'char_end': 210,\n        'character_start': 200,\n        'character_end': 210,\n        'surrounding_text': 'CEO John Smith announced',\n        'models_detected': ['bert_base'],\n        'all_confidences': {'bert_base': 0.88},\n        'primary_model': 'bert_base',\n        'entity_variations': {'bert_base': 'John Smith'},\n        'is_merged': False,\n        'section_name': 'test_section',\n        'data_source': 'sec_filings',\n        'extraction_timestamp': datetime.now().isoformat(),\n        'original_label': 'PER',\n        'model_source': 'bert_base',\n        'quality_score': 0.85,\n        'consensus_count': 1,\n        'detecting_models': ['bert_base'],\n        'consensus_score': 0.88,\n        'filing_id': 999999,\n        'sec_filing_ref': 'SEC_999999'\n    },\n    {\n        'extraction_id': str(uuid.uuid4()),\n        'company_domain': 'test.com',\n        'entity_text': 'FDA',\n        'entity_type': 'ORGANIZATION',\n        'entity_category': 'ORGANIZATION',\n        'confidence_score': 0.99,\n        'char_start': 300,\n        'char_end': 303,\n        'character_start': 300,\n        'character_end': 303,\n        'surrounding_text': 'approved by the FDA for clinical',\n        'models_detected': ['bert_base', 'roberta', 'biobert'],\n        'all_confidences': {'bert_base': 0.98, 'roberta': 0.99, 'biobert': 1.0},\n        'primary_model': 'biobert',\n        'entity_variations': {'bert_base': 'FDA', 'roberta': 'FDA', 'biobert': 'FDA'},\n        'is_merged': True,\n        'section_name': 'test_section',\n        'data_source': 'sec_filings',\n        'extraction_timestamp': datetime.now().isoformat(),\n        'original_label': 'ORG',\n        'model_source': 'biobert',\n        'quality_score': 0.99,\n        'consensus_count': 3,\n        'detecting_models': ['bert_base', 'roberta', 'biobert'],\n        'consensus_score': 0.99,\n        'filing_id': 999999,\n        'sec_filing_ref': 'SEC_999999'\n    }\n]\n\nprint(f\"âœ… Created {len(test_entities)} test entities\")\nprint(\"\\nğŸ“Š Test entity details:\")\nfor i, entity in enumerate(test_entities, 1):\n    print(f\"   {i}. {entity['entity_text']} ({entity['entity_type']}) - confidence: {entity['confidence_score']:.2f}\")\n\n# Test storage\nprint(\"\\nğŸ’¾ Testing entity storage...\")\nprint(\"   Using PipelineEntityStorage from Cell 4...\")\n\ntry:\n    # Initialize storage (assuming NEON_CONFIG is available from Cell 0)\n    test_storage = PipelineEntityStorage(NEON_CONFIG)\n    print(\"   âœ… Storage initialized successfully\")\n    \n    # Attempt to store test entities\n    filing_ref = \"SEC_TEST_999999\"\n    print(f\"\\n   ğŸ“¤ Attempting to store {len(test_entities)} entities...\")\n    \n    success = test_storage.store_entities(test_entities, filing_ref)\n    \n    if success:\n        print(\"   âœ… STORAGE SUCCESSFUL! Entities stored to database\")\n        \n        # Verify by querying the database\n        print(\"\\n   ğŸ” Verifying stored entities...\")\n        import psycopg2\n        \n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        cursor.execute(\"\"\"\n            SELECT COUNT(*), \n                   COUNT(DISTINCT entity_text),\n                   AVG(confidence_score)::numeric(4,3),\n                   AVG(quality_score)::numeric(4,3),\n                   AVG(consensus_count)\n            FROM system_uno.sec_entities_raw\n            WHERE sec_filing_ref = %s\n        \"\"\", (filing_ref,))\n        \n        result = cursor.fetchone()\n        if result and result[0] > 0:\n            print(f\"   âœ… VERIFIED: {result[0]} entities found in database\")\n            print(f\"      â€¢ Unique entities: {result[1]}\")\n            print(f\"      â€¢ Avg confidence: {result[2]}\")\n            print(f\"      â€¢ Avg quality: {result[3]}\")\n            print(f\"      â€¢ Avg consensus: {result[4]}\")\n        else:\n            print(\"   âŒ WARNING: Entities not found in database after storage\")\n        \n        # Clean up test data\n        print(\"\\n   ğŸ§¹ Cleaning up test data...\")\n        cursor.execute(\"DELETE FROM system_uno.sec_entities_raw WHERE sec_filing_ref = %s\", (filing_ref,))\n        conn.commit()\n        deleted = cursor.rowcount\n        print(f\"   âœ… Cleaned up {deleted} test entities\")\n        \n        cursor.close()\n        conn.close()\n        \n    else:\n        print(\"   âŒ STORAGE FAILED! Check error messages above\")\n        print(\"   âš ï¸  The entity storage is NOT working correctly\")\n        \nexcept Exception as e:\n    print(f\"   âŒ ERROR during storage test: {e}\")\n    print(\"   âš ï¸  This error needs to be fixed before running the full pipeline\")\n    import traceback\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ğŸ STORAGE TEST COMPLETE\")\nprint(\"=\"*80)\nprint(\"\\nğŸ“ Summary:\")\nprint(\"   â€¢ If you see 'âœ… STORAGE SUCCESSFUL', the storage is working\")\nprint(\"   â€¢ If you see 'âŒ STORAGE FAILED', check the error and fix before running full pipeline\")\nprint(\"   â€¢ This test takes <10 seconds vs 90 minutes for full pipeline\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}