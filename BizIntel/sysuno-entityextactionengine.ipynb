{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: GitHub Setup and Enhanced Import Configuration with PHASE 3 Improvements\n\n# Install required packages first\n!pip install edgartools transformers torch requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k groq --quiet\n\nimport os\nimport sys\nimport importlib\nimport importlib.util\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom psycopg2 import pool\nimport time\nimport json\nimport pickle\nimport traceback\nfrom pathlib import Path\nfrom functools import wraps\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime\nfrom edgar import set_identity\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION - All settings in one place\n# ============================================================================\n\n# Use Kaggle secrets for all sensitive credentials\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n# Database Configuration (using Kaggle secrets for security)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"),\n    'user': user_secrets.get_secret(\"NEON_USER\"), \n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'sslmode': 'require'\n}\n\n# Master Configuration Dictionary - PHASE 3 ENHANCED\nCONFIG = {\n    # GitHub Settings\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': f\"https://{user_secrets.get_secret('GITHUB_TOKEN')}@github.com/amiralpert/SmartReach.git\",\n        'local_path': \"/kaggle/working/SmartReach\"\n    },\n    \n    # Database Settings\n    'database': NEON_CONFIG,\n    \n    # Connection Pool Settings - PHASE 3\n    'pool': {\n        'min_connections': 2,\n        'max_connections': 10,\n        'keepalives': 1,\n        'keepalives_idle': 30,\n        'keepalives_interval': 10,\n        'keepalives_count': 5\n    },\n    \n    # Connection Retry Settings\n    'retry': {\n        'max_attempts': 3,\n        'initial_delay': 1,  # seconds\n        'exponential_base': 2,\n        'max_delay': 30  # seconds\n    },\n    \n    # Model Configuration - PHASE 3 ENHANCED\n    'models': {\n        'confidence_threshold': 0.8, #confidence NER model needs to store an entity \n        'batch_size': 16, # number of text chunks to process at one time through NER\n        'max_length': 512, #max chunk size to process through NER 512 token limit for BERT models \n        'chunk_overlap': 0.1,  # 10% overlap between chunks for complete entity extraction\n        'warm_up_enabled': True,  # PHASE 3: Model warm-up\n        'warm_up_text': 'Pfizer announced FDA approval for new cancer drug targeting BRCA mutations.'\n    },\n    \n    # Cache Settings - PHASE 2 ENHANCED\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 100,  # Maximum cache size in MB\n        'ttl': 3600,  # seconds\n        'eviction_policy': 'LRU'  # Least Recently Used\n    },\n    \n    # Processing Settings - PHASE 3 ENHANCED  \n    'processing': {\n        'filing_batch_size': 3,\n        'entity_batch_size': 100,  # Max entities per database insert\n        'section_validation': True,  # Enforce section name validation\n        'debug_mode': False,\n        'max_insert_batch': 500,  # Maximum batch for database inserts\n        'deprecation_warnings': True,  # Show warnings for deprecated functions\n        'checkpoint_enabled': True,  # PHASE 3: Enable checkpointing\n        'checkpoint_dir': '/kaggle/working/checkpoints',  # PHASE 3: Checkpoint directory\n        'deduplication_threshold': 0.85  # PHASE 3: Similarity threshold for dedup\n    },\n    \n    # EdgarTools Settings\n    'edgar': {\n        'identity': \"SmartReach BizIntel amir@leanbio.consulting\"\n    }\n}\n\nif not CONFIG['github']['token']:\n    raise ValueError(\"❌ GITHUB_TOKEN is required in Kaggle secrets\")\n\nif not CONFIG['database']['password']:\n    raise ValueError(\"❌ NEON_PASSWORD is required in Kaggle secrets\")\n\nprint(\"✅ Configuration loaded from Kaggle secrets\")\nprint(f\"   Database: {CONFIG['database']['host']}\")\nprint(f\"   Processing: Batch size={CONFIG['processing']['filing_batch_size']}, Section validation={CONFIG['processing']['section_validation']}\")\nprint(f\"   Cache: Max size={CONFIG['cache']['max_size_mb']}MB, TTL={CONFIG['cache']['ttl']}s\")\nprint(f\"   Database batching: Max insert batch={CONFIG['processing']['max_insert_batch']}\")\nprint(f\"   Checkpointing: {'Enabled' if CONFIG['processing']['checkpoint_enabled'] else 'Disabled'}\")\n\n# ============================================================================\n# CONNECTION RETRY DECORATOR\n# ============================================================================\n\ndef retry_on_connection_error(func):\n    \"\"\"Decorator to retry database operations on connection errors\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        max_attempts = CONFIG['retry']['max_attempts']\n        delay = CONFIG['retry']['initial_delay']\n        \n        for attempt in range(max_attempts):\n            try:\n                return func(*args, **kwargs)\n            except (psycopg2.OperationalError, psycopg2.InterfaceError) as e:\n                if attempt == max_attempts - 1:\n                    error_msg = f\"ERROR [ConnectionRetry]: Failed after {max_attempts} attempts - {type(e).__name__}: {str(e)}\"\n                    print(error_msg)\n                    if logger:\n                        logger.log(error_msg)\n                    raise\n                \n                wait_time = min(delay * (CONFIG['retry']['exponential_base'] ** attempt), CONFIG['retry']['max_delay'])\n                print(f\"WARNING [ConnectionRetry]: Attempt {attempt + 1}/{max_attempts} failed. Retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n        \n        return None\n    return wrapper\n\n# ============================================================================\n# PHASE 3: ENHANCED ERROR LOGGING WITH STACK TRACES\n# ============================================================================\n\ndef log_error(component: str, message: str, exception: Exception = None, context: dict = None):\n    \"\"\"Enhanced error logging with stack traces\"\"\"\n    if exception:\n        error_msg = f\"ERROR [{component}]: {message} - {type(exception).__name__}: {str(exception)}\"\n        # Add stack trace for debugging\n        if CONFIG['processing'].get('debug_mode', False):\n            error_msg += f\"\\nStack trace:\\n{traceback.format_exc()}\"\n    else:\n        error_msg = f\"ERROR [{component}]: {message}\"\n    \n    if context:\n        error_msg += f\" | Context: {context}\"\n    \n    print(error_msg)  # Auto-logger captures this\n    return error_msg\n\ndef log_warning(component: str, message: str, context: dict = None):\n    \"\"\"Standardized warning logging\"\"\"\n    warning_msg = f\"WARNING [{component}]: {message}\"\n    if context:\n        warning_msg += f\" | Context: {context}\"\n    print(warning_msg)\n    return warning_msg\n\ndef log_info(component: str, message: str):\n    \"\"\"Standardized info logging\"\"\"\n    info_msg = f\"INFO [{component}]: {message}\"\n    print(info_msg)\n    return info_msg\n\n# ============================================================================\n# PHASE 3: ENHANCED DATABASE MANAGER WITH CONNECTION POOLING\n# ============================================================================\n\nclass DatabaseManager:\n    \"\"\"Singleton database manager with connection pooling\"\"\"\n    \n    _instance = None\n    _pool = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(DatabaseManager, cls).__new__(cls)\n        return cls._instance\n    \n    def __init__(self):\n        if DatabaseManager._pool is None:\n            self._initialize_pool()\n    \n    def _initialize_pool(self):\n        \"\"\"Initialize connection pool\"\"\"\n        try:\n            DatabaseManager._pool = psycopg2.pool.ThreadedConnectionPool(\n                CONFIG['pool']['min_connections'],\n                CONFIG['pool']['max_connections'],\n                **CONFIG['database'],\n                keepalives=CONFIG['pool']['keepalives'],\n                keepalives_idle=CONFIG['pool']['keepalives_idle'],\n                keepalives_interval=CONFIG['pool']['keepalives_interval'],\n                keepalives_count=CONFIG['pool']['keepalives_count']\n            )\n            log_info(\"DatabaseManager\", f\"Connection pool initialized with {CONFIG['pool']['max_connections']} max connections\")\n        except Exception as e:\n            log_error(\"DatabaseManager\", \"Failed to initialize connection pool\", e)\n            raise\n    \n    @contextmanager\n    def get_connection(self):\n        \"\"\"Get connection from pool with automatic return\"\"\"\n        conn = None\n        try:\n            conn = DatabaseManager._pool.getconn()\n            yield conn\n            conn.commit()\n        except Exception as e:\n            if conn:\n                conn.rollback()\n            raise e\n        finally:\n            if conn:\n                DatabaseManager._pool.putconn(conn)\n    \n    def close_all(self):\n        \"\"\"Close all connections in pool\"\"\"\n        if DatabaseManager._pool:\n            DatabaseManager._pool.closeall()\n            log_info(\"DatabaseManager\", \"All connections closed\")\n    \n    def get_pool_status(self) -> Dict:\n        \"\"\"Get current pool status\"\"\"\n        if DatabaseManager._pool:\n            return {\n                'minconn': DatabaseManager._pool.minconn,\n                'maxconn': DatabaseManager._pool.maxconn,\n                'closed': DatabaseManager._pool.closed\n            }\n        return {'status': 'not initialized'}\n\n# Initialize global database manager\nDB_MANAGER = DatabaseManager()\n\n# PHASE 2: Keep backward compatibility\n@contextmanager\ndef get_db_connection():\n    \"\"\"Legacy context manager - now uses DatabaseManager\"\"\"\n    with DB_MANAGER.get_connection() as conn:\n        yield conn\n\n# ============================================================================\n# PHASE 3: CHECKPOINT MANAGER FOR FAILURE RECOVERY\n# ============================================================================\n\nclass CheckpointManager:\n    \"\"\"Manage pipeline checkpoints for failure recovery\"\"\"\n    \n    def __init__(self, checkpoint_dir: str = None):\n        self.checkpoint_dir = Path(checkpoint_dir or CONFIG['processing']['checkpoint_dir'])\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.current_checkpoint = None\n        log_info(\"CheckpointManager\", f\"Initialized with directory: {self.checkpoint_dir}\")\n    \n    def save_checkpoint(self, state: Dict, checkpoint_name: str = None) -> str:\n        \"\"\"Save pipeline state to checkpoint\"\"\"\n        try:\n            if not checkpoint_name:\n                checkpoint_name = f\"checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            \n            checkpoint_path = self.checkpoint_dir / f\"{checkpoint_name}.pkl\"\n            \n            # Add metadata\n            state['checkpoint_metadata'] = {\n                'created_at': datetime.now().isoformat(),\n                'pipeline_version': '3.0',\n                'config_hash': hash(str(CONFIG))\n            }\n            \n            with open(checkpoint_path, 'wb') as f:\n                pickle.dump(state, f)\n            \n            self.current_checkpoint = checkpoint_path\n            log_info(\"CheckpointManager\", f\"Saved checkpoint: {checkpoint_name}\")\n            return str(checkpoint_path)\n            \n        except Exception as e:\n            log_error(\"CheckpointManager\", \"Failed to save checkpoint\", e)\n            return None\n    \n    def load_checkpoint(self, checkpoint_path: str = None) -> Optional[Dict]:\n        \"\"\"Load pipeline state from checkpoint\"\"\"\n        try:\n            if not checkpoint_path:\n                checkpoint_path = self.current_checkpoint\n            \n            if not checkpoint_path:\n                # Find latest checkpoint\n                checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n                if not checkpoints:\n                    log_warning(\"CheckpointManager\", \"No checkpoints found\")\n                    return None\n                checkpoint_path = max(checkpoints, key=lambda p: p.stat().st_mtime)\n            \n            with open(checkpoint_path, 'rb') as f:\n                state = pickle.load(f)\n            \n            log_info(\"CheckpointManager\", f\"Loaded checkpoint: {Path(checkpoint_path).name}\")\n            return state\n            \n        except Exception as e:\n            log_error(\"CheckpointManager\", \"Failed to load checkpoint\", e)\n            return None\n    \n    def list_checkpoints(self) -> List[Dict]:\n        \"\"\"List available checkpoints\"\"\"\n        checkpoints = []\n        for cp_file in self.checkpoint_dir.glob(\"checkpoint_*.pkl\"):\n            try:\n                stats = cp_file.stat()\n                checkpoints.append({\n                    'name': cp_file.stem,\n                    'path': str(cp_file),\n                    'size_mb': stats.st_size / (1024 * 1024),\n                    'modified': datetime.fromtimestamp(stats.st_mtime).isoformat()\n                })\n            except:\n                continue\n        return sorted(checkpoints, key=lambda x: x['modified'], reverse=True)\n    \n    def cleanup_old_checkpoints(self, keep_last: int = 5):\n        \"\"\"Clean up old checkpoints\"\"\"\n        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n        checkpoints.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n        \n        for cp_file in checkpoints[keep_last:]:\n            try:\n                cp_file.unlink()\n                log_info(\"CheckpointManager\", f\"Deleted old checkpoint: {cp_file.name}\")\n            except:\n                continue\n\n# Initialize global checkpoint manager\nCHECKPOINT_MANAGER = CheckpointManager()\n\n# ============================================================================\n# PHASE 2: SIZE-LIMITED LRU CACHE\n# ============================================================================\n\nclass SizeLimitedLRUCache:\n    \"\"\"LRU cache with size limit in MB\"\"\"\n    \n    def __init__(self, max_size_mb: int):\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.cache = OrderedDict()\n        self.current_size = 0\n        self.hits = 0\n        self.misses = 0\n    \n    def _estimate_size(self, value: str) -> int:\n        \"\"\"Estimate size of cached value in bytes\"\"\"\n        return len(value.encode('utf-8')) if isinstance(value, str) else sys.getsizeof(value)\n    \n    def get(self, key: str):\n        \"\"\"Get item from cache\"\"\"\n        if key in self.cache:\n            self.hits += 1\n            # Move to end (most recently used)\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        self.misses += 1\n        return None\n    \n    def put(self, key: str, value, size: int = None):\n        \"\"\"Put item in cache with LRU eviction\"\"\"\n        if size is None:\n            size = self._estimate_size(value)\n        \n        # Remove old entries if needed\n        while self.current_size + size > self.max_size_bytes and self.cache:\n            evicted_key, evicted_value = self.cache.popitem(last=False)\n            self.current_size -= self._estimate_size(evicted_value)\n            log_info(\"Cache\", f\"Evicted {evicted_key} to maintain size limit\")\n        \n        # Add new entry\n        if key in self.cache:\n            self.current_size -= self._estimate_size(self.cache[key])\n        \n        self.cache[key] = value\n        self.current_size += size\n        self.cache.move_to_end(key)\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get cache statistics\"\"\"\n        hit_rate = (self.hits / (self.hits + self.misses) * 100) if (self.hits + self.misses) > 0 else 0\n        return {\n            'entries': len(self.cache),\n            'size_mb': self.current_size / (1024 * 1024),\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': hit_rate\n        }\n\n# Initialize global cache for EdgarTools sections\nSECTION_CACHE = SizeLimitedLRUCache(CONFIG['cache']['max_size_mb'])\n\n# ============================================================================\n# PHASE 2: DEPRECATION WARNING SYSTEM\n# ============================================================================\n\ndef deprecated(replacement_func: str = None):\n    \"\"\"Decorator to mark functions as deprecated\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if CONFIG['processing']['deprecation_warnings']:\n                msg = f\"Function '{func.__name__}' is deprecated and will be removed.\"\n                if replacement_func:\n                    msg += f\" Use '{replacement_func}' instead.\"\n                log_warning(\"Deprecation\", msg)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# ============================================================================\n# GITHUB SETUP\n# ============================================================================\n\nprint(\"\\n📦 Setting up GitHub repository...\")\nlocal_path = CONFIG['github']['local_path']\nrepo_url = CONFIG['github']['repo_url']\n\n# Clone or update repo with force pull\nif os.path.exists(local_path):\n    log_info(\"GitHub\", f\"Repository exists at {local_path}\")\n    log_info(\"GitHub\", \"Force updating from main branch\")\n    !cd {local_path} && git fetch origin\n    !cd {local_path} && git reset --hard origin/main\n    !cd {local_path} && git pull origin main\n    log_info(\"GitHub\", \"Repository updated successfully\")\n    \n    # Show current commit\n    !cd {local_path} && echo \"Current commit:\" && git log --oneline -1\nelse:\n    log_info(\"GitHub\", f\"Cloning repository to {local_path}\")\n    !git clone {repo_url} {local_path}\n    log_info(\"GitHub\", \"Repository cloned successfully\")\n\n# Clear any cached modules from previous runs\nmodules_to_clear = [key for key in sys.modules.keys() if 'auto_logger' in key.lower() or 'clean' in key.lower()]\nfor mod in modules_to_clear:\n    del sys.modules[mod]\n    log_info(\"ModuleCache\", f\"Cleared cached module: {mod}\")\n\n# Add to Python path for regular imports\nif f'{local_path}/BizIntel' in sys.path:\n    sys.path.remove(f'{local_path}/BizIntel')\nsys.path.insert(0, f'{local_path}/BizIntel')\n\nlog_info(\"Setup\", \"Python path configured for SEC entity extraction\")\n\n# Configure EdgarTools authentication - REQUIRED by SEC\nset_identity(CONFIG['edgar']['identity'])\nlog_info(\"EdgarTools\", f\"Identity configured: {CONFIG['edgar']['identity']}\")\n\n# ============================================================================\n# AUTO-LOGGER SETUP\n# ============================================================================\n\n@retry_on_connection_error\ndef setup_logger():\n    \"\"\"Set up the auto-logger with retry logic\"\"\"\n    log_info(\"AutoLogger\", \"Initializing clean logger with connection pooling\")\n    \n    # Import the redesigned clean auto-logger\n    logger_module_path = f\"{local_path}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\n    if os.path.exists(logger_module_path):\n        spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_module_path)\n        auto_logger_module = importlib.util.module_from_spec(spec)\n        sys.modules[\"auto_logger\"] = auto_logger_module\n        spec.loader.exec_module(auto_logger_module)\n\n        # Use the new clean logging setup with DB_MANAGER for connection pooling\n        setup_clean_logging = auto_logger_module.setup_clean_logging\n        logger = setup_clean_logging(DB_MANAGER, \"SEC_EntityExtraction\")\n        \n        log_info(\"AutoLogger\", \"Clean auto-logging enabled successfully\")\n        print(\"📋 Features:\")\n        print(\"   • One row per cell execution\")\n        print(\"   • Complete output capture\")\n        print(\"   • Proper cell numbers from # Cell N: comments\")\n        print(\"   • Full error tracebacks\")\n        print(\"   • Execution timing\")\n        print(\"   • Connection pooling for reliability\")\n        return logger\n    else:\n        log_error(\"AutoLogger\", f\"Logger module not found at {logger_module_path}\")\n        return None\n\ntry:\n    logger = setup_logger()\nexcept Exception as e:\n    log_error(\"AutoLogger\", \"Logger setup failed\", e)\n    log_warning(\"AutoLogger\", \"Continuing without auto-logging\")\n    logger = None\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🚀 SEC ENTITY EXTRACTION ENGINE INITIALIZED - PHASE 3 PRODUCTION-READY\")\nprint(\"=\"*80)\nprint(f\"✅ GitHub: Repository at {CONFIG['github']['local_path']}\")\nprint(f\"✅ Database: Connected to {CONFIG['database']['host']}\")\nprint(f\"✅ EdgarTools: Configured as '{CONFIG['edgar']['identity']}'\")\nprint(f\"✅ Logging: {'Enabled' if logger else 'Disabled'}\")\nprint(f\"✅ Section Validation: {'ENFORCED' if CONFIG['processing']['section_validation'] else 'Disabled'}\")\nprint(f\"✅ PHASE 2 Enhancements:\")\nprint(f\"   • Database context manager: get_db_connection()\")\nprint(f\"   • Size-limited LRU cache: {CONFIG['cache']['max_size_mb']}MB\")\nprint(f\"   • Batch size limits: Max {CONFIG['processing']['max_insert_batch']} per insert\")\nprint(f\"   • Deprecation warnings: {'Enabled' if CONFIG['processing']['deprecation_warnings'] else 'Disabled'}\")\nprint(f\"✅ PHASE 3 PRODUCTION FEATURES:\")\nprint(f\"   • Connection Pool: {CONFIG['pool']['max_connections']} max connections\")\nprint(f\"   • Checkpoint System: Recovery from failures enabled\")\nprint(f\"   • Enhanced Error Logging: Stack traces included\")\nprint(f\"   • Model Warm-up: Enabled for faster first inference\")\nprint(f\"   • Entity Deduplication: {CONFIG['processing']['deduplication_threshold']} threshold\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:08:20.284603Z","iopub.execute_input":"2025-09-08T02:08:20.284905Z","iopub.status.idle":"2025-09-08T02:09:58.212834Z","shell.execute_reply.started":"2025-09-08T02:08:20.284885Z","shell.execute_reply":"2025-09-08T02:09:58.212138Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for uuid (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 24.1.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✅ Configuration loaded from Kaggle secrets\n   Database: ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech\n   Processing: Batch size=3, Section validation=True\n   Cache: Max size=100MB, TTL=3600s\n   Database batching: Max insert batch=500\n   Checkpointing: Enabled\nINFO [DatabaseManager]: Connection pool initialized with 10 max connections\nINFO [CheckpointManager]: Initialized with directory: /kaggle/working/checkpoints\n\n📦 Setting up GitHub repository...\nINFO [GitHub]: Cloning repository to /kaggle/working/SmartReach\nCloning into '/kaggle/working/SmartReach'...\nremote: Enumerating objects: 509, done.\u001b[K\nremote: Counting objects: 100% (157/157), done.\u001b[K\nremote: Compressing objects: 100% (112/112), done.\u001b[K\nremote: Total 509 (delta 102), reused 85 (delta 45), pack-reused 352 (from 1)\u001b[K\nReceiving objects: 100% (509/509), 592.24 KiB | 11.61 MiB/s, done.\nResolving deltas: 100% (269/269), done.\nINFO [GitHub]: Repository cloned successfully\nINFO [Setup]: Python path configured for SEC entity extraction\nINFO [EdgarTools]: Identity configured: SmartReach BizIntel amir@leanbio.consulting\nINFO [AutoLogger]: Initializing clean logger with connection pooling\nERROR [AutoLogger]: Logger setup failed - IndentationError: unexpected indent (auto_logger.py, line 72)\nWARNING [AutoLogger]: Continuing without auto-logging\n\n================================================================================\n🚀 SEC ENTITY EXTRACTION ENGINE INITIALIZED - PHASE 3 PRODUCTION-READY\n================================================================================\n✅ GitHub: Repository at /kaggle/working/SmartReach\n✅ Database: Connected to ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech\n✅ EdgarTools: Configured as 'SmartReach BizIntel amir@leanbio.consulting'\n✅ Logging: Disabled\n✅ Section Validation: ENFORCED\n✅ PHASE 2 Enhancements:\n   • Database context manager: get_db_connection()\n   • Size-limited LRU cache: 100MB\n   • Batch size limits: Max 500 per insert\n   • Deprecation warnings: Enabled\n✅ PHASE 3 PRODUCTION FEATURES:\n   • Connection Pool: 10 max connections\n   • Checkpoint System: Recovery from failures enabled\n   • Enhanced Error Logging: Stack traces included\n   • Model Warm-up: Enabled for faster first inference\n   • Entity Deduplication: 0.85 threshold\n================================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Accession-Based SEC Content Extraction with EdgarTools - SIMPLIFIED\n\nimport edgar\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nimport requests\nimport re\nfrom typing import Dict, List, Optional, Tuple\nfrom bs4 import BeautifulSoup\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# REMOVED extract_accession_from_url - No longer needed since database has accession_number column\n\ndef get_filing_sections(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get structured sections from SEC filing using accession number\n    \n    PHASE 2 ENHANCED: Using cache for sections to avoid repeated API calls\n    \"\"\"\n    # Check cache first\n    cache_key = f\"{accession_number}#{filing_type or 'UNKNOWN'}\"\n    cached_sections = SECTION_CACHE.get(cache_key)\n    if cached_sections:\n        log_info(\"Cache\", f\"Cache hit for {accession_number}\")\n        return cached_sections\n    \n    try:\n        # Find filing using accession number\n        filing = find(accession_number)\n        \n        if not filing:\n            raise ValueError(f\"Filing not found for accession: {accession_number}\")\n            \n        # Auto-detect filing type if not provided\n        if not filing_type:\n            filing_type = getattr(filing, 'form', '10-K')\n        \n        log_info(\"EdgarTools\", f\"Found {filing_type} for {getattr(filing, 'company', 'Unknown Company')}\")\n        \n        # Get structured HTML content\n        html_content = filing.html()\n        if not html_content:\n            raise ValueError(\"No HTML content available\")\n        \n        # Parse HTML to Document object\n        document = parse_html(html_content)\n        \n        # Extract sections using SectionExtractor\n        extractor = SectionExtractor(filing_type=filing_type)\n        sections = extractor.extract(document)\n        \n        log_info(\"EdgarTools\", f\"SectionExtractor found {len(sections)} sections\")\n        \n        # Convert sections to text dictionary\n        section_texts = {}\n        for section_name, section in sections.items():\n            try:\n                if hasattr(section, 'text'):\n                    text = section.text() if callable(section.text) else section.text\n                    if isinstance(text, str) and text.strip():\n                        section_texts[section_name] = text.strip()\n                        print(f\"      • {section_name}: {len(text):,} chars\")\n                elif hasattr(section, '__str__'):\n                    text = str(section).strip()\n                    if text:\n                        section_texts[section_name] = text\n                        print(f\"      • {section_name}: {len(text):,} chars (via str)\")\n            except Exception as section_e:\n                log_warning(\"EdgarTools\", f\"Could not extract section {section_name}\", {\"error\": str(section_e)})\n                continue\n        \n        # If SectionExtractor returns no sections, fall back to full document text\n        if not section_texts:\n            log_warning(\"EdgarTools\", \"No structured sections found, using full document fallback\")\n            full_text = document.text() if hasattr(document, 'text') and callable(document.text) else str(document)\n            if full_text and len(full_text.strip()) > 100:  # Only use if substantial content\n                section_texts['full_document'] = full_text.strip()\n                log_warning('EdgarTools', f'Processing entire document ({len(full_text):,} chars) - this may take several minutes')\n                log_info(\"EdgarTools\", f\"Using full document: {len(full_text):,} chars\")\n        \n        # Cache the result\n        if section_texts and CONFIG['cache']['enabled']:\n            SECTION_CACHE.put(cache_key, section_texts)\n            log_info(\"Cache\", f\"Cached sections for {accession_number} ({len(section_texts)} sections)\")\n        \n        return section_texts\n        \n    except Exception as e:\n        log_error(\"EdgarTools\", f\"Failed to fetch filing {accession_number}\", e)\n        return {}  # Return empty dict on network/API failure\n\ndef route_sections_to_models(sections: Dict[str, str], filing_type: str) -> Dict[str, List[str]]:\n    \"\"\"Route sections to appropriate NER models based on filing type\"\"\"\n    routing = {\n        'biobert': [],\n        'bert_base': [],\n        'roberta': [],\n        'finbert': []\n    }\n    \n    if filing_type.upper() in ['10-K', '10-Q']:\n        for section_name, section_text in sections.items():\n            # FinBERT gets financial statements exclusively\n            if 'financial' in section_name.lower() or 'statement' in section_name.lower():\n                routing['finbert'].append(section_name)\n            else:\n                # All other sections go to BERT/RoBERTa/BioBERT\n                routing['bert_base'].append(section_name)\n                routing['roberta'].append(section_name)\n                routing['biobert'].append(section_name)\n    \n    elif filing_type.upper() == '8-K':\n        # 8-K: all item sections go to all four models\n        for section_name in sections.keys():\n            routing['biobert'].append(section_name)\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['finbert'].append(section_name)\n    \n    else:\n        # Default routing for other filing types\n        for section_name in sections.keys():\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['biobert'].append(section_name)\n    \n    # Remove empty routing\n    routing = {model: sections_list for model, sections_list in routing.items() if sections_list}\n    \n    return routing\n\ndef process_sec_filing_with_sections(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing with section-based extraction\n    \n    SIMPLIFIED: Now uses accession_number directly from database\n    \"\"\"\n    try:\n        filing_id = filing_data.get('id')\n        accession_number = filing_data.get('accession_number')  # DIRECT FROM DATABASE\n        filing_type = filing_data.get('filing_type', '10-K')\n        company_domain = filing_data.get('company_domain', 'Unknown')\n        filing_url = filing_data.get('url')  # Still keep for reference\n        \n        log_info(\"FilingProcessor\", f\"Processing {filing_type} for {company_domain}\")\n        print(f\"   📄 Filing ID: {filing_id}\")\n        print(f\"   📑 Accession: {accession_number}\")\n        \n        # Validate accession number\n        if not accession_number:\n            raise ValueError(f\"Missing accession number for filing {filing_id}\")\n        \n        # Get structured sections using accession directly\n        sections = get_filing_sections(accession_number, filing_type)\n        if not sections:\n            raise ValueError(\"No sections extracted\")\n        \n        log_info(\"FilingProcessor\", f\"Extracted {len(sections)} sections\")\n        \n        # Route sections to models\n        model_routing = route_sections_to_models(sections, filing_type)\n        print(f\"   🎯 Model routing: {[f'{model}: {len(secs)} sections' for model, secs in model_routing.items()]}\")\n        \n        # Validate section names if configured\n        if CONFIG['processing']['section_validation']:\n            missing_sections = [name for name in sections.keys() if not name]\n            if missing_sections:\n                log_warning(\"FilingProcessor\", f\"Found {len(missing_sections)} sections without names\")\n        \n        # Show cache statistics\n        cache_stats = SECTION_CACHE.get_stats()\n        if cache_stats['hits'] > 0:\n            print(f\"   📊 Cache: {cache_stats['hit_rate']:.1f}% hit rate, {cache_stats['size_mb']:.1f}MB used\")\n        \n        return {\n            'filing_id': filing_id,\n            'company_domain': company_domain,\n            'filing_type': filing_type,\n            'accession_number': accession_number,\n            'url': filing_url,\n            'sections': sections,\n            'model_routing': model_routing,\n            'total_sections': len(sections),\n            'processing_status': 'success'\n        }\n        \n    except Exception as e:\n        log_error(\"FilingProcessor\", \"Filing processing failed\", e, \n                 {\"filing_id\": filing_data.get('id'), \"accession\": filing_data.get('accession_number')})\n        return {\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain', 'Unknown'),\n            'filing_type': filing_data.get('filing_type', 'Unknown'),\n            'accession_number': filing_data.get('accession_number'),\n            'error': str(e),\n            'processing_status': 'failed'\n        }\n\n@retry_on_connection_error\ndef get_unprocessed_filings(limit: int = 5) -> List[Dict]:\n    \"\"\"Get SEC filings that haven't been processed yet\n    \n    ENHANCED: Now retrieves accession_number directly from database\n    \"\"\"\n    with get_db_connection() as conn:  # PHASE 2: Using context manager\n        cursor = conn.cursor()\n        \n        cursor.execute(\"\"\"\n            SELECT \n                sf.id, \n                sf.company_domain, \n                sf.filing_type, \n                sf.accession_number,  -- DIRECT FROM DATABASE\n                sf.url, \n                sf.filing_date, \n                sf.title\n            FROM raw_data.sec_filings sf\n            LEFT JOIN system_uno.sec_entities_raw ser \n                ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n            WHERE sf.accession_number IS NOT NULL  -- Must have accession\n                AND ser.sec_filing_ref IS NULL     -- Not yet processed\n            ORDER BY sf.filing_date DESC\n            LIMIT %s\n        \"\"\", (limit,))\n        \n        filings = cursor.fetchall()\n        cursor.close()\n        \n        log_info(\"DatabaseQuery\", f\"Retrieved {len(filings)} unprocessed filings with accession numbers\")\n        \n        return [{\n            'id': filing[0],\n            'company_domain': filing[1],\n            'filing_type': filing[2],\n            'accession_number': filing[3],  # ACCESSION NUMBER INCLUDED\n            'url': filing[4],\n            'filing_date': filing[5],\n            'title': filing[6]\n        } for filing in filings]\n\n# Test the simplified extraction\nlog_info(\"Test\", \"Starting simplified section extraction test (using direct accession)\")\n\ntest_filings = get_unprocessed_filings(limit=1)\n\nif test_filings:\n    test_result = process_sec_filing_with_sections(test_filings[0])\n    if test_result['processing_status'] == 'success':\n        log_info(\"Test\", f\"✅ Successfully extracted {test_result['total_sections']} sections\")\n    else:\n        log_error(\"Test\", f\"❌ Section extraction failed: {test_result.get('error')}\")\nelse:\n    log_info(\"Test\", \"No test filings available\")\n\nprint(\"✅ Cell 2 complete - EdgarTools section extraction ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:20.728084Z","iopub.execute_input":"2025-09-08T02:10:20.728828Z","iopub.status.idle":"2025-09-08T02:10:31.499514Z","shell.execute_reply.started":"2025-09-08T02:10:20.728799Z","shell.execute_reply":"2025-09-08T02:10:31.498872Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stdout","text":"INFO [Test]: Starting simplified section extraction test (using direct accession)\nINFO [DatabaseQuery]: Retrieved 1 unprocessed filings with accession numbers\nINFO [FilingProcessor]: Processing 10-Q for grail.com\n   📄 Filing ID: 425\n   📑 Accession: 0001699031-25-000166\nINFO [EdgarTools]: Found 10-Q for GRAIL, Inc.\nINFO [EdgarTools]: SectionExtractor found 6 sections\n      • financial_statements: 46,388 chars\n      • mda: 77,570 chars\n      • market_risk: 2,134 chars\n      • controls_procedures: 1,607 chars\n      • legal_proceedings: 236 chars\n      • risk_factors: 33,402 chars\nINFO [Cache]: Cached sections for 0001699031-25-000166 (6 sections)\nINFO [FilingProcessor]: Extracted 6 sections\n   🎯 Model routing: ['biobert: 5 sections', 'bert_base: 5 sections', 'roberta: 5 sections', 'finbert: 1 sections']\nINFO [Test]: ✅ Successfully extracted 6 sections\n✅ Cell 2 complete - EdgarTools section extraction ready\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 3: EntityExtractionPipeline Class and NER Model Loading\n\nimport torch\nimport time\nimport uuid\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nfrom abc import ABC, abstractmethod\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nprint(\"🚀 Loading EntityExtractionPipeline and NER Models...\")\n\nclass EntityExtractionPipeline:\n    \"\"\"Extensible pipeline for entity extraction from multiple data sources\"\"\"\n    \n    def __init__(self, db_config: Dict, model_config: Dict = None):\n        self.db_config = db_config\n        self.model_config = model_config or self._get_default_model_config()\n        self.loaded_models = {}\n        self.pipeline_stats = {\n            'documents_processed': 0,\n            'total_entities_extracted': 0,\n            'processing_time_total': 0,\n            'data_sources_supported': ['sec_filings', 'press_releases', 'patents']\n        }\n        \n        print(f\"🔧 EntityExtractionPipeline initialized\")\n        print(f\"   📊 Supported data sources: {self.pipeline_stats['data_sources_supported']}\")\n    \n    def _get_default_model_config(self) -> Dict:\n        \"\"\"Default model configuration for biotech entity extraction\"\"\"\n        return {\n            'models': {\n                'biobert': {\n                    'model_name': 'alvaroalon2/biobert_diseases_ner',\n                    'confidence_threshold': 0.5,\n                    'description': 'Biomedical entities (diseases, medications, treatments)'\n                },\n                'bert_base': {\n                    'model_name': 'dslim/bert-base-NER',\n                    'confidence_threshold': 0.5,\n                    'description': 'General entities (persons, organizations, locations)'\n                },\n                'finbert': {\n                    'model_name': 'ProsusAI/finbert',\n                    'confidence_threshold': 0.5,\n                    'description': 'Financial entities and metrics'\n                },\n                'roberta': {\n                    'model_name': 'Jean-Baptiste/roberta-large-ner-english',\n                    'confidence_threshold': 0.6,\n                    'description': 'High-precision general entities'\n                }\n            },\n            'routing_rules': {\n                'sec_filings': {\n                    '10-K': {\n                        'financial_sections': ['finbert'],\n                        'other_sections': ['biobert', 'bert_base', 'roberta']\n                    },\n                    '10-Q': {\n                        'financial_sections': ['finbert'],\n                        'other_sections': ['biobert', 'bert_base', 'roberta']\n                    },\n                    '8-K': {\n                        'all_sections': ['biobert', 'bert_base', 'roberta', 'finbert']\n                    }\n                },\n                'press_releases': {\n                    'all_content': ['biobert', 'bert_base', 'roberta']\n                },\n                'patents': {\n                    'all_content': ['biobert', 'bert_base', 'roberta']\n                }\n            },\n            'entity_type_mapping': {\n                # BioBERT mappings\n                'Disease': 'MEDICAL_CONDITION',\n                'Chemical': 'MEDICATION',\n                'CHEMICAL': 'MEDICATION',\n                'DISEASE': 'MEDICAL_CONDITION',\n                'DRUG': 'MEDICATION',\n                'Drug': 'MEDICATION',\n                'Compound': 'MEDICATION',\n                'Treatment': 'THERAPY',\n                'Therapy': 'THERAPY',\n                \n                # BERT-base mappings\n                'PER': 'PERSON',\n                'ORG': 'ORGANIZATION',\n                'LOC': 'LOCATION',\n                'MISC': 'MISCELLANEOUS',\n                \n                # Financial mappings\n                'MONEY': 'FINANCIAL',\n                'PERCENT': 'FINANCIAL',\n                'NUMBER': 'METRIC'\n            }\n        }\n    \n    def load_models(self) -> Dict:\n        \"\"\"Load all NER models specified in configuration\"\"\"\n        print(f\"📦 Loading {len(self.model_config['models'])} NER models...\")\n        \n        for model_key, model_info in self.model_config['models'].items():\n            try:\n                model_name = model_info['model_name']\n                print(f\"   🧠 Loading {model_key}: {model_name}\")\n                \n                tokenizer = AutoTokenizer.from_pretrained(model_name)\n                model = AutoModelForTokenClassification.from_pretrained(model_name)\n                ner_pipeline = pipeline(\n                    \"ner\", \n                    model=model, \n                    tokenizer=tokenizer,\n                    aggregation_strategy=\"average\", \n                    device=0 if torch.cuda.is_available() else -1\n                )\n                \n                self.loaded_models[model_key] = {\n                    'pipeline': ner_pipeline,\n                    'threshold': model_info['confidence_threshold'],\n                    'description': model_info['description'],\n                    'stats': {\n                        'texts_processed': 0,\n                        'entities_found': 0,\n                        'processing_time': 0\n                    }\n                }\n                \n                print(f\"      ✓ {model_key} loaded (threshold: {model_info['confidence_threshold']})\")\n                \n            except Exception as e:\n                print(f\"      ❌ {model_key} failed: {e}\")\n                # Use BERT-base as fallback for failed models\n                if model_key != 'bert_base' and 'bert_base' in self.loaded_models:\n                    self.loaded_models[model_key] = self.loaded_models['bert_base']\n                    print(f\"      🔄 Using BERT-base fallback for {model_key}\")\n        \n        loaded_count = len(self.loaded_models)\n        device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n        \n        print(f\"   ✅ Successfully loaded: {loaded_count}/4 models on {device}\")\n        \n        return self.loaded_models\n    \n    def get_routing_for_content(self, data_source: str, filing_type: str = None, section_name: str = None) -> List[str]:\n        \"\"\"Get model routing for specific content\"\"\"\n        routing_rules = self.model_config['routing_rules']\n        \n        if data_source not in routing_rules:\n            # Default to all text models for unknown data sources\n            return ['biobert', 'bert_base', 'roberta']\n        \n        source_rules = routing_rules[data_source]\n        \n        if data_source == 'sec_filings' and filing_type:\n            filing_rules = source_rules.get(filing_type, source_rules.get('10-K'))\n            \n            # Check if it's a financial section\n            if section_name and any(fin_word in section_name.lower() \n                                  for fin_word in ['financial', 'statement', 'balance', 'income', 'cash']):\n                return filing_rules.get('financial_sections', ['finbert'])\n            else:\n                return filing_rules.get('other_sections', ['biobert', 'bert_base', 'roberta'])\n        \n        elif data_source in ['press_releases', 'patents']:\n            return source_rules['all_content']\n        \n        # Default routing\n        return ['biobert', 'bert_base', 'roberta']\n    \n    def extract_entities_from_text(self, text: str, model_name: str, metadata: Dict = None) -> List[Dict]:\n        \"\"\"Extract entities from text using specific model\"\"\"\n        if model_name not in self.loaded_models or not text.strip():\n            return []\n        \n        try:\n            start_time = time.time()\n            model_info = self.loaded_models[model_name]\n            \n            # Process in overlapping chunks to handle long text\n            max_length = CONFIG[\"models\"][\"max_length\"]  # 512 tokens\n            overlap = int(max_length * CONFIG[\"models\"][\"chunk_overlap\"])  # Use configured overlap percentage\n            stride = max(1, max_length - overlap)  # 461 token stride\n            \n            # Create overlapping chunks\n            all_raw_entities = []\n            for chunk_start in range(0, len(text), stride):\n                chunk_end = min(chunk_start + max_length, len(text))\n                chunk = text[chunk_start:chunk_end]\n                \n                # Run NER on chunk\n                chunk_entities = model_info[\"pipeline\"](chunk)\n                \n                # Adjust positions to full text coordinates\n                for entity in chunk_entities:\n                    entity[\"start\"] = entity.get(\"start\", 0) + chunk_start\n                    entity[\"end\"] = entity.get(\"end\", 0) + chunk_start\n                    all_raw_entities.append(entity)\n                \n                # Break if we've processed the entire text\n                if chunk_end >= len(text):\n                    break\n            \n            # Deduplicate entities found in overlapping regions\n            raw_entities = []\n            seen_entities = set()\n            for entity in all_raw_entities:\n                # Create unique key for entity (position + text + type)\n                entity_key = (entity.get(\"start\"), entity.get(\"end\"), entity.get(\"word\"), entity.get(\"entity_group\"))\n                if entity_key not in seen_entities:\n                    seen_entities.add(entity_key)\n                    raw_entities.append(entity)\n            \n            # Process and filter results\n            processed_entities = []\n            for entity in raw_entities:\n                if entity['score'] >= model_info['threshold']:\n                    # Normalize entity type\n                    entity_type = self.model_config['entity_type_mapping'].get(\n                        entity['entity_group'], entity['entity_group']\n                    )\n                    \n                    processed_entity = {\n                        'entity_text': entity['word'].strip(),\n                        'entity_type': entity_type,\n                        'confidence_score': float(entity['score']),\n                        'char_start': entity['start'],\n                        'char_end': entity['end'],\n                        'model_source': model_name,\n                        'original_label': entity['entity_group'],\n                        'extraction_id': str(uuid.uuid4()),\n                        'extraction_timestamp': datetime.now().isoformat()\n                    }\n                    \n                    # Add metadata if provided\n                    if metadata:\n                        processed_entity.update(metadata)\n                    \n                    processed_entities.append(processed_entity)\n            \n            # Update statistics\n            processing_time = time.time() - start_time\n            model_info['stats']['texts_processed'] += 1\n            model_info['stats']['entities_found'] += len(processed_entities)\n            model_info['stats']['processing_time'] += processing_time\n            \n            return processed_entities\n            \n        except Exception as e:\n            print(f\"   ❌ {model_name} extraction failed: {e}\")\n            return []\n    \n    def process_sec_filing_sections(self, filing_result: Dict) -> List[Dict]:\n        \"\"\"Process SEC filing sections with appropriate model routing\"\"\"\n        if filing_result.get('processing_status') != 'success':\n            return []\n        \n        filing_id = filing_result['filing_id']\n        filing_type = filing_result['filing_type']\n        company_domain = filing_result['company_domain']\n        sections = filing_result['sections']\n        \n        print(f\"🔍 Processing {len(sections)} sections for {company_domain} {filing_type}\")\n        \n        all_entities = []\n        \n        for section_name, section_text in sections.items():\n            # Get routing for this section\n            applicable_models = self.get_routing_for_content('sec_filings', filing_type, section_name)\n            applicable_models = [m for m in applicable_models if m in self.loaded_models]\n            \n            if not applicable_models:\n                continue\n            \n            print(f\"   📑 Processing '{section_name}' with {len(applicable_models)} models\")\n            \n            # Metadata for this section\n            section_metadata = {\n                'filing_id': filing_id,\n                'company_domain': company_domain,\n                'filing_type': filing_type,\n                'section_name': section_name,\n                'sec_filing_ref': f'SEC_{filing_id}',\n                'data_source': 'sec_filings'\n            }\n            \n            # Process with each applicable model\n            section_entities = []\n            for model_name in applicable_models:\n                model_entities = self.extract_entities_from_text(section_text, model_name, section_metadata)\n                section_entities.extend(model_entities)\n                print(f\"      • {model_name}: {len(model_entities)} entities\")\n            \n            all_entities.extend(section_entities)\n        \n        # Merge entities at same positions\n        merged_entities = self.merge_position_overlaps(all_entities)\n        print(f\"   🔗 Merged: {len(all_entities)} → {len(merged_entities)} entities\")\n        \n        return merged_entities\n    \n    def merge_position_overlaps(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Merge entities detected at same position by different models\"\"\"\n        if not entities:\n            return []\n        \n        # Group entities by position\n        position_groups = {}\n        \n        for entity in entities:\n            # Create position key\n            pos_key = f\"{entity.get('filing_id', 'unknown')}_{entity.get('section_name', 'unknown')}_{entity.get('char_start', 0)}_{entity.get('char_end', 0)}\"\n            \n            if pos_key not in position_groups:\n                position_groups[pos_key] = []\n            position_groups[pos_key].append(entity)\n        \n        merged_entities = []\n        \n        for pos_key, group in position_groups.items():\n            if len(group) == 1:\n                merged_entities.append(group[0])\n            else:\n                # Merge multiple detections\n                merged = self._merge_entity_group(group)\n                merged_entities.append(merged)\n        \n        return merged_entities\n    \n    def _merge_entity_group(self, entities: List[Dict]) -> Dict:\n        \"\"\"Merge entities from different models at same position\"\"\"\n        # Priority for biotech domain: BioBERT > FinBERT > RoBERTa > BERT-base\n        priority = {'biobert': 4, 'finbert': 3, 'roberta': 2, 'bert_base': 1}\n        \n        # Sort by priority, then by confidence\n        entities.sort(key=lambda x: (priority.get(x['model_source'], 0), x['confidence_score']), reverse=True)\n        \n        # Use best entity as base\n        best = entities[0].copy()\n        \n        # Add multi-model metadata\n        best['models_detected'] = [e['model_source'] for e in entities]\n        best['all_confidences'] = {e['model_source']: e['confidence_score'] for e in entities}\n        best['primary_model'] = best['model_source']\n        best['entity_variations'] = {e['model_source']: e['entity_text'] for e in entities}\n        best['is_merged'] = True\n        best['confidence_score'] = max(e['confidence_score'] for e in entities)\n        \n        return best\n    \n    def add_data_source_support(self, source_name: str, routing_config: Dict, processor_func: callable = None):\n        \"\"\"Dynamically add support for new data sources\"\"\"\n        # Add to routing rules\n        self.model_config['routing_rules'][source_name] = routing_config\n        \n        # Add to supported sources\n        if source_name not in self.pipeline_stats['data_sources_supported']:\n            self.pipeline_stats['data_sources_supported'].append(source_name)\n        \n        print(f\"✅ Added support for data source: {source_name}\")\n        print(f\"   📊 Routing config: {routing_config}\")\n    \n    def get_pipeline_statistics(self) -> Dict:\n        \"\"\"Get comprehensive pipeline statistics\"\"\"\n        return {\n            'pipeline_stats': self.pipeline_stats.copy(),\n            'model_stats': {\n                name: info['stats'].copy() \n                for name, info in self.loaded_models.items()\n            },\n            'loaded_models': list(self.loaded_models.keys()),\n            'supported_data_sources': self.pipeline_stats['data_sources_supported'],\n            'device': \"GPU\" if torch.cuda.is_available() else \"CPU\"\n        }\n\n\n# Initialize the EntityExtractionPipeline\nentity_pipeline = EntityExtractionPipeline(NEON_CONFIG)\n\n# Load all NER models\nloaded_models = entity_pipeline.load_models()\n\nprint(f\"\\n✅ EntityExtractionPipeline ready!\")\nprint(f\"   🎯 Loaded models: {list(loaded_models.keys())}\")\nprint(f\"   🔧 Supported sources: {entity_pipeline.pipeline_stats['data_sources_supported']}\")\nprint(f\"   💻 Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:10:54.258334Z","iopub.execute_input":"2025-09-08T02:10:54.258990Z","iopub.status.idle":"2025-09-08T02:11:45.757663Z","shell.execute_reply.started":"2025-09-08T02:10:54.258962Z","shell.execute_reply":"2025-09-08T02:11:45.756612Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"name":"stderr","text":"2025-09-08 02:11:06.632270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757297466.904105      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757297466.979162      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🚀 Loading EntityExtractionPipeline and NER Models...\n🔧 EntityExtractionPipeline initialized\n   📊 Supported data sources: ['sec_filings', 'press_releases', 'patents']\n📦 Loading 4 NER models...\n   🧠 Loading biobert: alvaroalon2/biobert_diseases_ner\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c82b3e9d3ad4bc696025242b9f3c2e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b0bd1bd0c2d47cbbeaf966c6ffd3125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82371504b8f5438cba8685fa2ed28414"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4baabc947e24ec39d1491d4f980c1ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/431M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e671231e97a417b90ad4aede2c7455c"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f1175f81a146ada1870d5bc4ed8304"}},"metadata":{}},{"name":"stdout","text":"      ✓ biobert loaded (threshold: 0.5)\n   🧠 Loading bert_base: dslim/bert-base-NER\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"799a29714e5d41b48285771f3e76431d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"010ecab8739141b9bdaee7d53a8796f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39f728928cc42a790d7ee45f2bfa05f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb79faad89b44b208f06fe2d39b5209c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bcf6a69b8b7415b96c99a45be80a259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6928d1f7c0204d1ba00f3ecc62e20e38"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"      ✓ bert_base loaded (threshold: 0.5)\n   🧠 Loading finbert: ProsusAI/finbert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"479156a0e4174f428dd2da69b86b9813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a1bd2b3916648b9aafa74be7ee4fc5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc22d4d739944bbf9b9a711377824e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"772808de402f4207964c45976ad3d4b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f67282e7784d4d60bad93149f4dbea38"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"072654814b904475ba82e0e2c2604807"}},"metadata":{}},{"name":"stdout","text":"      ✓ finbert loaded (threshold: 0.5)\n   🧠 Loading roberta: Jean-Baptiste/roberta-large-ner-english\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165203760da645dc8e7b0df665e41e53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/849 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6d9ff9a6f04414a6ce1bb1b4aec90d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6547ee35f03e4019b547aa67d1e51e30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c0c267b98f44c6b7c05d9320731123"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35da9c64b0ec4e638c11f3cea3527d85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"053c435de3a5400b8f72ba83cfadf8c8"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"      ✓ roberta loaded (threshold: 0.6)\n   ✅ Successfully loaded: 4/4 models on GPU\n\n✅ EntityExtractionPipeline ready!\n   🎯 Loaded models: ['biobert', 'bert_base', 'finbert', 'roberta']\n   🔧 Supported sources: ['sec_filings', 'press_releases', 'patents']\n   💻 Device: GPU\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Cell 4: In-Memory Pipeline with Entity Storage and Relationship Extraction\n\nimport psycopg2\nfrom psycopg2.extras import execute_values\nimport numpy as np\nfrom groq import Groq\nimport re\nfrom typing import List, Dict, Tuple, Optional\nimport json\nimport uuid\nimport time\nimport os\nfrom datetime import datetime\n\nprint(\"🚀 Loading In-Memory Pipeline with Storage and Relationship Extraction...\")\n\n# ================================================================================\n# RELATIONSHIP EXTRACTOR WITH LLAMA 3.1\n# ================================================================================\n\nclass RelationshipExtractor:\n    \"\"\"Extract company-centric relationships using Llama 3.1\"\"\"\n    \n    def __init__(self, llama_config: Dict = None):\n        \"\"\"Initialize with Llama configuration\"\"\"\n        self.config = llama_config or CONFIG.get('llama', {})\n        self.client = None\n        self.stats = {\n            'entities_processed': 0,\n            'relationships_found': 0,\n            'llama_calls': 0,\n            'processing_time': 0\n        }\n        \n        # Initialize Groq client if API key available\n        api_key = self.config.get('api_key') or os.getenv('GROQ_API_KEY') or user_secrets.get_secret(\"GROQ_API_KEY\")\n        if api_key:\n            self.client = Groq(api_key=api_key)\n            print(f\"   ✅ Llama 3.1 client initialized via Groq\")\n        else:\n            print(f\"   ⚠️ No Groq API key found - relationship extraction disabled\")\n    \n\n    def _filter_entities(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Filter out non-business entities like SEC section headers and document structure\"\"\"\n        filtered = []\n        \n        # Patterns to exclude (SEC filing structure elements)\n        exclude_patterns = [\n            r'^Item \\d+[A-Za-z]?\\.?\\s*',  # Item 1, Item 2A, etc.\n            r'^Part [IVX]+\\.?\\s*',  # Part I, Part II, etc.\n            r'^Section \\d+\\.?\\s*',  # Section 1, Section 2, etc.\n            r'^Exhibit \\d+\\.?\\s*',  # Exhibit 1, etc.\n            r'^Schedule [A-Z]\\.?\\s*',  # Schedule A, etc.\n            r'^Note \\d+\\.?\\s*',  # Note 1, Note 2, etc.\n            r'^Table of Contents',\n            r'^Index to',\n            r'^Forward.?Looking Statements',\n            r'^Risk Factors$',\n            r'^Legal Proceedings$',\n            r'^Management.s Discussion',\n            r'^Quantitative and Qualitative Disclosures',\n            r'^Controls and Procedures',\n            r'^Financial Statements',\n            r'^Signatures$',\n            r'^SIGNATURES$'\n        ]\n        \n        for entity in entities:\n            entity_text = entity.get('entity_text', '').strip()\n            entity_type = entity.get('entity_type', '')\n            \n            # Skip empty or very short entities\n            if not entity_text or len(entity_text) < 3:\n                continue\n            \n            # Skip entities that are just numbers\n            if entity_text.replace('.', '').replace(',', '').isdigit():\n                continue\n            \n            # Skip if entity matches SEC structure patterns\n            is_structure = False\n            for pattern in exclude_patterns:\n                if re.match(pattern, entity_text, re.IGNORECASE):\n                    is_structure = True\n                    break\n            \n            if is_structure:\n                continue\n            \n            # Skip entities with category '0' that look like SEC headers\n            if entity.get('entity_category') == '0' or entity_type == '0':\n                # Check if it starts with common SEC filing patterns\n                if any(entity_text.startswith(prefix) for prefix in ['Item ', 'Part ', 'Section ', 'Note ']):\n                    continue\n            \n            # Additional filtering for low-quality entities\n            if entity_type in ['MISCELLANEOUS', '0']:\n                # For miscellaneous entities, apply stricter filtering\n                # Keep only if confidence is high and text looks like a real entity\n                confidence = entity.get('confidence_score', 0)\n                if confidence < 0.8:\n                    continue\n                \n                # Check if it looks like a real company/organization name\n                # (contains capitalized words, not all caps, not all lowercase)\n                words = entity_text.split()\n                if len(words) > 0:\n                    has_capital = any(word[0].isupper() for word in words if word)\n                    all_caps = entity_text.isupper()\n                    all_lower = entity_text.islower()\n                    \n                    if not has_capital or all_caps or all_lower:\n                        continue\n            \n            # Entity passed all filters\n            filtered.append(entity)\n        \n        print(f\"      🎯 Filtered: {len(entities)} → {len(filtered)} entities (removed {len(entities) - len(filtered)} non-business entities)\")\n        return filtered\n\n    def extract_company_relationships(self, \n                                     entities: List[Dict], \n                                     sections: Dict[str, str],\n                                     company_domain: str) -> List[Dict]:\n        \"\"\"Extract relationships between company and all found entities\"\"\"\n        if not self.client or not entities:\n            return []\n        # Filter out non-business entities (SEC headers, etc.)\n        entities = self._filter_entities(entities)\n        if not entities:\n            print(\"      ⚠️ No valid business entities after filtering\")\n            return []\n\n        \n        print(f\"   🔍 Analyzing relationships for {company_domain}\")\n        relationships = []\n        \n        # Group entities by section for context efficiency\n        entities_by_section = {}\n        for entity in entities:\n            section = entity.get('section_name', 'unknown')\n            if section not in entities_by_section:\n                entities_by_section[section] = []\n            entities_by_section[section].append(entity)\n        \n        # Process each section's entities\n        for section_name, section_entities in entities_by_section.items():\n            if section_name not in sections:\n                continue\n                \n            section_text = sections[section_name]\n            print(f\"      📑 Processing {len(section_entities)} entities in '{section_name}'\")\n            \n            # Analyze each entity's relationship to the company\n            for entity in section_entities:\n                # Skip self-references\n                company_name = company_domain.replace('.com', '').replace('tx', '')\n                if entity['entity_text'].lower() == company_name.lower():\n                    continue\n                \n                # Get context around entity\n                context = self._get_entity_context(entity, section_text)\n                \n                # Build and send prompt to Llama\n                relationship = self._analyze_relationship(\n                    company_domain, entity, context, section_name\n                )\n                \n                if relationship:\n                    relationships.append(relationship)\n                    self.stats['relationships_found'] += 1\n                \n                self.stats['entities_processed'] += 1\n        \n        print(f\"   ✅ Found {len(relationships)} relationships from {len(entities)} entities\")\n        return relationships\n    \n    def _get_entity_context(self, entity: Dict, section_text: str, window: int = 500) -> str:\n        \"\"\"Get context around an entity\"\"\"\n        start = max(0, entity['char_start'] - window)\n        end = min(len(section_text), entity['char_end'] + window)\n        return section_text[start:end]\n    \n    def _analyze_relationship(self, \n                            company_domain: str,\n                            entity: Dict,\n                            context: str,\n                            section_name: str) -> Optional[Dict]:\n        \"\"\"Analyze a single entity's relationship to the company using Llama\"\"\"\n        if not self.client:\n            return None\n        \n        try:\n            # Build prompt\n            prompt = f\"\"\"Analyze the business relationship between the filing company and the mentioned entity.\n\nFiling Company: {company_domain}\nEntity Found: {entity['entity_text']} (Type: {entity['entity_type']})\nSection: {section_name}\n\nContext:\n{context}\n\nBased on the context, determine:\n1. Relationship type (choose ONE from: PARTNERSHIP, COMPETITOR, REGULATORY, CLINICAL_TRIAL, SUPPLIER, CUSTOMER, INVESTOR, ACQUISITION, LICENSING, RESEARCH, NONE)\n2. Relationship direction (company_to_entity or entity_to_company)\n3. Business impact (positive, negative, or neutral)\n4. Confidence level (high, medium, or low)\n5. Brief summary (one sentence)\n\nFormat your response as:\nTYPE: <relationship_type>\nDIRECTION: <direction>\nIMPACT: <impact>\nCONFIDENCE: <confidence>\nSUMMARY: <one_sentence_summary>\n\"\"\"\n            \n            # Call Llama 3.1\n            response = self.client.chat.completions.create(\n                model=\"llama-3.1-70b-versatile\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=0.3,\n                max_tokens=200\n            )\n            \n            self.stats['llama_calls'] += 1\n            llama_response = response.choices[0].message.content\n            \n            # Parse response\n            parsed = self._parse_llama_response(llama_response)\n            \n            if parsed and parsed['type'] != 'NONE':\n                return {\n                    'company_domain': company_domain,\n                    'entity_text': entity['entity_text'],\n                    'entity_type': entity['entity_type'],\n                    'entity_id': entity['extraction_id'],\n                    'relationship_type': parsed['type'],\n                    'relationship_direction': parsed['direction'],\n                    'business_impact': parsed['impact'],\n                    'confidence_level': parsed['confidence'],\n                    'summary': parsed['summary'],\n                    'section_name': section_name,\n                    'context_used': context[:1000],  # Store first 1000 chars\n                    'llama_response': llama_response,\n                    'extraction_timestamp': datetime.now().isoformat()\n                }\n            \n            return None\n            \n        except Exception as e:\n            print(f\"         ⚠️ Llama analysis failed for {entity['entity_text']}: {e}\")\n            return None\n    \n    def _parse_llama_response(self, response: str) -> Optional[Dict]:\n        \"\"\"Parse structured response from Llama\"\"\"\n        try:\n            lines = response.strip().split('\\n')\n            parsed = {}\n            \n            for line in lines:\n                if ':' in line:\n                    key, value = line.split(':', 1)\n                    key = key.strip().upper()\n                    value = value.strip()\n                    \n                    if key == 'TYPE':\n                        parsed['type'] = value\n                    elif key == 'DIRECTION':\n                        parsed['direction'] = value\n                    elif key == 'IMPACT':\n                        parsed['impact'] = value.lower()\n                    elif key == 'CONFIDENCE':\n                        parsed['confidence'] = value.lower()\n                    elif key == 'SUMMARY':\n                        parsed['summary'] = value\n            \n            # Validate required fields\n            required = ['type', 'direction', 'impact', 'confidence', 'summary']\n            if all(field in parsed for field in required):\n                return parsed\n            \n            return None\n            \n        except Exception:\n            return None\n\n# ================================================================================\n# ENHANCED STORAGE WITH ATOMIC TRANSACTIONS\n# ================================================================================\n\nclass PipelineEntityStorage:\n    \"\"\"Enhanced storage system with atomic entity+relationship storage\"\"\"\n    \n    def __init__(self, db_config: Dict):\n        self.db_config = db_config\n        self.storage_stats = {\n            'total_entities_stored': 0,\n            'total_relationships_stored': 0,\n            'filings_processed': 0,\n            'transactions_completed': 0,\n            'transactions_failed': 0,\n            'merged_entities': 0,\n            'single_model_entities': 0,\n            'failed_inserts': 0\n        }\n        \n        # Ensure table structures\n        self._ensure_table_structures()\n    \n    def _ensure_table_structures(self):\n        \"\"\"Ensure both entity and relationship tables exist with proper schema\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Ensure entity table columns (same as before)\n            self._ensure_entity_table(cursor)\n            \n            # Create relationship table if not exists\n            cursor.execute(\"\"\"\n                CREATE TABLE IF NOT EXISTS system_uno.entity_relationships (\n                    relationship_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n                    company_domain TEXT NOT NULL,\n                    entity_id UUID REFERENCES system_uno.sec_entities_raw(extraction_id),\n                    entity_text TEXT NOT NULL,\n                    entity_type TEXT,\n                    relationship_type TEXT NOT NULL,\n                    relationship_direction TEXT,\n                    business_impact TEXT,\n                    confidence_level TEXT,\n                    summary TEXT,\n                    section_name TEXT,\n                    context_used TEXT,\n                    llama_response TEXT,\n                    filing_ref TEXT,\n                    extraction_timestamp TIMESTAMP,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            \"\"\")\n            \n            # Create indexes for relationships\n            cursor.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_relationships_company \n                ON system_uno.entity_relationships(company_domain)\n            \"\"\")\n            cursor.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_relationships_type \n                ON system_uno.entity_relationships(relationship_type)\n            \"\"\")\n            cursor.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_relationships_impact \n                ON system_uno.entity_relationships(business_impact)\n            \"\"\")\n            \n            conn.commit()\n            cursor.close()\n            conn.close()\n            print(\"   ✅ Database tables verified/created\")\n            \n        except Exception as e:\n            print(f\"   ⚠️ Table structure setup failed: {e}\")\n    \n    def _ensure_entity_table(self, cursor):\n        \"\"\"Ensure entity table has all required columns\"\"\"\n        # Check existing columns\n        cursor.execute(\"\"\"\n            SELECT column_name \n            FROM information_schema.columns \n            WHERE table_schema = 'system_uno' \n            AND table_name = 'sec_entities_raw'\n        \"\"\")\n        \n        existing_columns = {row[0] for row in cursor.fetchall()}\n        \n        # Required columns for pipeline\n        required_columns = {\n            'models_detected': 'TEXT[]',\n            'all_confidences': 'JSONB',\n            'primary_model': 'TEXT',\n            'entity_variations': 'JSONB',\n            'is_merged': 'BOOLEAN DEFAULT FALSE',\n            'section_name': 'TEXT',\n            'data_source': 'TEXT DEFAULT \\'sec_filings\\'',\n            'extraction_timestamp': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',\n            'original_label': 'TEXT'\n        }\n        \n        # Add missing columns\n        for col_name, col_type in required_columns.items():\n            if col_name not in existing_columns:\n                cursor.execute(f'ALTER TABLE system_uno.sec_entities_raw ADD COLUMN {col_name} {col_type}')\n                print(f\"      ✓ Added column: {col_name}\")\n    \n    def store_entities_and_relationships(self, \n                                        entities: List[Dict], \n                                        relationships: List[Dict],\n                                        filing_ref: str) -> bool:\n        \"\"\"Store entities and relationships in a single atomic transaction\"\"\"\n        if not entities:\n            return True\n        \n        conn = None\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Start transaction\n            conn.autocommit = False\n            \n            print(f\"   💾 Storing {len(entities)} entities and {len(relationships)} relationships...\")\n            \n            # Count merged vs single-model entities BEFORE storage\n            merged_count = sum(1 for e in entities if e.get('is_merged', False))\n            single_model_count = len(entities) - merged_count\n            \n            # Step 1: Store entities\n            entity_data = []\n            for entity in entities:\n                # Prepare entity data (same as original)\n                models_detected = entity.get('models_detected', [entity.get('model_source', 'unknown')])\n                if not isinstance(models_detected, list):\n                    models_detected = [str(models_detected)]\n                \n                all_confidences = entity.get('all_confidences', \n                    {entity.get('model_source', 'unknown'): entity.get('confidence_score', 0.0)})\n                entity_variations = entity.get('entity_variations', \n                    {entity.get('model_source', 'unknown'): entity.get('entity_text', '')})\n                \n                entity_tuple = (\n                    entity.get('extraction_id', str(uuid.uuid4())),\n                    entity.get('company_domain', ''),\n                    entity.get('entity_text', '').strip()[:1000],\n                    entity.get('entity_type', 'UNKNOWN'),\n                    float(entity.get('confidence_score', 0.0)),\n                    int(entity.get('char_start', 0)),\n                    int(entity.get('char_end', 0)),\n                    entity.get('surrounding_text', '')[:2000] if entity.get('surrounding_text') else '',\n                    entity.get('sec_filing_ref', filing_ref),\n                    models_detected,\n                    json.dumps(all_confidences),\n                    entity.get('primary_model', entity.get('model_source', 'unknown')),\n                    json.dumps(entity_variations),\n                    entity.get('is_merged', False),\n                    entity.get('section_name', ''),\n                    entity.get('data_source', 'sec_filings'),\n                    entity.get('extraction_timestamp'),\n                    entity.get('original_label', '')\n                )\n                entity_data.append(entity_tuple)\n            \n            # Batch insert entities\n            entity_query = \"\"\"\n                INSERT INTO system_uno.sec_entities_raw \n                (extraction_id, company_domain, entity_text, entity_category, \n                 confidence_score, character_start, character_end, surrounding_text, \n                 sec_filing_ref, models_detected, all_confidences, primary_model,\n                 entity_variations, is_merged, section_name, data_source,\n                 extraction_timestamp, original_label)\n                VALUES %s\n                ON CONFLICT (extraction_id) DO UPDATE SET\n                    models_detected = EXCLUDED.models_detected,\n                    all_confidences = EXCLUDED.all_confidences,\n                    is_merged = EXCLUDED.is_merged\n            \"\"\"\n            \n            execute_values(cursor, entity_query, entity_data, page_size=100)\n            entities_stored = cursor.rowcount\n            \n            # Step 2: Store relationships\n            if relationships:\n                relationship_data = []\n                for rel in relationships:\n                    rel_tuple = (\n                        rel.get('company_domain', ''),\n                        rel.get('entity_id'),\n                        rel.get('entity_text', ''),\n                        rel.get('entity_type', ''),\n                        rel.get('relationship_type', ''),\n                        rel.get('relationship_direction', ''),\n                        rel.get('business_impact', ''),\n                        rel.get('confidence_level', ''),\n                        rel.get('summary', ''),\n                        rel.get('section_name', ''),\n                        rel.get('context_used', ''),\n                        rel.get('llama_response', ''),\n                        filing_ref,\n                        rel.get('extraction_timestamp')\n                    )\n                    relationship_data.append(rel_tuple)\n                \n                relationship_query = \"\"\"\n                    INSERT INTO system_uno.entity_relationships\n                    (company_domain, entity_id, entity_text, entity_type,\n                     relationship_type, relationship_direction, business_impact,\n                     confidence_level, summary, section_name, context_used,\n                     llama_response, filing_ref, extraction_timestamp)\n                    VALUES %s\n                \"\"\"\n                \n                execute_values(cursor, relationship_query, relationship_data, page_size=100)\n                relationships_stored = cursor.rowcount\n            else:\n                relationships_stored = 0\n            \n            # Commit transaction\n            conn.commit()\n            \n            # Update statistics\n            self.storage_stats['total_entities_stored'] += entities_stored\n            self.storage_stats['total_relationships_stored'] += relationships_stored\n            self.storage_stats['transactions_completed'] += 1\n            self.storage_stats['merged_entities'] += merged_count\n            self.storage_stats['single_model_entities'] += single_model_count\n            \n            print(f\"      ✅ Transaction complete: {entities_stored} entities, {relationships_stored} relationships\")\n            print(f\"         • Merged entities: {merged_count}, Single-model: {single_model_count}\")\n            \n            cursor.close()\n            conn.close()\n            return True\n            \n        except Exception as e:\n            print(f\"      ❌ Transaction failed: {e}\")\n            if conn:\n                conn.rollback()\n                conn.close()\n            self.storage_stats['transactions_failed'] += 1\n            self.storage_stats['failed_inserts'] += len(entities)\n            return False\n    \n    def get_storage_verification(self, filing_ref: str) -> Dict:\n        \"\"\"Verify stored entities and relationships for a filing\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Get entity statistics\n            cursor.execute(\"\"\"\n                SELECT \n                    COUNT(*) as total_entities,\n                    COUNT(DISTINCT entity_category) as unique_categories,\n                    COUNT(DISTINCT section_name) as sections_processed,\n                    AVG(confidence_score) as avg_confidence\n                FROM system_uno.sec_entities_raw\n                WHERE sec_filing_ref = %s\n            \"\"\", (filing_ref,))\n            \n            entity_stats = cursor.fetchone()\n            \n            # Get relationship statistics\n            cursor.execute(\"\"\"\n                SELECT \n                    COUNT(*) as total_relationships,\n                    COUNT(DISTINCT relationship_type) as relationship_types,\n                    COUNT(CASE WHEN business_impact = 'positive' THEN 1 END) as positive_impact,\n                    COUNT(CASE WHEN business_impact = 'negative' THEN 1 END) as negative_impact,\n                    COUNT(CASE WHEN business_impact = 'neutral' THEN 1 END) as neutral_impact\n                FROM system_uno.entity_relationships\n                WHERE filing_ref = %s\n            \"\"\", (filing_ref,))\n            \n            rel_stats = cursor.fetchone()\n            \n            cursor.close()\n            conn.close()\n            \n            return {\n                'filing_ref': filing_ref,\n                'entities': {\n                    'total': entity_stats[0] if entity_stats else 0,\n                    'categories': entity_stats[1] if entity_stats else 0,\n                    'sections': entity_stats[2] if entity_stats else 0,\n                    'avg_confidence': float(entity_stats[3]) if entity_stats and entity_stats[3] else 0\n                },\n                'relationships': {\n                    'total': rel_stats[0] if rel_stats else 0,\n                    'types': rel_stats[1] if rel_stats else 0,\n                    'positive': rel_stats[2] if rel_stats else 0,\n                    'negative': rel_stats[3] if rel_stats else 0,\n                    'neutral': rel_stats[4] if rel_stats else 0\n                }\n            }\n            \n        except Exception as e:\n            print(f\"   ❌ Verification failed: {e}\")\n            return {}\n\n# ================================================================================\n# REFACTORED MAIN PIPELINE WITH IN-MEMORY PROCESSING\n# ================================================================================\n\n# Initialize components\npipeline_storage = PipelineEntityStorage(NEON_CONFIG)\nrelationship_extractor = RelationshipExtractor()\n\ndef process_filing_with_pipeline(filing_data: Dict) -> Dict:\n    \"\"\"Process filing with in-memory entity and relationship extraction\"\"\"\n    try:\n        start_time = time.time()\n        \n        # Step 1: Extract sections (Cell 2 function)\n        print(f\"\\n📄 Processing {filing_data['filing_type']} for {filing_data['company_domain']}\")\n        section_result = process_sec_filing_with_sections(filing_data)\n        \n        if section_result['processing_status'] != 'success':\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': section_result.get('error', 'Section extraction failed'),\n                'processing_time': time.time() - start_time\n            }\n        \n        # Keep sections in memory for context retrieval\n        sections_dict = section_result['sections']\n        \n        # Step 2: Extract entities (Cell 3 function) - keep in memory\n        entities = entity_pipeline.process_sec_filing_sections(section_result)\n        \n        if not entities:\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': 'No entities extracted',\n                'processing_time': time.time() - start_time\n            }\n        \n        print(f\"   🔍 Extracted {len(entities)} entities\")\n        \n        # Step 3: Extract relationships using in-memory entities and sections\n        relationships = relationship_extractor.extract_company_relationships(\n            entities, \n            sections_dict,\n            filing_data['company_domain']\n        )\n        \n        # Step 4: Store everything in single transaction\n        filing_ref = f\"SEC_{filing_data.get('id')}\"\n        storage_success = pipeline_storage.store_entities_and_relationships(\n            entities, \n            relationships,\n            filing_ref\n        )\n        \n        # Step 5: Verify storage\n        verification = pipeline_storage.get_storage_verification(filing_ref)\n        \n        processing_time = time.time() - start_time\n        \n        return {\n            'success': storage_success,\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain'),\n            'filing_type': filing_data.get('filing_type'),\n            'sections_processed': len(sections_dict),\n            'entities_extracted': len(entities),\n            'relationships_found': len(relationships),\n            'entities_stored': verification.get('entities', {}).get('total', 0),\n            'relationships_stored': verification.get('relationships', {}).get('total', 0),\n            'processing_time': round(processing_time, 2),\n            'verification': verification,\n            'sample_entities': entities[:3],\n            'sample_relationships': relationships[:3]\n        }\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'filing_id': filing_data.get('id'),\n            'error': str(e),\n            'processing_time': time.time() - start_time\n        }\n\ndef process_filings_batch(limit: int = 3) -> Dict:\n    \"\"\"Process multiple filings with complete in-memory pipeline\"\"\"\n    print(f\"\\n🚀 Processing batch of {limit} SEC filings with in-memory pipeline...\")\n    \n    batch_start = time.time()\n    \n    # Get unprocessed filings\n    filings = get_unprocessed_filings(limit)\n    \n    if not filings:\n        return {'success': False, 'message': 'No filings to process'}\n    \n    print(f\"📊 Found {len(filings)} filings to process\")\n    \n    # Process each filing\n    results = []\n    successful = 0\n    total_entities = 0\n    total_relationships = 0\n    \n    for i, filing in enumerate(filings, 1):\n        print(f\"\\n[{i}/{len(filings)}] Processing {filing['filing_type']} for {filing['company_domain']}\")\n        \n        result = process_filing_with_pipeline(filing)\n        results.append(result)\n        \n        if result['success']:\n            successful += 1\n            total_entities += result.get('entities_extracted', 0)\n            total_relationships += result.get('relationships_found', 0)\n            \n            print(f\"   ✅ Success: {result['entities_extracted']} entities, {result['relationships_found']} relationships\")\n            print(f\"   ⏱️ Processing time: {result['processing_time']}s\")\n            \n            # Show sample relationships\n            for rel in result.get('sample_relationships', [])[:2]:\n                print(f\"      • {rel['entity_text']} → {rel['relationship_type']} ({rel['business_impact']})\")\n        else:\n            print(f\"   ❌ Failed: {result.get('error', 'Unknown error')}\")\n        \n        # Brief pause between filings\n        if i < len(filings):\n            time.sleep(1)\n    \n    batch_time = time.time() - batch_start\n    \n    # Update pipeline statistics\n    entity_pipeline.pipeline_stats['documents_processed'] += successful\n    entity_pipeline.pipeline_stats['total_entities_extracted'] += total_entities\n    pipeline_storage.storage_stats['filings_processed'] += successful\n    \n    return {\n        'success': successful > 0,\n        'filings_processed': len(filings),\n        'successful_filings': successful,\n        'failed_filings': len(filings) - successful,\n        'total_entities_extracted': total_entities,\n        'total_relationships_found': total_relationships,\n        'batch_processing_time': round(batch_time, 2),\n        'avg_time_per_filing': round(batch_time / len(filings), 2) if filings else 0,\n        'results': results\n    }\n\n# ================================================================================\n# QUICK ACCESS FUNCTIONS\n# ================================================================================\n\ndef test_pipeline(company_domain: str = None):\n    \"\"\"Test the pipeline with a single filing\"\"\"\n    print(\"\\n🧪 Testing in-memory pipeline...\")\n    \n    # Get one filing\n    if company_domain:\n        # Modify get_unprocessed_filings to accept company filter\n        # For now, just get any filing\n        filings = get_unprocessed_filings(limit=1)\n    else:\n        filings = get_unprocessed_filings(limit=1)\n    \n    if not filings:\n        print(\"❌ No test filings available\")\n        return None\n    \n    result = process_filing_with_pipeline(filings[0])\n    \n    if result['success']:\n        print(f\"\\n✅ Pipeline test successful!\")\n        print(f\"   📊 Sections: {result['sections_processed']}\")\n        print(f\"   🔍 Entities: {result['entities_extracted']}\")\n        print(f\"   🔗 Relationships: {result['relationships_found']}\")\n        print(f\"   💾 Stored: {result['entities_stored']} entities, {result['relationships_stored']} relationships\")\n        print(f\"   ⏱️ Time: {result['processing_time']}s\")\n    else:\n        print(f\"\\n❌ Pipeline test failed: {result.get('error')}\")\n    \n    return result\n\nprint(\"\\n✅ In-Memory Pipeline Components Ready!\")\nprint(\"   🎯 RelationshipExtractor with Llama 3.1\")\nprint(\"   💾 Atomic storage for entities + relationships\")\nprint(\"   🚀 In-memory processing (no DB round-trips)\")\nprint(\"   📊 Usage: batch_results = process_filings_batch(limit=5)\")\nprint(\"   🧪 Test: test_result = test_pipeline()\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T02:11:58.680063Z","iopub.execute_input":"2025-09-08T02:11:58.681013Z","iopub.status.idle":"2025-09-08T02:11:59.243907Z","shell.execute_reply.started":"2025-09-08T02:11:58.680986Z","shell.execute_reply":"2025-09-08T02:11:59.243086Z"}},"outputs":[{"name":"stdout","text":"🚀 Loading In-Memory Pipeline with Storage and Relationship Extraction...\n   ✅ Database tables verified/created\n   ✅ Llama 3.1 client initialized via Groq\n\n✅ In-Memory Pipeline Components Ready!\n   🎯 RelationshipExtractor with Llama 3.1\n   💾 Atomic storage for entities + relationships\n   🚀 In-memory processing (no DB round-trips)\n   📊 Usage: batch_results = process_filings_batch(limit=5)\n   🧪 Test: test_result = test_pipeline()\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5: Execute Pipeline - Process SEC Filings\n\nprint(\"=\"*80)\nprint(\"🚀 STARTING SEC FILING PROCESSING PIPELINE\")\nprint(\"=\"*80)\n\n# Configure processing parameters\nBATCH_SIZE = 3  # Number of filings to process in this run\nPROCESS_RELATIONSHIPS = True  # Set to False to skip relationship extraction\n\n# Check if we have a Groq API key for relationship extraction\nhas_groq = bool(user_secrets.get_secret(\"GROQ_API_KEY\") or os.getenv('GROQ_API_KEY'))\nif not has_groq and PROCESS_RELATIONSHIPS:\n    print(\"⚠️ No Groq API key found - relationship extraction will be skipped\")\n    print(\"   To enable relationships, add GROQ_API_KEY to Kaggle secrets\")\n\n# Check for available unprocessed filings\nprint(\"\\n📊 Checking for unprocessed filings...\")\navailable_filings = get_unprocessed_filings(limit=10)\nprint(f\"   Found {len(available_filings)} unprocessed filings\")\n\nif available_filings:\n    print(\"\\n📋 Available filings to process:\")\n    for i, filing in enumerate(available_filings[:5], 1):\n        print(f\"   {i}. {filing['company_domain']} - {filing['filing_type']} ({filing['filing_date']})\")\n    \n    # Process the batch\n    print(f\"\\n🔄 Processing {min(BATCH_SIZE, len(available_filings))} filings...\")\n    print(\"-\"*60)\n    \n    # Run the pipeline\n    batch_results = process_filings_batch(limit=BATCH_SIZE)\n    \n    # Display results summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"📊 PROCESSING SUMMARY\")\n    print(\"=\"*80)\n    \n    if batch_results['success']:\n        print(f\"✅ Successfully processed {batch_results['successful_filings']}/{batch_results['filings_processed']} filings\")\n        print(f\"   • Total entities extracted: {batch_results['total_entities_extracted']:,}\")\n        print(f\"   • Total relationships found: {batch_results['total_relationships_found']:,}\")\n        print(f\"   • Total processing time: {batch_results['batch_processing_time']:.1f}s\")\n        print(f\"   • Average time per filing: {batch_results['avg_time_per_filing']:.1f}s\")\n        \n        # Show detailed results for each filing\n        print(f\"\\n📈 Detailed Results:\")\n        for i, result in enumerate(batch_results['results'], 1):\n            if result['success']:\n                print(f\"\\n   Filing {i}: {result['company_domain']} - {result['filing_type']}\")\n                print(f\"      ✓ Sections: {result['sections_processed']}\")\n                print(f\"      ✓ Entities: {result['entities_extracted']}\")\n                print(f\"      ✓ Relationships: {result['relationships_found']}\")\n                print(f\"      ✓ Time: {result['processing_time']:.1f}s\")\n            else:\n                print(f\"\\n   Filing {i}: FAILED - {result.get('error', 'Unknown error')}\")\n        \n        # Show pipeline statistics\n        print(f\"\\n📊 Pipeline Statistics:\")\n        print(f\"   • Documents processed (total): {entity_pipeline.pipeline_stats['documents_processed']}\")\n        print(f\"   • Entities extracted (total): {entity_pipeline.pipeline_stats['total_entities_extracted']}\")\n        print(f\"   • Storage transactions: {pipeline_storage.storage_stats['transactions_completed']} successful, {pipeline_storage.storage_stats['transactions_failed']} failed\")\n        print(f\"   • Merged entities: {pipeline_storage.storage_stats['merged_entities']}\")\n        print(f\"   • Single-model entities: {pipeline_storage.storage_stats['single_model_entities']}\")\n        \n    else:\n        print(f\"❌ Processing failed: {batch_results.get('message', 'Unknown error')}\")\n    \n    # Generate analytics report\n    print(\"\\n\" + \"=\"*80)\n    generate_pipeline_analytics_report()\n    \nelse:\n    print(\"\\n⚠️ No unprocessed filings found in raw_data.sec_filings\")\n    print(\"   All available filings have already been processed\")\n    print(\"\\n💡 To add new filings:\")\n    print(\"   1. Insert new records into raw_data.sec_filings with accession_number\")\n    print(\"   2. Make sure the accession_number is valid (20 characters)\")\n    print(\"   3. Run this cell again to process them\")\n\nprint(\"\\n✅ Pipeline execution complete!\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: Production Analytics and Monitoring Interface\n\ndef generate_pipeline_analytics_report() -> None:\n    \"\"\"Generate comprehensive analytics report for the pipeline\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"📊 ENTITYEXTRACTIONPIPELINE ANALYTICS DASHBOARD\")\n    print(\"=\"*80)\n    \n    # Database overview with enhanced metrics\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Enhanced database statistics\n        cursor.execute(\"\"\"\n            SELECT \n                COUNT(*) as total_entities,\n                COUNT(DISTINCT company_domain) as companies,\n                COUNT(DISTINCT sec_filing_ref) as filings,\n                COUNT(DISTINCT entity_category) as entity_types,\n                AVG(confidence_score) as avg_confidence,\n                COUNT(*) FILTER (WHERE is_merged = true) as merged_entities,\n                COUNT(DISTINCT primary_model) as active_models,\n                COUNT(DISTINCT section_name) as sections_processed,\n                COUNT(*) FILTER (WHERE section_name IS NOT NULL AND section_name != '') as entities_with_sections,\n                MAX(extraction_timestamp) as last_extraction\n            FROM system_uno.sec_entities_raw\n            WHERE data_source = 'sec_filings'\n        \"\"\")\n        \n        db_overview = cursor.fetchone()\n        \n        if db_overview and db_overview[0] > 0:\n            total_entities = db_overview[0]\n            entities_with_sections = db_overview[8]\n            section_success_rate = (entities_with_sections / total_entities * 100) if total_entities > 0 else 0\n            \n            print(f\"\\n📈 DATABASE OVERVIEW:\")\n            print(f\"   Total Entities Extracted: {db_overview[0]:,}\")\n            print(f\"   Companies Processed: {db_overview[1]:,}\")\n            print(f\"   SEC Filings Analyzed: {db_overview[2]:,}\")\n            print(f\"   Entity Categories Found: {db_overview[3]:,}\")\n            print(f\"   Average Confidence Score: {db_overview[4]:.3f}\")\n            print(f\"   Multi-Model Entities: {db_overview[5]:,} ({db_overview[5]/db_overview[0]*100:.1f}%)\")\n            print(f\"   Active Models: {db_overview[6]:,}\")\n            print(f\"   Unique Sections Found: {db_overview[7]:,}\")\n            print(f\"   🎯 SECTION SUCCESS RATE: {entities_with_sections:,}/{total_entities:,} ({section_success_rate:.1f}%)\")\n            print(f\"   Last Extraction: {db_overview[9] or 'Never'}\")\n            \n            # Alert if section success rate is low\n            if section_success_rate < 90 and total_entities > 10:\n                print(f\"   🚨 WARNING: Section success rate is {section_success_rate:.1f}% - Pipeline routing issue!\")\n            elif section_success_rate >= 90:\n                print(f\"   ✅ EXCELLENT: Section success rate is {section_success_rate:.1f}% - Pipeline working correctly!\")\n        else:\n            print(f\"\\n📈 DATABASE OVERVIEW: No entities found - database is clean for testing\")\n        \n        cursor.close()\n        conn.close()\n                \n    except Exception as e:\n        print(f\"   ❌ Could not retrieve analytics: {e}\")\n    \n    # Pipeline statistics\n    try:\n        pipeline_stats = entity_pipeline.get_pipeline_statistics()\n        \n        print(f\"\\n🔧 PIPELINE STATISTICS:\")\n        print(f\"   Documents Processed: {pipeline_stats['pipeline_stats']['documents_processed']:,}\")\n        print(f\"   Total Entities Found: {pipeline_stats['pipeline_stats']['total_entities_extracted']:,}\")\n        print(f\"   Processing Time: {pipeline_stats['pipeline_stats']['processing_time_total']:.2f}s\")\n        print(f\"   Device: {pipeline_stats['device']}\")\n        print(f\"   Loaded Models: {', '.join(pipeline_stats['loaded_models'])}\")\n        print(f\"   Supported Sources: {', '.join(pipeline_stats['supported_data_sources'])}\")\n        \n        # Individual model statistics\n        print(f\"\\n📊 INDIVIDUAL MODEL PERFORMANCE:\")\n        for model_name, stats in pipeline_stats['model_stats'].items():\n            if stats['texts_processed'] > 0:\n                entities_per_text = stats['entities_found'] / stats['texts_processed']\n                avg_time = stats['processing_time'] / stats['texts_processed']\n                print(f\"   {model_name:>12}: {stats['texts_processed']:>4} texts | {stats['entities_found']:>5} entities | {entities_per_text:>4.1f} avg/text | {avg_time:>4.2f}s avg\")\n        \n        # Storage statistics\n        storage_stats = pipeline_storage.storage_stats\n        if storage_stats['total_entities_stored'] > 0:\n            print(f\"\\n💾 STORAGE STATISTICS:\")\n            print(f\"   Entities Stored: {storage_stats['total_entities_stored']:,}\")\n            print(f\"   Filings Processed: {storage_stats['filings_processed']:,}\")\n            print(f\"   Merged Entities: {storage_stats['merged_entities']:,}\")\n            print(f\"   Single-Model Entities: {storage_stats['single_model_entities']:,}\")\n            print(f\"   Failed Inserts: {storage_stats['failed_inserts']:,}\")\n            \n            merge_rate = (storage_stats['merged_entities'] / storage_stats['total_entities_stored'] * 100) if storage_stats['total_entities_stored'] > 0 else 0\n            print(f\"   Multi-Model Detection Rate: {merge_rate:.1f}%\")\n    \n    except Exception as e:\n        print(f\"\\n🔧 Pipeline statistics unavailable: {e}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"✅ EntityExtractionPipeline Analytics Complete!\")\n    print(\"=\"*80)\n\nprint(f\"\\n🎯 PRODUCTION COMMANDS:\")\nprint(f\"   • Process new filings: batch_results = process_filings_batch(limit=5)\")\nprint(f\"   • Check results:       generate_pipeline_analytics_report()\")\nprint(f\"   • View statistics:     context_retriever.get_retrieval_statistics()\")\n\nprint(f\"\\n✅ EntityExtractionPipeline Production Interface Ready!\")\nprint(f\"🔧 SINGLE ENTRY POINT: process_filings_batch() - ensures all extractions use section-based pipeline\")\nprint(f\"📊 Database cleared - ready for fresh testing with guaranteed section extraction!\")","metadata":{},"outputs":[],"execution_count":null}]}