{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Cell 0: Consolidated Imports and Auto-Logger Bootstrap\n# \n# Purpose: All imports consolidated here following PEP 8 order\n# Initialize basic logging - all other setup in Cell 1\n\n# ============================================================================\n# CONSOLIDATED IMPORTS - ALL IMPORTS FOR THE NOTEBOOK\n# ============================================================================\n\n# Standard library imports (alphabetical order)\nimport importlib\nimport importlib.util\nimport json\nimport os\nimport pickle\nimport re\nimport signal\nimport sys\nimport time\nimport traceback\nimport uuid\nimport warnings\n\n# Standard library from imports (alphabetical order)\nfrom collections import OrderedDict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom functools import wraps\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Set, Tuple\n\n# Third-party imports (alphabetical order)\nimport edgar\nimport numpy as np\nimport psycopg2\nimport requests\nimport torch\n\n# Third-party from imports (alphabetical order by module)\nfrom bs4 import BeautifulSoup\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nfrom huggingface_hub import login\nfrom IPython import get_ipython\nfrom ipykernel.iostream import OutStream\nfrom psycopg2 import pool\nfrom psycopg2.extras import execute_values\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoModelForTokenClassification, \n    pipeline,\n    BitsAndBytesConfig\n)\n\n# Environment imports\nfrom kaggle_secrets import UserSecretsClient\n\n# ============================================================================\n# AUTO-LOGGER BOOTSTRAP (USING CONSOLIDATED IMPORTS)\n# ============================================================================\n\n# Get GitHub token for logger access\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n\nprint(\"\ud83d\udd27 Setting up consolidated imports and logger bootstrap...\")\n\n# Clone/update repo for logger access\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\nif os.path.exists(LOCAL_PATH):\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\nelse:\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n\n# Add to path\nif f'{LOCAL_PATH}/BizIntel' not in sys.path:\n    sys.path.insert(0, f'{LOCAL_PATH}/BizIntel')\n\n# Initialize logger with minimal setup\nlogger_path = f\"{LOCAL_PATH}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\nif os.path.exists(logger_path):\n    spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_path)\n    auto_logger = importlib.util.module_from_spec(spec)\n    sys.modules[\"auto_logger\"] = auto_logger\n    spec.loader.exec_module(auto_logger)\n    \n    # Simple logger setup - database manager will be provided by Cell 1\n    logger = None  # Will be properly initialized after Cell 1 runs\n    print(\"\u2705 Auto-logger module loaded\")\nelse:\n    logger = None\n    print(\"\u26a0\ufe0f  Logger module not found - continuing without logging\")\n\nprint(\"\u2705 Cell 0: All imports consolidated (33+ imports) + bootstrap complete\")\nprint(\"   \ud83d\udce6 Standard library: importlib, json, os, pickle, re, signal, sys, time, etc.\")\nprint(\"   \ud83d\udd17 Third-party: edgar, numpy, psycopg2, requests, torch, transformers, bs4\")\nprint(\"   \ud83c\udf10 Environment: kaggle_secrets\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Cell 1: GitHub Setup and Simplified Configuration\n\n# Install required packages first\n!pip install edgartools transformers torch accelerate huggingface_hub requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n!pip install -U bitsandbytes --quiet\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION - Simplified\n# ============================================================================\n\n# Use Kaggle secrets for all sensitive credentials\nuser_secrets = UserSecretsClient()\n\n# Database Configuration (using Kaggle secrets for security)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"),\n    'user': user_secrets.get_secret(\"NEON_USER\"), \n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'sslmode': 'require'\n}\n\n# Master Configuration Dictionary - Simplified\nCONFIG = {\n    # GitHub Settings\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': f\"https://{user_secrets.get_secret('GITHUB_TOKEN')}@github.com/amiralpert/SmartReach.git\",\n        'local_path': \"/kaggle/working/SmartReach\"\n    },\n    \n    # Database Settings\n    'database': NEON_CONFIG,\n    \n    # Model Configuration\n    'models': {\n        'confidence_threshold': 0.8, # confidence NER model needs to store an entity \n        'batch_size': 16, # number of text chunks to process at one time through NER\n        'max_length': 512, # max chunk size to process through NER 512 token limit for BERT models \n        'chunk_overlap': 0.1,  # 10% overlap between chunks for complete entity extraction\n        'warm_up_enabled': True,\n        'warm_up_text': 'Pfizer announced FDA approval for new cancer drug targeting BRCA mutations.'\n    },\n    \n    # Cache Settings - Simplified\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 100,  # Maximum cache size in MB\n        'eviction_policy': 'LRU'  # Least Recently Used\n    },\n    \n    # Processing Settings - Simplified  \n    'processing': {\n        'filing_batch_size': 1,\n        'filing_query_limit': 1,       # Explicit limit for get_unprocessed_filings()\n        'enable_relationships': True,   # Enable/disable relationship extraction\n        'entity_batch_size': 10000,    # Max entities per database insert\n        'section_validation': True,    # Enforce section name validation\n        'debug_mode': False,\n        'max_insert_batch': 50000,     # Maximum batch for database inserts\n        'deduplication_threshold': 0.85\n    },\n    \n    # Llama 3.1 Configuration\n    'llama': {\n        'enabled': True,\n        'model_name': 'meta-llama/Llama-3.1-8B-Instruct',\n        'batch_size': 15,              # Entities per Llama call (for future batching)\n        'max_new_tokens': 50,          # Reduced from 200 for speed\n        'context_window': 400,         # Reduced from 1000 chars for speed  \n        'temperature': 0.3,            # Sampling temperature\n        'entity_context_window': 400,  # Reduced from 500 chars for entity context\n        'test_max_tokens': 50,         # For model testing\n        'min_confidence_filter': 0.8,  # Entity filtering threshold\n        'timeout_seconds': 30,         # Timeout for model calls\n    },\n    \n    # EdgarTools Settings\n    'edgar': {\n        'identity': \"SmartReach BizIntel amir@leanbio.consulting\"\n    }\n}\n\nif not CONFIG['github']['token']:\n    raise ValueError(\"\u274c GITHUB_TOKEN is required in Kaggle secrets\")\n\nif not CONFIG['database']['password']:\n    raise ValueError(\"\u274c NEON_PASSWORD is required in Kaggle secrets\")\n\nprint(\"\u2705 Configuration loaded from Kaggle secrets\")\nprint(f\"   Database: {CONFIG['database']['host']}\")\nprint(f\"   Processing: Filing batch={CONFIG['processing']['filing_batch_size']}, Query limit={CONFIG['processing']['filing_query_limit']}\")\nprint(f\"   Llama 3.1: Enabled={CONFIG['llama']['enabled']}, Tokens={CONFIG['llama']['max_new_tokens']}, Context={CONFIG['llama']['context_window']}\")\nprint(f\"   Cache: Max size={CONFIG['cache']['max_size_mb']}MB\")\nprint(f\"   Relationships: {'Enabled' if CONFIG['processing']['enable_relationships'] else 'Disabled'}\")\n\n# ============================================================================\n# ERROR LOGGING FUNCTIONS\n# ============================================================================\n\ndef log_error(component: str, message: str, exception: Exception = None, context: dict = None):\n    \"\"\"Enhanced error logging with stack traces\"\"\"\n    if exception:\n        error_msg = f\"ERROR [{component}]: {message} - {type(exception).__name__}: {str(exception)}\"\n        # Add stack trace for debugging\n        if CONFIG['processing'].get('debug_mode', False):\n            error_msg += f\"\nStack trace:\n{traceback.format_exc()}\"\n    else:\n        error_msg = f\"ERROR [{component}]: {message}\"\n    \n    if context:\n        error_msg += f\" | Context: {context}\"\n    \n    print(error_msg)  # Auto-logger captures this\n    return error_msg\n\ndef log_warning(component: str, message: str, context: dict = None):\n    \"\"\"Standardized warning logging\"\"\"\n    warning_msg = f\"WARNING [{component}]: {message}\"\n    if context:\n        warning_msg += f\" | Context: {context}\"\n    print(warning_msg)\n    return warning_msg\n\ndef log_info(component: str, message: str):\n    \"\"\"Standardized info logging\"\"\"\n    info_msg = f\"INFO [{component}]: {message}\"\n    print(info_msg)\n    return info_msg\n\n# ============================================================================\n# SIMPLE DATABASE CONNECTION\n# ============================================================================\n\n@contextmanager\ndef get_db_connection():\n    \"\"\"Simple database connection context manager\"\"\"\n    conn = None\n    try:\n        conn = psycopg2.connect(**CONFIG['database'])\n        yield conn\n        conn.commit()\n    except Exception as e:\n        if conn:\n            conn.rollback()\n        raise e\n    finally:\n        if conn:\n            conn.close()\n\n# ============================================================================\n# SIZE-LIMITED LRU CACHE\n# ============================================================================\n\nclass SizeLimitedLRUCache:\n    \"\"\"LRU cache with size limit in MB\"\"\"\n    \n    def __init__(self, max_size_mb: int):\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.cache = OrderedDict()\n        self.current_size = 0\n        self.hits = 0\n        self.misses = 0\n    \n    def _estimate_size(self, value: str) -> int:\n        \"\"\"Estimate size of cached value in bytes\"\"\"\n        return len(value.encode('utf-8')) if isinstance(value, str) else sys.getsizeof(value)\n    \n    def get(self, key: str):\n        \"\"\"Get item from cache\"\"\"\n        if key in self.cache:\n            self.hits += 1\n            # Move to end (most recently used)\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        self.misses += 1\n        return None\n    \n    def put(self, key: str, value, size: int = None):\n        \"\"\"Put item in cache with LRU eviction\"\"\"\n        if size is None:\n            size = self._estimate_size(value)\n        \n        # Remove old entries if needed\n        while self.current_size + size > self.max_size_bytes and self.cache:\n            evicted_key, evicted_value = self.cache.popitem(last=False)\n            self.current_size -= self._estimate_size(evicted_value)\n            log_info(\"Cache\", f\"Evicted {evicted_key} to maintain size limit\")\n        \n        # Add new entry\n        if key in self.cache:\n            self.current_size -= self._estimate_size(self.cache[key])\n        \n        self.cache[key] = value\n        self.current_size += size\n        self.cache.move_to_end(key)\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get cache statistics\"\"\"\n        hit_rate = (self.hits / (self.hits + self.misses) * 100) if (self.hits + self.misses) > 0 else 0\n        return {\n            'entries': len(self.cache),\n            'size_mb': self.current_size / (1024 * 1024),\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': hit_rate\n        }\n\n# Initialize global cache for EdgarTools sections\nSECTION_CACHE = SizeLimitedLRUCache(CONFIG['cache']['max_size_mb'])\n\n# ============================================================================\n# GITHUB SETUP\n# ============================================================================\n\nprint(\"\n\ud83d\udce6 Setting up GitHub repository...\")\nlocal_path = CONFIG['github']['local_path']\nrepo_url = CONFIG['github']['repo_url']\n\n# Clone or update repo with force pull\nif os.path.exists(local_path):\n    log_info(\"GitHub\", f\"Repository exists at {local_path}\")\n    log_info(\"GitHub\", \"Force updating from main branch\")\n    !cd {local_path} && git fetch origin\n    !cd {local_path} && git reset --hard origin/main\n    !cd {local_path} && git pull origin main\n    log_info(\"GitHub\", \"Repository updated successfully\")\n    \n    # Show current commit\n    !cd {local_path} && echo \"Current commit:\" && git log --oneline -1\nelse:\n    log_info(\"GitHub\", f\"Cloning repository to {local_path}\")\n    !git clone {repo_url} {local_path}\n    log_info(\"GitHub\", \"Repository cloned successfully\")\n\n# Clear any cached modules from previous runs\nmodules_to_clear = [key for key in sys.modules.keys() if 'auto_logger' in key.lower() or 'clean' in key.lower()]\nfor mod in modules_to_clear:\n    del sys.modules[mod]\n    log_info(\"ModuleCache\", f\"Cleared cached module: {mod}\")\n\n# Add to Python path for regular imports\nif f'{local_path}/BizIntel' in sys.path:\n    sys.path.remove(f'{local_path}/BizIntel')\nsys.path.insert(0, f'{local_path}/BizIntel')\n\nlog_info(\"Setup\", \"Python path configured for SEC entity extraction\")\n\n# Configure EdgarTools authentication - REQUIRED by SEC\nset_identity(CONFIG['edgar']['identity'])\nlog_info(\"EdgarTools\", f\"Identity configured: {CONFIG['edgar']['identity']}\")\n\nprint(\"\ud83d\ude80 SEC ENTITY EXTRACTION ENGINE INITIALIZED - SIMPLIFIED AND CLEAN\")\nprint(\"=\"*80)\nprint(f\"\u2705 GitHub: Repository ready at {local_path}\")\nprint(f\"\u2705 Database: Connected to {CONFIG['database']['host']}\")\nprint(f\"\u2705 Cache: {CONFIG['cache']['max_size_mb']}MB LRU cache initialized\")\nprint(f\"\u2705 EdgarTools: Identity set for SEC compliance\")\nprint(\"=\"*80)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Cell 2: Database Functions and ORM-like Models with Batching - SIMPLIFIED\n\n# Basic startup check - restart kernel if issues persist\nprint(\"Starting Cell 2 - EdgarTools section extraction\")\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# ============================================================================\n# TIMEOUT HANDLER FOR EDGARTOOLS API CALLS - FIX FOR HANGING\n# ============================================================================\n\nclass TimeoutError(Exception):\n    \"\"\"Custom timeout exception\"\"\"\n    pass\n\ndef timeout_handler(signum, frame):\n    \"\"\"Signal handler for timeout\"\"\"\n    raise TimeoutError(\"EdgarTools API call timed out\")\n\ndef with_timeout(seconds=30):\n    \"\"\"Decorator to add timeout to functions\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            # Set the signal alarm\n            signal.signal(signal.SIGALRM, timeout_handler)\n            signal.alarm(seconds)\n            try:\n                result = func(*args, **kwargs)\n            finally:\n                # Disable the alarm\n                signal.alarm(0)\n            return result\n        return wrapper\n    return decorator\n\n# ============================================================================\n# PROBLEMATIC FILINGS TO SKIP\n# ============================================================================\n\n# Known problematic filings that cause indefinite hangs\nPROBLEMATIC_FILINGS = [\n    '0001699031-25-000166',  # Grail 10-Q that caused 11+ hour hang\n]\n\n# ============================================================================\n# TIMEOUT-WRAPPED EDGARTOOLS CALLS\n# ============================================================================\n\n@with_timeout(30)  # 30 second timeout\ndef find_filing_with_timeout(accession_number: str):\n    \"\"\"Find filing with timeout protection\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting EdgarTools find() for {accession_number}\")\n    filing = find(accession_number)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] find() completed successfully\")\n    return filing\n\n@with_timeout(60)  # 60 second timeout for HTML download\ndef get_html_with_timeout(filing):\n    \"\"\"Get HTML content with timeout protection\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting html() fetch...\")\n    html_content = filing.html()\n    if html_content:\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] html() completed, size: {len(html_content):,} bytes\")\n    else:\n        print(f\"[{datetime.now().strftime('%H:%M:%S')}] html() returned empty content\")\n    return html_content\n\n@with_timeout(30)  # 30 second timeout for parsing\ndef parse_html_with_timeout(html_content):\n    \"\"\"Parse HTML with timeout protection\"\"\"\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting HTML parsing...\")\n    document = parse_html(html_content)\n    print(f\"[{datetime.now().strftime('%H:%M:%S')}] HTML parsing completed\")\n    return document\n\ndef get_filing_sections(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get structured sections from SEC filing using accession number\n    \n    ENHANCED: With timeouts, progress monitoring, and problematic filing skipping\n    \"\"\"\n    # Skip known problematic filings\n    if accession_number in PROBLEMATIC_FILINGS:\n        log_warning(\"EdgarTools\", f\"Skipping known problematic filing: {accession_number}\")\n        return {}\n    \n    # Check cache first\n    cache_key = f\"{accession_number}#{filing_type or 'UNKNOWN'}\"\n    cached_sections = SECTION_CACHE.get(cache_key)\n    if cached_sections:\n        log_info(\"Cache\", f\"Cache hit for {accession_number}\")\n        return cached_sections\n    \n    try:\n        # Find filing with timeout protection\n        try:\n            filing = find_filing_with_timeout(accession_number)\n        except TimeoutError:\n            log_error(\"EdgarTools\", f\"Timeout finding filing {accession_number} (30s exceeded)\")\n            return {}\n        \n        if not filing:\n            raise ValueError(f\"Filing not found for accession: {accession_number}\")\n            \n        # Auto-detect filing type if not provided\n        if not filing_type:\n            filing_type = getattr(filing, 'form', '10-K')\n        \n        log_info(\"EdgarTools\", f\"Found {filing_type} for {getattr(filing, 'company', 'Unknown Company')}\")\n        \n        # Get structured HTML content with timeout\n        try:\n            html_content = get_html_with_timeout(filing)\n        except TimeoutError:\n            log_error(\"EdgarTools\", f\"Timeout fetching HTML for {accession_number} (60s exceeded)\")\n            return {}\n        \n        if not html_content:\n            raise ValueError(\"No HTML content available\")\n        \n        # Limit HTML size to prevent memory issues\n        MAX_HTML_SIZE = 10 * 1024 * 1024  # 10MB limit\n        if len(html_content) > MAX_HTML_SIZE:\n            log_warning(\"EdgarTools\", f\"HTML too large ({len(html_content):,} bytes), truncating to {MAX_HTML_SIZE:,}\")\n            html_content = html_content[:MAX_HTML_SIZE]\n        \n        # Parse HTML to Document object with timeout\n        try:\n            document = parse_html_with_timeout(html_content)\n        except TimeoutError:\n            log_error(\"EdgarTools\", f\"Timeout parsing HTML for {accession_number} (30s exceeded)\")\n            return {}\n        \n        # Extract sections using SectionExtractor\n        extractor = SectionExtractor(filing_type=filing_type)\n        sections = extractor.extract(document)\n        \n        log_info(\"EdgarTools\", f\"SectionExtractor found {len(sections)} sections\")\n        \n        # Convert sections to text dictionary\n        section_texts = {}\n        for section_name, section in sections.items():\n            try:\n                if hasattr(section, 'text'):\n                    text = section.text() if callable(section.text) else section.text\n                    if isinstance(text, str) and text.strip():\n                        section_texts[section_name] = text.strip()\n                        print(f\"      \u2022 {section_name}: {len(text):,} chars\")\n                elif hasattr(section, '__str__'):\n                    text = str(section).strip()\n                    if text:\n                        section_texts[section_name] = text\n                        print(f\"      \u2022 {section_name}: {len(text):,} chars (via str)\")\n            except Exception as section_e:\n                log_warning(\"EdgarTools\", f\"Could not extract section {section_name}\", {\"error\": str(section_e)})\n                continue\n        \n        # If SectionExtractor returns no sections, fall back to full document text\n        if not section_texts:\n            log_warning(\"EdgarTools\", \"No structured sections found, using full document fallback\")\n            full_text = document.text() if hasattr(document, 'text') and callable(document.text) else str(document)\n            if full_text and len(full_text.strip()) > 100:  # Only use if substantial content\n                # Limit full document size\n                if len(full_text) > MAX_HTML_SIZE:\n                    log_warning(\"EdgarTools\", f\"Full document too large ({len(full_text):,} chars), truncating\")\n                    full_text = full_text[:MAX_HTML_SIZE]\n                section_texts['full_document'] = full_text.strip()\n                log_info(\"EdgarTools\", f\"Using full document: {len(full_text):,} chars\")\n        \n        # Cache the result\n        if section_texts and CONFIG['cache']['enabled']:\n            SECTION_CACHE.put(cache_key, section_texts)\n            log_info(\"Cache\", f\"Cached sections for {accession_number} ({len(section_texts)} sections)\")\n        \n        return section_texts\n        \n    except Exception as e:\n        log_error(\"EdgarTools\", f\"Failed to fetch filing {accession_number}\", e)\n        return {}  # Return empty dict on network/API failure\n\ndef route_sections_to_models(sections: Dict[str, str], filing_type: str) -> Dict[str, List[str]]:\n    \"\"\"Route sections to appropriate NER models based on filing type\"\"\"\n    routing = {\n        'biobert': [],\n        'bert_base': [],\n        'roberta': [],\n        'finbert': []\n    }\n    \n    if filing_type.upper() in ['10-K', '10-Q']:\n        for section_name, section_text in sections.items():\n            # FinBERT gets financial statements exclusively\n            if 'financial' in section_name.lower() or 'statement' in section_name.lower():\n                routing['finbert'].append(section_name)\n            else:\n                # All other sections go to BERT/RoBERTa/BioBERT\n                routing['bert_base'].append(section_name)\n                routing['roberta'].append(section_name)\n                routing['biobert'].append(section_name)\n    \n    elif filing_type.upper() == '8-K':\n        # 8-K: all item sections go to all four models\n        for section_name in sections.keys():\n            routing['biobert'].append(section_name)\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['finbert'].append(section_name)\n    \n    else:\n        # Default routing for other filing types\n        for section_name in sections.keys():\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['biobert'].append(section_name)\n    \n    # Remove empty routing\n    routing = {model: sections_list for model, sections_list in routing.items() if sections_list}\n    \n    return routing\n\ndef process_sec_filing_with_sections(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing with section-based extraction\n    \n    ENHANCED: With timeout protection and progress monitoring\n    \"\"\"\n    try:\n        filing_id = filing_data.get('id')\n        accession_number = filing_data.get('accession_number')  # DIRECT FROM DATABASE\n        filing_type = filing_data.get('filing_type', '10-K')\n        company_domain = filing_data.get('company_domain', 'Unknown')\n        filing_url = filing_data.get('url')  # Still keep for reference\n        \n        log_info(\"FilingProcessor\", f\"Processing {filing_type} for {company_domain}\")\n        print(f\"   \ud83d\udcc4 Filing ID: {filing_id}\")\n        print(f\"   \ud83d\udcd1 Accession: {accession_number}\")\n        \n        # Validate accession number\n        if not accession_number:\n            raise ValueError(f\"Missing accession number for filing {filing_id}\")\n        \n        # Check if this is a problematic filing\n        if accession_number in PROBLEMATIC_FILINGS:\n            log_warning(\"FilingProcessor\", f\"Skipping problematic filing: {accession_number}\")\n            return {\n                'filing_id': filing_id,\n                'company_domain': company_domain,\n                'filing_type': filing_type,\n                'accession_number': accession_number,\n                'error': 'Skipped - known problematic filing',\n                'processing_status': 'skipped'\n            }\n        \n        # Get structured sections using accession directly\n        sections = get_filing_sections(accession_number, filing_type)\n        if not sections:\n            raise ValueError(\"No sections extracted\")\n        \n        log_info(\"FilingProcessor\", f\"Extracted {len(sections)} sections\")\n        \n        # Route sections to models\n        model_routing = route_sections_to_models(sections, filing_type)\n        print(f\"   \ud83c\udfaf Model routing: {[f'{model}: {len(secs)} sections' for model, secs in model_routing.items()]}\")\n        \n        # Validate section names if configured\n        if CONFIG['processing']['section_validation']:\n            missing_sections = [name for name in sections.keys() if not name]\n            if missing_sections:\n                log_warning(\"FilingProcessor\", f\"Found {len(missing_sections)} sections without names\")\n        \n        # Show cache statistics\n        cache_stats = SECTION_CACHE.get_stats()\n        if cache_stats['hits'] > 0:\n            print(f\"   \ud83d\udcca Cache: {cache_stats['hit_rate']:.1f}% hit rate, {cache_stats['size_mb']:.1f}MB used\")\n        \n        return {\n            'filing_id': filing_id,\n            'company_domain': company_domain,\n            'filing_type': filing_type,\n            'accession_number': accession_number,\n            'url': filing_url,\n            'sections': sections,\n            'model_routing': model_routing,\n            'total_sections': len(sections),\n            'processing_status': 'success'\n        }\n        \n    except TimeoutError as e:\n        log_error(\"FilingProcessor\", \"Filing processing timed out\", e, \n                 {\"filing_id\": filing_data.get('id'), \"accession\": filing_data.get('accession_number')})\n        return {\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain', 'Unknown'),\n            'filing_type': filing_data.get('filing_type', 'Unknown'),\n            'accession_number': filing_data.get('accession_number'),\n            'error': 'Processing timeout',\n            'processing_status': 'timeout'\n        }\n    except Exception as e:\n        log_error(\"FilingProcessor\", \"Filing processing failed\", e, \n                 {\"filing_id\": filing_data.get('id'), \"accession\": filing_data.get('accession_number')})\n        return {\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain', 'Unknown'),\n            'filing_type': filing_data.get('filing_type', 'Unknown'),\n            'accession_number': filing_data.get('accession_number'),\n            'error': str(e),\n            'processing_status': 'failed'\n        }\n\ndef get_unprocessed_filings(limit: int = 5) -> List[Dict]:\n    \"\"\"Get SEC filings that haven't been processed yet\n    \n    ENHANCED: Skip known problematic filings\n    \"\"\"\n    with get_db_connection() as conn:  # PHASE 2: Using context manager\n        cursor = conn.cursor()\n        \n        # Build exclusion list for SQL\n        exclusion_list = \"', '\".join(PROBLEMATIC_FILINGS)\n        exclusion_clause = f\"AND sf.accession_number NOT IN ('{exclusion_list}')\" if PROBLEMATIC_FILINGS else \"\"\n        \n        cursor.execute(f\"\"\"\n            SELECT \n                sf.id, \n                sf.company_domain, \n                sf.filing_type, \n                sf.accession_number,\n                sf.url, \n                sf.filing_date, \n                sf.title\n            FROM raw_data.sec_filings sf\n            LEFT JOIN system_uno.sec_entities_raw ser \n                ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n            WHERE sf.accession_number IS NOT NULL  -- Must have accession\n                AND ser.sec_filing_ref IS NULL     -- Not yet processed\n                {exclusion_clause}                 -- Skip problematic filings\n            ORDER BY sf.filing_date DESC\n            LIMIT %s\n        \"\"\", (limit,))\n        \n        filings = cursor.fetchall()\n        cursor.close()\n        \n        log_info(\"DatabaseQuery\", f\"Retrieved {len(filings)} unprocessed filings (excluded {len(PROBLEMATIC_FILINGS)} problematic)\")\n        \n        return [{\n            'id': filing[0],\n            'company_domain': filing[1],\n            'filing_type': filing[2],\n            'accession_number': filing[3],\n            'url': filing[4],\n            'filing_date': filing[5],\n            'title': filing[6]\n        } for filing in filings]\n\n# Test the simplified extraction with timeout protection\nlog_info(\"Test\", \"Starting section extraction test with timeout protection\")\n\ntest_filings = get_unprocessed_filings(limit=1)\n\nif test_filings:\n    print(f\"\n\ud83e\uddea Testing with filing: {test_filings[0]['company_domain']} - {test_filings[0]['filing_type']}\")\n    print(f\"   Accession: {test_filings[0]['accession_number']}\")\n    \n    test_result = process_sec_filing_with_sections(test_filings[0])\n    \n    if test_result['processing_status'] == 'success':\n        log_info(\"Test\", f\"\u2705 Successfully extracted {test_result['total_sections']} sections\")\n    elif test_result['processing_status'] == 'timeout':\n        log_warning(\"Test\", f\"\u23f1\ufe0f Processing timed out - filing may be too large or slow\")\n    elif test_result['processing_status'] == 'skipped':\n        log_info(\"Test\", f\"\u23ed\ufe0f Skipped problematic filing\")\n    else:\n        log_error(\"Test\", f\"\u274c Section extraction failed: {test_result.get('error')}\")\nelse:\n    log_info(\"Test\", \"No test filings available (all may be processed or problematic)\")\n\nprint(\"\u2705 Cell 2 complete - EdgarTools section extraction with timeout protection ready\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-13T18:10:05.069316Z",
          "iopub.execute_input": "2025-09-13T18:10:05.069606Z",
          "iopub.status.idle": "2025-09-13T18:10:12.336615Z",
          "shell.execute_reply.started": "2025-09-13T18:10:05.069581Z",
          "shell.execute_reply": "2025-09-13T18:10:12.336382Z"
        },
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "# Cell 3: Optimized Entity Extraction Pipeline - Uses Cell 2's Pre-Extracted Sections\n\nprint(\"\ud83d\ude80 Loading Optimized EntityExtractionPipeline (Handler Classes Eliminated)...\")\n\nclass EntityExtractionPipeline:\n    \"\"\"Streamlined entity extraction using Cell 2's pre-processed sections and routing\"\"\"\n    \n    def __init__(self, config: Dict):\n        self.config = config\n        self.models = {}\n        self.stats = {\n            \"entities_extracted\": 0,\n            \"entities_filtered\": 0,\n            \"sections_processed\": 0,\n            \"filings_processed\": 0\n        }\n        \n        # Essential filtering from CONFIG\n        self._biobert_skip_categories = {'0'}  # Skip BioBERT category \"0\"\n        self._finbert_common_words = {'the', 'and', 'or', 'but', 'company', 'inc', 'corporation', 'corp'}\n        self._bert_skip_misc = True  # Skip BERT MISC category\n        \n        # Map Cell 2's routing names to our model names\n        self._routing_to_model_map = {\n            'biobert': 'biobert',\n            'bert_base': 'bert',      # Cell 2 uses 'bert_base'\n            'roberta': 'roberta', \n            'finbert': 'finbert'\n        }\n        \n        self._load_models()\n    \n    def _load_models(self):\n        \"\"\"Load NER models efficiently\"\"\"\n        model_configs = [\n            ('biobert', 'alvaroalon2/biobert_diseases_ner'),\n            ('bert', 'dslim/bert-base-NER'),\n            ('finbert', 'ProsusAI/finbert'), \n            ('roberta', 'Jean-Baptiste/roberta-large-ner-english')\n        ]\n        \n        # Determine device\n        device = -1  # CPU by default\n        if torch.cuda.is_available():\n            device = 0\n            print(\"   \ud83d\ude80 Using GPU acceleration\")\n        else:\n            print(\"   \ud83d\udcbb Using CPU (GPU not available)\")\n        \n        for name, model_id in model_configs:\n            try:\n                self.models[name] = pipeline(\n                    \"ner\",\n                    model=model_id,\n                    aggregation_strategy=\"average\",\n                    device=device\n                )\n                print(f\"      \u2713 {name} loaded\")\n            except Exception as e:\n                print(f\"      \u274c Failed to load {name}: {e}\")\n        \n        print(f\"   \u2705 Loaded {len(self.models)} NER models\")\n        \n        # Warm up models if enabled\n        if self.config.get('models', {}).get('warm_up_enabled', False):\n            self._warm_up_models()\n    \n    def _warm_up_models(self):\n        \"\"\"Warm up models with test text\"\"\"\n        test_text = self.config.get('models', {}).get('warm_up_text', 'Test entity extraction.')\n        print(\"   \ud83d\udd25 Warming up models...\")\n        \n        for name, model in self.models.items():\n            try:\n                model(test_text)\n                print(f\"      \u2713 {name} warmed up\")\n            except Exception as e:\n                print(f\"      \u26a0\ufe0f {name} warm-up failed: {e}\")\n    \n    def process_filing_entities(self, filing_data: Dict) -> List[Dict]:\n        \"\"\"Main function: Extract entities using Cell 2's section extraction and routing\"\"\"\n        \n        # Step 1: Use Cell 2's section extraction function directly\n        section_result = process_sec_filing_with_sections(filing_data)\n        \n        if section_result['processing_status'] != 'success':\n            print(f\"   \u274c Section extraction failed: {section_result.get('error', 'Unknown')}\")\n            return []\n        \n        # Step 2: Extract entities using Cell 2's sections and routing\n        entities = self._extract_entities_from_sections(section_result)\n        \n        self.stats['filings_processed'] += 1\n        self.stats['entities_extracted'] += len(entities)\n        \n        print(f\"   \u2705 Extracted {len(entities)} entities from {section_result['total_sections']} sections\")\n        \n        return entities\n    \n    def _extract_entities_from_sections(self, section_result: Dict) -> List[Dict]:\n        \"\"\"Extract entities using Cell 2's sections and model routing\"\"\"\n        sections = section_result['sections']\n        model_routing = section_result['model_routing']\n        \n        all_entities = []\n        self.stats['sections_processed'] += len(sections)\n        \n        # Process each model's assigned sections (using Cell 2's routing)\n        for routing_model_name, assigned_section_names in model_routing.items():\n            \n            # Map Cell 2's model name to our model name\n            our_model_name = self._routing_to_model_map.get(routing_model_name)\n            \n            if not our_model_name or our_model_name not in self.models:\n                print(f\"      \u26a0\ufe0f Model '{routing_model_name}' -> '{our_model_name}' not available\")\n                continue\n            \n            print(f\"      \ud83d\udd04 Processing {len(assigned_section_names)} sections with {our_model_name}\")\n            \n            # Extract entities from each assigned section\n            for section_name in assigned_section_names:\n                section_text = sections.get(section_name)\n                if not section_text:\n                    continue\n                \n                section_entities = self._extract_from_single_section(\n                    section_text, our_model_name, section_name, section_result\n                )\n                all_entities.extend(section_entities)\n        \n        # Merge overlapping entities\n        merged_entities = self._merge_entities(all_entities)\n        \n        return merged_entities\n    \n    def _extract_from_single_section(self, section_text: str, model_name: str, \n                                   section_name: str, section_result: Dict) -> List[Dict]:\n        \"\"\"Extract entities from single section with essential filtering\"\"\"\n        try:\n            # Extract raw entities\n            raw_entities = self.models[model_name](section_text)\n            \n            filtered_entities = []\n            for entity in raw_entities:\n                # Apply confidence threshold\n                if entity['score'] < self.config.get('models', {}).get('confidence_threshold', 0.5):\n                    continue\n                \n                entity_text = entity['word'].strip()\n                entity_category = entity['entity_group']\n                \n                # Apply essential model-specific filtering\n                if not self._passes_essential_filters(model_name, entity_text, entity_category):\n                    self.stats['entities_filtered'] += 1\n                    continue\n                \n                filtered_entities.append({\n                    'extraction_id': str(uuid.uuid4()),\n                    'company_domain': section_result['company_domain'],\n                    'entity_text': entity_text,\n                    'entity_category': self._normalize_entity_type(entity_category),\n                    'confidence_score': float(entity['score']),\n                    'character_start': entity['start'],\n                    'character_end': entity['end'],\n                    'section_name': section_name,\n                    'sec_filing_ref': f\"SEC_{section_result['filing_id']}\",\n                    'primary_model': model_name,\n                    'filing_type': section_result['filing_type'],\n                    'filing_date': section_result.get('filing_date'),\n                    'accession_number': section_result['accession_number'],\n                    'model_source': model_name,\n                    'surrounding_text': self._get_surrounding_text(section_text, entity['start'], entity['end']),\n                    'data_source': 'sec_filings',\n                    'extraction_timestamp': datetime.now()\n                })\n            \n            return filtered_entities\n            \n        except Exception as e:\n            print(f\"      \u274c Entity extraction failed with {model_name} on {section_name}: {e}\")\n            return []\n    \n    def _passes_essential_filters(self, model_name: str, entity_text: str, entity_category: str) -> bool:\n        \"\"\"Essential filtering logic per model (consolidated from handler classes)\"\"\"\n        entity_lower = entity_text.lower()\n        \n        # Essential filtering based on model\n        if model_name == 'biobert':\n            # Skip BioBERT category \"0\" (non-medical text misclassified)\n            return entity_category not in self._biobert_skip_categories\n        \n        elif model_name == 'finbert':\n            # Skip common words for FinBERT\n            return entity_lower not in self._finbert_common_words\n        \n        elif model_name == 'bert':\n            # Skip BERT MISC category if configured\n            return not (self._bert_skip_misc and entity_category == 'MISC')\n        \n        # RoBERTa and others: minimal filtering\n        return len(entity_text) >= 2\n    \n    def _normalize_entity_type(self, entity_type: str) -> str:\n        \"\"\"Normalize entity types across models\"\"\"\n        mappings = {\n            'Disease': 'MEDICAL_CONDITION',\n            'Chemical': 'MEDICATION',\n            'Drug': 'MEDICATION',\n            'PER': 'PERSON',\n            'ORG': 'ORGANIZATION', \n            'LOC': 'LOCATION',\n            'MONEY': 'FINANCIAL',\n            'PERCENT': 'FINANCIAL'\n        }\n        return mappings.get(entity_type, entity_type.upper())\n    \n    def _get_surrounding_text(self, section_text: str, start: int, end: int, window: int = 100) -> str:\n        \"\"\"Get surrounding text for context\"\"\"\n        text_start = max(0, start - window)\n        text_end = min(len(section_text), end + window)\n        return section_text[text_start:text_end]\n    \n    def _merge_entities(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Simplified entity merging - highest confidence wins\"\"\"\n        if not entities:\n            return []\n        \n        # Group by position within same section and filing\n        position_groups = {}\n        for entity in entities:\n            key = (entity['sec_filing_ref'], entity['section_name'], \n                  entity['character_start'], entity['character_end'])\n            position_groups.setdefault(key, []).append(entity)\n        \n        # Merge logic: take highest confidence entity from each group\n        merged = []\n        for group in position_groups.values():\n            if len(group) == 1:\n                # Single entity - keep as is\n                entity = group[0]\n                entity['is_merged'] = False\n                entity['models_detected'] = [entity['primary_model']]\n                merged.append(entity)\n            else:\n                # Multiple entities at same position - merge\n                best_entity = max(group, key=lambda x: x['confidence_score'])\n                best_entity['is_merged'] = True\n                best_entity['models_detected'] = [e['primary_model'] for e in group]\n                best_entity['all_confidences'] = {e['primary_model']: e['confidence_score'] for e in group}\n                merged.append(best_entity)\n        \n        return merged\n    \n    def get_extraction_stats(self) -> Dict:\n        \"\"\"Get extraction statistics\"\"\"\n        return {\n            'models_loaded': len(self.models),\n            'filings_processed': self.stats['filings_processed'],\n            'sections_processed': self.stats['sections_processed'],\n            'entities_extracted': self.stats['entities_extracted'],\n            'entities_filtered': self.stats['entities_filtered'],\n            'filter_rate': f\"{(self.stats['entities_filtered'] / max(1, self.stats['entities_extracted'] + self.stats['entities_filtered']) * 100):.1f}%\"\n        }\n\n# Initialize the entity extraction pipeline\nentity_pipeline = EntityExtractionPipeline(CONFIG)\n\nprint(f\"\u2705 EntityExtractionPipeline initialized:\")\nstats = entity_pipeline.get_extraction_stats()\nfor key, value in stats.items():\n    print(f\"   \u2022 {key}: {value}\")\n\nprint(\"\u2705 Cell 3 complete - Optimized entity extraction ready (handler classes eliminated)\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-13T18:10:12.338593Z",
          "iopub.execute_input": "2025-09-13T18:10:12.338823Z",
          "iopub.status.idle": "2025-09-13T18:10:58.440614Z",
          "shell.execute_reply.started": "2025-09-13T18:10:12.338803Z",
          "shell.execute_reply": "2025-09-13T18:10:58.440277Z"
        },
        "jupyter": {
          "source_hidden": true
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Relationship Extractor with Local Llama 3.1-8B\n\nprint(\"\ud83d\ude80 Loading In-Memory Pipeline with Storage and Local Llama 3.1-8B...\")\n\n# ================================================================================\n# RELATIONSHIP EXTRACTOR WITH LOCAL LLAMA 3.1-8B\n# ================================================================================\n\nclass RelationshipExtractor:\n    \"\"\"Extract company-centric relationships using local Llama 3.1-8B\"\"\"\n    \n    def __init__(self, llama_config: Dict = None):\n        \"\"\"Initialize with local Llama 3.1-8B model\"\"\"\n        self.config = llama_config or CONFIG.get('llama', {})\n        self.model = None\n        self.tokenizer = None\n        self.stats = {\n            'entities_processed': 0,\n            'relationships_found': 0,\n            'llama_calls': 0,\n            'processing_time': 0\n        }\n        \n                \n        # Verify CONFIG is available\n        if not CONFIG.get('llama', {}).get('enabled', False):\n            print(\"   \u26a0\ufe0f Llama configuration disabled in CONFIG\")\n            return\n        \n        try:\n            # Auto-login to HuggingFace using Kaggle secret\n            print(\"   \ud83d\udd10 Logging in to HuggingFace...\")\n            user_secrets = UserSecretsClient()\n            hf_token = user_secrets.get_secret('HUGGINGFACE_TOKEN')\n            \n            if hf_token:\n                login(token=hf_token, add_to_git_credential=False)\n                print(\"   \u2705 Logged in to HuggingFace\")\n            else:\n                print(\"   \u26a0\ufe0f No HUGGINGFACE_TOKEN found in Kaggle secrets\")\n                return\n            \n            # Configure 4-bit quantization for memory efficiency\n            print(\"   \u2699\ufe0f Configuring 4-bit quantization...\")\n            bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.bfloat16\n            )\n            \n            # Load Llama 3.1-8B model\n            print(\"   \ud83d\udce5 Loading Llama 3.1-8B-Instruct (this may take a minute)...\")\n            model_name = CONFIG[\"llama\"][\"model_name\"]\n            \n            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n            self.model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                trust_remote_code=True\n            )\n            \n            print(\"   \u2705 Llama 3.1-8B loaded successfully (4-bit quantized)\")\n            \n            # Test the model\n            test_messages = [\n                {\"role\": \"user\", \"content\": \"What is a partnership? Answer in one sentence.\"}\n            ]\n            test_input = self.tokenizer.apply_chat_template(test_messages, return_tensors=\"pt\", tokenize=True)\n            \n            with torch.no_grad():\n                outputs = self.model.generate(test_input, max_new_tokens=CONFIG[\"llama\"][\"test_max_tokens\"], temperature=CONFIG[\"llama\"][\"temperature\"])\n            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            print(f\"   \ud83e\uddea Test response: {response[:100]}...\")\n            \n        except Exception as e:\n            print(f\"   \u274c Failed to load Llama 3.1-8B: {e}\")\n            print(\"   \u26a0\ufe0f Relationship extraction will be disabled\")\n            self.model = None\n            self.tokenizer = None\n    \n    def extract_company_relationships(self, \n                                     entities: List[Dict], \n                                     sections: Dict[str, str],\n                                     company_domain: str) -> List[Dict]:\n        \"\"\"Extract relationships between company and all found entities\"\"\"\n        if not self.model or not self.tokenizer or not entities:\n            return []\n        \n        print(f\"   \ud83d\udd0d Analyzing relationships for {company_domain}\")\n        \n        # Trust Cell 3's filtering - entities are already high-quality\n        \n        relationships = []\n        \n        # Group entities by section for context efficiency\n        entities_by_section = {}\n        for entity in entities:\n            section = entity.get('section_name', 'unknown')\n            if section not in entities_by_section:\n                entities_by_section[section] = []\n            entities_by_section[section].append(entity)\n        \n        # Process each section's entities\n        # Progress tracking initialization\n        total_entities_to_process = sum(len(ents) for ents in entities_by_section.values())\n        entities_processed_count = 0\n        start_time = time.time()\n        print(f\"      \ud83c\udfaf Total entities to analyze: {total_entities_to_process}\")\n\n        for section_name, section_entities in entities_by_section.items():\n            if section_name not in sections:\n                continue\n                \n            section_text = sections[section_name]\n            section_relationships_count = 0  # Initialize counter for this section\n            print(f\"      \ud83d\udcd1 Processing {len(section_entities)} entities in '{section_name}'\")\n            \n            # Process entities in batches for efficiency\n            batch_size = CONFIG.get('llama', {}).get('batch_size', 5)  # Default 5 entities per batch\n            \n            # Filter out self-references first\n            filtered_entities = []\n            company_name = company_domain.replace('.com', '').replace('tx', '')\n            \n            for entity in section_entities:\n                if entity['entity_text'].lower() != company_name.lower():\n                    context = self._get_entity_context(entity, section_text)\n                    filtered_entities.append((entity, context, section_name))\n            \n            if not filtered_entities:\n                continue\n            \n            print(f\"         \ud83d\udd2c Processing {len(filtered_entities)} entities in batches of {batch_size}\")\n            \n            # Process entities in batches\n            for batch_start in range(0, len(filtered_entities), batch_size):\n                batch_end = min(batch_start + batch_size, len(filtered_entities))\n                entities_batch = filtered_entities[batch_start:batch_end]\n                \n                entities_processed_count += len(entities_batch)\n                \n                # Show progress\n                progress_pct = (entities_processed_count * 100) // total_entities_to_process\n                print(f\"         \u23f3 Batch {batch_start//batch_size + 1}: Processing entities {batch_start+1}-{batch_end} ({progress_pct}% complete)\")\n                \n                # Process batch with Llama\n                batch_relationships = self._analyze_relationship_batch(entities_batch)\n                \n                if batch_relationships:\n                    relationships.extend(batch_relationships)\n                    section_relationships_count += len(batch_relationships)\n                    self.stats['relationships_found'] += len(batch_relationships)\n                \n                self.stats['entities_processed'] += len(entities_batch)\n        \n        print(f\"   \u2705 Found {len(relationships)} relationships from {len(entities)} entities\")\n        return relationships\n    \n    def _get_entity_context(self, entity: Dict, section_text: str, window: int = None) -> str:\n        \"\"\"Get context around an entity\"\"\"\n        if window is None:\n            window = CONFIG[\"llama\"][\"entity_context_window\"]\n        start = max(0, entity.get('character_start', entity.get('char_start', 0)) - window)\n        end = min(len(section_text), entity.get('character_end', entity.get('char_end', 0)) + window)\n        return section_text[start:end]\n    \n    def _analyze_relationship_batch(self, entities_batch: List[Tuple[Dict, str, str]]) -> List[Dict]:\n        \"\"\"Analyze multiple entities in a single Llama call for efficiency\"\"\"\n        if not self.model or not self.tokenizer or not entities_batch:\n            return []\n        \n        try:\n            # Build batch prompt for multiple entities\n            prompt = f\"\"\"You are an expert at analyzing business relationships from SEC filings.\n\nAnalyze the business relationships for the following entities and provide detailed semantic extraction.\n\nENTITIES TO ANALYZE:\n\"\"\"\n            \n            # Add each entity to the prompt\n            for i, (entity, context, section_name) in enumerate(entities_batch, 1):\n                company_domain = entity.get(\"company_domain\", \"Unknown\")\n                prompt += f\"\"\"\nEntity {i}:\n- Company: {company_domain}\n- Entity: {entity[\"entity_text\"]} (Type: {entity.get(\"entity_category\", \"UNKNOWN\")})\n- Section: {section_name}\n- Context: {context[:400]}\n\n\"\"\"\n            \n            prompt += \"\"\"\nFor EACH entity, extract the following information and respond in JSON format:\n\n{\n  \"entity_1\": {\n    \"relationship_type\": \"<PARTNERSHIP|COMPETITOR|REGULATORY|CLINICAL_TRIAL|SUPPLIER|CUSTOMER|INVESTOR|ACQUISITION|LICENSING|RESEARCH|NONE>\",\n    \"semantic_action\": \"<initiated|expanded|milestone_reached|terminated|ongoing>\",\n    \"semantic_impact\": \"<positive|negative|neutral|mixed>\",\n    \"semantic_tags\": [\"tag1\", \"tag2\", \"tag3\"],\n    \"monetary_value\": \"<number_or_null>\",\n    \"percentage_value\": \"<number_or_null>\",\n    \"duration_months\": \"<number_or_null>\",\n    \"entity_count\": \"<number_or_null>\",\n    \"mentioned_time_period\": \"<Q1 2024|2025|next year|etc>\",\n    \"temporal_precision\": \"<EXACT_DATE|QUARTER|YEAR|RELATIVE>\",\n    \"confidence_level\": \"<high|medium|low>\",\n    \"summary\": \"<one_sentence_summary>\",\n    \"business_impact_summary\": \"<detailed_3_sentence_analysis>\",\n    \"regulatory_implications\": \"<regulatory_analysis_or_none>\",\n    \"competitive_implications\": \"<competitive_analysis_or_none>\"\n  },\n  \"entity_2\": { ... },\n  ...\n}\n\nEXTRACTION GUIDELINES:\n- monetary_value: Extract dollar amounts as numbers (e.g., \"M\" \u2192 50000000)\n- percentage_value: Extract percentages as numbers (e.g., \"45%\" \u2192 45.0)\n- duration_months: Convert time periods to months (e.g., \"3 years\" \u2192 36)\n- entity_count: Extract numerical counts (e.g., \"three trials\" \u2192 3)\n- semantic_tags: Key biotech/business terms like [\"oncology\", \"phase_2\", \"FDA\", \"partnership\"]\n- temporal_precision: How precise is the time reference\n- Set fields to null if not mentioned in context\n\nReturn valid JSON only, no additional text.\"\"\"\n\n            # Create messages for chat format\n            messages = [\n                {\"role\": \"system\", \"content\": \"You are an expert at analyzing business relationships from SEC filings. Always respond with valid JSON in the exact format requested.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n            \n            # Apply chat template\n            inputs = self.tokenizer.apply_chat_template(\n                messages,\n                return_tensors=\"pt\",\n                tokenize=True\n            )\n            \n            # Generate response with expanded token limit\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    inputs,\n                    max_new_tokens=2000,  # Increased from 50 to 2000\n                    temperature=CONFIG[\"llama\"][\"temperature\"],\n                    do_sample=True,\n                    pad_token_id=self.tokenizer.eos_token_id\n                )\n            \n            # Decode response\n            llama_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n            \n            # Extract just the assistant's response\n            if \"assistant\" in llama_response:\n                llama_response = llama_response.split(\"assistant\")[-1].strip()\n            \n            self.stats['llama_calls'] += 1\n            \n            # Parse JSON response\n            return self._parse_batch_llama_response(llama_response, entities_batch)\n            \n        except Exception as e:\n            print(f\"         \u26a0\ufe0f Batch Llama analysis failed: {e}\")\n            return []\n\n    def _parse_batch_llama_response(self, response: str, entities_batch: List) -> List[Dict]:\n        \"\"\"Parse JSON response from batch Llama analysis\"\"\"\n        try:\n            import json\n            \n            # Clean up response to extract JSON\n            json_start = response.find('{')\n            json_end = response.rfind('}') + 1\n            if json_start >= 0 and json_end > json_start:\n                json_str = response[json_start:json_end]\n                \n                # Parse JSON\n                batch_results = json.loads(json_str)\n                \n                relationships = []\n                for i, (entity, context, section_name) in enumerate(entities_batch, 1):\n                    entity_key = f\"entity_{i}\"\n                    if entity_key in batch_results:\n                        result = batch_results[entity_key]\n                        \n                        # Skip if no relationship found\n                        if result.get('relationship_type') == 'NONE':\n                            continue\n                        \n                        # Build relationship record with rich metadata\n                        relationship = {\n                            'company_domain': entity.get('company_domain', ''),\n                            'entity_text': entity['entity_text'],\n                            'entity_type': entity.get('entity_category', 'UNKNOWN'),\n                            'entity_id': entity.get('extraction_id', str(uuid.uuid4())),\n                            'relationship_type': result.get('relationship_type', 'UNKNOWN'),\n                            'semantic_action': result.get('semantic_action', 'ongoing'),\n                            'semantic_impact': result.get('semantic_impact', 'neutral'),\n                            'semantic_tags': result.get('semantic_tags', []),\n                            'monetary_value': result.get('monetary_value'),\n                            'percentage_value': result.get('percentage_value'),\n                            'duration_months': result.get('duration_months'),\n                            'entity_count': result.get('entity_count'),\n                            'mentioned_time_period': result.get('mentioned_time_period', ''),\n                            'temporal_precision': result.get('temporal_precision', 'RELATIVE'),\n                            'business_impact': result.get('semantic_impact', 'neutral'),\n                            'confidence_level': result.get('confidence_level', 'medium'),\n                            'summary': result.get('summary', ''),\n                            'business_impact_summary': result.get('business_impact_summary', ''),\n                            'regulatory_implications': result.get('regulatory_implications', ''),\n                            'competitive_implications': result.get('competitive_implications', ''),\n                            'section_name': section_name,\n                            'context_used': context[:500],\n                            'llama_response': response[:1000],\n                            'extraction_timestamp': datetime.now().isoformat()\n                        }\n                        \n                        relationships.append(relationship)\n                \n                return relationships\n                \n        except Exception as e:\n            print(f\"Failed to parse batch Llama response: {e}\")\n            return []\n        \n        return []\n    def _parse_llama_response(self, response: str) -> Optional[Dict]:\n        \"\"\"Parse structured response from Llama\"\"\"\n        try:\n            lines = response.strip().split('\\n')\n            parsed = {}\n            \n            for line in lines:\n                if ':' in line:\n                    key, value = line.split(':', 1)\n                    key = key.strip().upper()\n                    value = value.strip()\n                    \n                    if key == 'TYPE':\n                        parsed['type'] = value\n                    elif key == 'DIRECTION':\n                        parsed['direction'] = value\n                    elif key == 'IMPACT':\n                        parsed['impact'] = value.lower()\n                    elif key == 'CONFIDENCE':\n                        parsed['confidence'] = value.lower()\n                    elif key == 'SUMMARY':\n                        parsed['summary'] = value\n            \n            # Validate required fields\n            required = ['type', 'direction', 'impact', 'confidence', 'summary']\n            if all(field in parsed for field in required):\n                return parsed\n            \n            return None\n            \n        except Exception:\n            return None\n\n# ================================================================================\n# SOPHISTICATED SEMANTIC RELATIONSHIP STORAGE\n# ================================================================================\n\n\nclass SemanticRelationshipStorage:\n    \"\"\"Advanced storage system using sophisticated semantic relationship schema\"\"\"\n    \n    def __init__(self, db_config: Dict):\n        self.db_config = db_config\n        self.storage_stats = {\n            'buckets_created': 0,\n            'buckets_updated': 0,\n            'events_stored': 0,\n            'sessions_tracked': 0,\n            'transactions_completed': 0,\n            'transactions_failed': 0\n        }\n    \n    def store_relationships_with_buckets(self, relationships: List[Dict], filing_ref: str, session_id: str = None) -> bool:\n        \"\"\"Store relationships using bucket aggregation pattern\"\"\"\n        if not relationships:\n            return True\n        \n        conn = None\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            conn.autocommit = False\n            \n            print(f\"   \ud83d\udcbe Storing {len(relationships)} relationships using sophisticated schema...\")\n            \n            for relationship in relationships:\n                # Step 1: Find or create bucket for this company-entity-type combination\n                bucket_id = self._find_or_create_bucket(cursor, relationship)\n                \n                # Step 2: Store semantic event with rich metadata\n                self._store_semantic_event(cursor, relationship, bucket_id, filing_ref, session_id)\n                \n                # Step 3: Update bucket aggregation\n                self._update_bucket_aggregation(cursor, bucket_id, relationship)\n            \n            conn.commit()\n            self.storage_stats['transactions_completed'] += 1\n            self.storage_stats['events_stored'] += len(relationships)\n            \n            print(f\"      \u2705 Stored {len(relationships)} relationship events\")\n            return True\n            \n        except Exception as e:\n            if conn:\n                conn.rollback()\n            self.storage_stats['transactions_failed'] += 1\n            print(f\"      \u274c Failed to store relationships: {e}\")\n            return False\n        finally:\n            if conn:\n                conn.close()\n    \n    def _find_or_create_bucket(self, cursor, relationship: Dict) -> str:\n        \"\"\"Find existing bucket or create new one for company-entity-type combination\"\"\"\n        company_domain = relationship['company_domain']\n        entity_name = relationship['entity_text']\n        relationship_type = relationship['relationship_type']\n        \n        # Try to find existing bucket\n        cursor.execute(\"\"\"\n            SELECT bucket_id FROM system_uno.relationship_buckets\n            WHERE company_domain = %s AND entity_name = %s AND relationship_type = %s\n        \"\"\", (company_domain, entity_name, relationship_type))\n        \n        result = cursor.fetchone()\n        if result:\n            return result[0]  # Return existing bucket_id\n        \n        # Create new bucket\n        cursor.execute(\"\"\"\n            INSERT INTO system_uno.relationship_buckets\n            (company_domain, entity_name, relationship_type, master_semantic_summary, \n             first_mentioned_date, last_mentioned_date, total_mentions, is_active)\n            VALUES (%s, %s, %s, %s, CURRENT_DATE, CURRENT_DATE, 1, TRUE)\n            RETURNING bucket_id\n        \"\"\", (company_domain, entity_name, relationship_type, relationship.get('summary', '')))\n        \n        bucket_id = cursor.fetchone()[0]\n        self.storage_stats['buckets_created'] += 1\n        return bucket_id\n    \n    def _store_semantic_event(self, cursor, relationship: Dict, bucket_id: str, filing_ref: str, session_id: str = None):\n        \"\"\"Store individual relationship event with semantic metadata\"\"\"\n        cursor.execute(\"\"\"\n            INSERT INTO system_uno.relationship_semantic_events\n            (bucket_id, source_entity_id, sec_filing_ref, filing_date, filing_type, section_name,\n             semantic_summary, semantic_action, semantic_impact, semantic_tags,\n             monetary_value, percentage_value, duration_months, entity_count,\n             mentioned_time_period, temporal_precision, \n             business_impact_summary, regulatory_implications, competitive_implications,\n             original_context_snippet, confidence_score, llama_prompt_version, event_timestamp)\n            VALUES (%s, %s, %s, CURRENT_DATE, '10-K', %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, '2.0', CURRENT_TIMESTAMP)\n        \"\"\", (\n            bucket_id,\n            relationship.get('entity_id'),\n            filing_ref,\n            relationship.get('section_name', ''),\n            relationship.get('summary', ''),\n            relationship.get('semantic_action', 'ongoing'),\n            relationship.get('semantic_impact', 'neutral'),\n            relationship.get('semantic_tags', []),\n            relationship.get('monetary_value'),\n            relationship.get('percentage_value'),\n            relationship.get('duration_months'),\n            relationship.get('entity_count'),\n            relationship.get('mentioned_time_period', ''),\n            relationship.get('temporal_precision', 'RELATIVE'),\n            relationship.get('business_impact_summary', ''),\n            relationship.get('regulatory_implications', ''),\n            relationship.get('competitive_implications', ''),\n            relationship.get('context_used', '')[:500],  # Limit to 500 chars\n            float(relationship.get('confidence_level', 0.5)) if isinstance(relationship.get('confidence_level'), (int, float)) else 0.5,\n        ))\n    \n    def _update_bucket_aggregation(self, cursor, bucket_id: str, relationship: Dict):\n        \"\"\"Update bucket aggregated metrics\"\"\"\n        cursor.execute(\"\"\"\n            UPDATE system_uno.relationship_buckets\n            SET \n                last_mentioned_date = CURRENT_DATE,\n                total_mentions = total_mentions + 1,\n                updated_at = CURRENT_TIMESTAMP,\n                master_semantic_summary = %s\n            WHERE bucket_id = %s\n        \"\"\", (relationship.get('summary', ''), bucket_id))\n        \n        self.storage_stats['buckets_updated'] += 1\n    \n    def create_analysis_session(self, company_domain: str, filing_batch: List[str]) -> str:\n        \"\"\"Create analysis session for tracking Llama processing\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            cursor.execute(\"\"\"\n                INSERT INTO system_uno.semantic_analysis_sessions\n                (company_domain, filing_batch, primary_prompt_version, session_start)\n                VALUES (%s, %s, '1.0', CURRENT_TIMESTAMP)\n                RETURNING session_id\n            \"\"\", (company_domain, filing_batch))\n            \n            session_id = cursor.fetchone()[0]\n            conn.commit()\n            cursor.close()\n            conn.close()\n            \n            self.storage_stats['sessions_tracked'] += 1\n            return session_id\n            \n        except Exception as e:\n            print(f\"Failed to create analysis session: {e}\")\n            return None\n    \n    def get_storage_stats(self) -> Dict:\n        \"\"\"Get sophisticated storage statistics\"\"\"\n        return self.storage_stats.copy()\n\n# ================================================================================\n# REFACTORED MAIN PIPELINE WITH IN-MEMORY PROCESSING\n# ================================================================================\n\n# Initialize components\nsemantic_storage = SemanticRelationshipStorage(CONFIG[\"database\"])\nrelationship_extractor = RelationshipExtractor()\n\ndef process_filing_with_pipeline(filing_data: Dict) -> Dict:\n    \"\"\"Process filing with in-memory entity and relationship extraction\"\"\"\n    try:\n        start_time = time.time()\n        \n        # Step 1: Extract sections (Cell 2 function)\n        print(f\"\\n\ud83d\udcc4 Processing {filing_data['filing_type']} for {filing_data['company_domain']}\")\n        section_result = process_sec_filing_with_sections(filing_data)\n        \n        if section_result['processing_status'] != 'success':\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': section_result.get('error', 'Section extraction failed'),\n                'processing_time': time.time() - start_time\n            }\n        \n        # Keep sections in memory for context retrieval\n        sections_dict = section_result['sections']\n        \n        # Step 2: Extract entities (Cell 3 function) - keep in memory\n        entities = entity_pipeline.process_sec_filing_sections(section_result)\n        \n        if not entities:\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': 'No entities extracted',\n                'processing_time': time.time() - start_time\n            }\n        \n        print(f\"   \ud83d\udd0d Extracted {len(entities)} entities\")\n\n        # Debug step removed\n        # debug_entities call removed - function not defined\n\n        # Step 3: Store entities IMMEDIATELY before Llama processing\n        filing_ref = f\"SEC_{filing_data.get('id')}\"\n\n\n\n\n\n        # Step 4: Extract relationships using in-memory entities and sections (LONG PROCESS)\n        print(f\"   \ud83e\udd16 Starting Llama 3.1 relationship extraction (this may take several minutes)...\")\n        \n        # (relationships extraction continues below...)\n        relationships = relationship_extractor.extract_company_relationships(\n            entities, \n            sections_dict,\n            filing_data['company_domain']\n        )\n        \n        # Step 5: Store relationships using sophisticated schema\n        filing_ref = f\"SEC_{filing_data.get('id')}\"\n        if relationships:\n            # Create analysis session for tracking\n            session_id = semantic_storage.create_analysis_session(\n                filing_data['company_domain'],\n                [filing_ref]\n            )\n            # Store relationships with bucket aggregation\n            relationship_storage_success = semantic_storage.store_relationships_with_buckets(\n                relationships, filing_ref, session_id\n            )\n        else:\n            relationship_storage_success = True\n            print(f\"   \u2139\ufe0f No relationships found to store\")\n        # Step 6: Verify storage\n\n        \n        processing_time = time.time() - start_time\n        \n        # Calculate overall storage success\n        storage_success = True  # Storage removed - processing successful\n        \n        return {\n            'success': storage_success,\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain'),\n            'filing_type': filing_data.get('filing_type'),\n            'sections_processed': len(sections_dict),\n            'entities_extracted': len(entities),\n            'relationships_found': len(relationships),\n            'entities_stored': len(entities),\n            'relationships_stored': len(relationships),\n            'processing_time': round(processing_time, 2),\n            'verification': {'entities': {'total': len(entities)}, 'relationships': {'total': len(relationships)}},\n            'sample_entities': entities[:3],\n            'sample_relationships': relationships[:3]\n        }\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'filing_id': filing_data.get('id'),\n            'error': str(e),\n            'processing_time': time.time() - start_time\n        }\n\ndef process_filings_batch(limit: int = 3) -> Dict:\n    \"\"\"Process multiple filings with complete in-memory pipeline\"\"\"\n    print(f\"\\n\ud83d\ude80 Processing batch of {limit} SEC filings with in-memory pipeline...\")\n    \n    batch_start = time.time()\n    \n    # Get unprocessed filings\n    filings = get_unprocessed_filings(limit)\n    \n    if not filings:\n        return {'success': False, 'message': 'No filings to process'}\n    \n    print(f\"\ud83d\udcca Found {len(filings)} filings to process\")\n    \n    # Process each filing\n    results = []\n    successful = 0\n    total_entities = 0\n    total_relationships = 0\n    \n    for i, filing in enumerate(filings, 1):\n        print(f\"\\n[{i}/{len(filings)}] Processing {filing['filing_type']} for {filing['company_domain']}\")\n        \n        result = process_filing_with_pipeline(filing)\n        results.append(result)\n        \n        if result['success']:\n            successful += 1\n            total_entities += result.get('entities_extracted', 0)\n            total_relationships += result.get('relationships_found', 0)\n            \n            print(f\"   \u2705 Success: {result['entities_extracted']} entities, {result['relationships_found']} relationships\")\n            print(f\"   \u23f1\ufe0f Processing time: {result['processing_time']}s\")\n            \n            # Show sample relationships\n            for rel in result.get('sample_relationships', [])[:2]:\n                print(f\"      \u2022 {rel['entity_text']} \u2192 {rel['relationship_type']} ({rel['business_impact']})\")\n        else:\n            print(f\"   \u274c Failed: {result.get('error', 'Unknown error')}\")\n        \n        # Brief pause between filings\n        if i < len(filings):\n            time.sleep(1)\n    \n    batch_time = time.time() - batch_start\n    \n    # Update pipeline statistics\n    entity_pipeline.pipeline_stats['documents_processed'] += successful\n    entity_pipeline.pipeline_stats['total_entities_extracted'] += total_entities\n\n    \n    return {\n        'success': successful > 0,\n        'filings_processed': len(filings),\n        'successful_filings': successful,\n        'failed_filings': len(filings) - successful,\n        'total_entities_extracted': total_entities,\n        'total_relationships_found': total_relationships,\n        'batch_processing_time': round(batch_time, 2),\n        'avg_time_per_filing': round(batch_time / len(filings), 2) if filings else 0,\n        'results': results\n    }\n\n# ================================================================================\n# QUICK ACCESS FUNCTIONS\n# ================================================================================\n\ndef test_pipeline(company_domain: str = None):\n    \"\"\"Test the pipeline with a single filing\"\"\"\n    print(\"\\n\ud83e\uddea Testing in-memory pipeline...\")\n    \n    # Get one filing\n    if company_domain:\n        # Modify get_unprocessed_filings to accept company filter\n        # For now, just get any filing\n        filings = get_unprocessed_filings(limit=1)\n    else:\n        filings = get_unprocessed_filings(limit=1)\n    \n    if not filings:\n        print(\"\u274c No test filings available\")\n        return None\n    \n    result = process_filing_with_pipeline(filings[0])\n    \n    if result['success']:\n        print(f\"\\n\u2705 Pipeline test successful!\")\n        print(f\"   \ud83d\udcca Sections: {result['sections_processed']}\")\n        print(f\"   \ud83d\udd0d Entities: {result['entities_extracted']}\")\n        print(f\"   \ud83d\udd17 Relationships: {result['relationships_found']}\")\n        print(f\"   \ud83d\udcbe Stored: {result['entities_stored']} entities, {result['relationships_stored']} relationships\")\n        print(f\"   \u23f1\ufe0f Time: {result['processing_time']}s\")\n    else:\n        print(f\"\\n\u274c Pipeline test failed: {result.get('error')}\")\n    \n    return result\n\nprint(\"\\n\u2705 In-Memory Pipeline Components Ready!\")\nprint(\"   \ud83c\udfaf RelationshipExtractor with Llama 3.1\")\nprint(\"   \ud83d\udcbe Atomic storage for entities + relationships\")\nprint(\"   \ud83d\ude80 In-memory processing (no DB round-trips)\")\nprint(\"   \ud83d\udcca Usage: batch_results = process_filings_batch(limit=5)\")\nprint(\"   \ud83e\uddea Test: test_result = test_pipeline()\")\n\ndef generate_pipeline_analytics_report() -> None:\n    \"\"\"Generate comprehensive analytics report for the pipeline\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"\ud83d\udcca ENTITYEXTRACTIONPIPELINE ANALYTICS DASHBOARD\")\n    print(\"=\"*80)\n    \n    # Database overview with enhanced metrics\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Enhanced database statistics\n        cursor.execute(\"\"\"\n            SELECT \n                COUNT(*) as total_entities,\n                COUNT(DISTINCT company_domain) as companies,\n                COUNT(DISTINCT sec_filing_ref) as filings,\n                COUNT(DISTINCT entity_category) as entity_types,\n                AVG(confidence_score) as avg_confidence,\n                COUNT(*) FILTER (WHERE is_merged = true) as merged_entities,\n                COUNT(DISTINCT primary_model) as active_models,\n                COUNT(DISTINCT section_name) as sections_processed,\n                COUNT(*) FILTER (WHERE section_name IS NOT NULL AND section_name != '') as entities_with_sections,\n                MAX(extraction_timestamp) as last_extraction\n            FROM system_uno.sec_entities_raw\n            WHERE data_source = 'sec_filings'\n        \"\"\")\n        \n        db_overview = cursor.fetchone()\n        \n        if db_overview and db_overview[0] > 0:\n            total_entities = db_overview[0]\n            entities_with_sections = db_overview[8]\n            section_success_rate = (entities_with_sections / total_entities * 100) if total_entities > 0 else 0\n            \n            print(f\"\\n\ud83d\udcc8 DATABASE OVERVIEW:\")\n            print(f\"   Total Entities Extracted: {db_overview[0]:,}\")\n            print(f\"   Companies Processed: {db_overview[1]:,}\")\n            print(f\"   SEC Filings Analyzed: {db_overview[2]:,}\")\n            print(f\"   Entity Categories Found: {db_overview[3]:,}\")\n            print(f\"   Average Confidence Score: {db_overview[4]:.3f}\")\n            print(f\"   Multi-Model Entities: {db_overview[5]:,} ({db_overview[5]/db_overview[0]*100:.1f}%)\")\n            print(f\"   Active Models: {db_overview[6]:,}\")\n            print(f\"   Unique Sections Found: {db_overview[7]:,}\")\n            print(f\"   \ud83c\udfaf SECTION SUCCESS RATE: {entities_with_sections:,}/{total_entities:,} ({section_success_rate:.1f}%)\")\n            print(f\"   Last Extraction: {db_overview[9] or 'Never'}\")\n            \n            # Alert if section success rate is low\n            if section_success_rate < 90 and total_entities > 10:\n                print(f\"   \ud83d\udea8 WARNING: Section success rate is {section_success_rate:.1f}% - Pipeline routing issue!\")\n            elif section_success_rate >= 90:\n                print(f\"   \u2705 EXCELLENT: Section success rate is {section_success_rate:.1f}% - Pipeline working correctly!\")\n        else:\n            print(f\"\\n\ud83d\udcc8 DATABASE OVERVIEW: No entities found - database is clean for testing\")\n        \n        cursor.close()\n        conn.close()\n                \n    except Exception as e:\n        print(f\"   \u274c Could not retrieve analytics: {e}\")\n    \n    # Pipeline statistics\n    try:\n        pipeline_stats = entity_pipeline.get_pipeline_statistics()\n        \n        print(f\"\\n\ud83d\udd27 PIPELINE STATISTICS:\")\n        print(f\"   Documents Processed: {pipeline_stats['pipeline_stats']['documents_processed']:,}\")\n        print(f\"   Total Entities Found: {pipeline_stats['pipeline_stats']['total_entities_extracted']:,}\")\n        print(f\"   Processing Time: {pipeline_stats['pipeline_stats']['processing_time_total']:.2f}s\")\n        print(f\"   Device: {pipeline_stats['device']}\")\n        print(f\"   Loaded Models: {', '.join(pipeline_stats['loaded_models'])}\")\n        print(f\"   Supported Sources: {', '.join(pipeline_stats['supported_data_sources'])}\")\n        \n        # Individual model statistics\n        print(f\"\\n\ud83d\udcca INDIVIDUAL MODEL PERFORMANCE:\")\n        for model_name, stats in pipeline_stats['model_stats'].items():\n            if stats['texts_processed'] > 0:\n                entities_per_text = stats['entities_found'] / stats['texts_processed']\n                avg_time = stats['processing_time'] / stats['texts_processed']\n                print(f\"   {model_name:>12}: {stats['texts_processed']:>4} texts | {stats['entities_found']:>5} entities | {entities_per_text:>4.1f} avg/text | {avg_time:>4.2f}s avg\")\n        \n        # Storage statistics\n\n\n            print(f\"\\n\ud83d\udcbe STORAGE STATISTICS:\")\n            print(f\"   Entities Stored: {storage_stats['total_entities_stored']:,}\")\n            print(f\"   Filings Processed: {storage_stats['filings_processed']:,}\")\n            print(f\"   Merged Entities: {storage_stats['merged_entities']:,}\")\n            print(f\"   Single-Model Entities: {storage_stats['single_model_entities']:,}\")\n            print(f\"   Failed Inserts: {storage_stats['failed_inserts']:,}\")\n            \n            merge_rate = (storage_stats['merged_entities'] / storage_stats['total_entities_stored'] * 100) if storage_stats['total_entities_stored'] > 0 else 0\n            print(f\"   Multi-Model Detection Rate: {merge_rate:.1f}%\")\n    \n    except Exception as e:\n        print(f\"\\n\ud83d\udd27 Pipeline statistics unavailable: {e}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"\u2705 EntityExtractionPipeline Analytics Complete!\")\n    print(\"=\"*80)\n"
      ],
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Cell 5: Main Processing Pipeline with Relationship Extraction\n\nprint(\"=\"*80)\nprint(\"\ud83d\ude80 STARTING SEC FILING PROCESSING PIPELINE\")\nprint(\"=\"*80)\n\n# Configure processing parameters\n# CONFIG[\"processing\"][\"filing_batch_size\"] = 3  # REMOVED: Use CONFIG[\"processing\"][\"filing_batch_size\"] instead  # Number of filings to process in this run\n# CONFIG[\"processing\"][\"enable_relationships\"] = True  # REMOVED: Use CONFIG[\"processing\"][\"enable_relationships\"] instead  # Set to False to skip relationship extraction\n\n# Llama 3.1-8B is loaded locally, no API key needed\nprint(\"\ud83d\udcdd Relationship extraction enabled with local Llama 3.1-8B\")\n\n# # Check if we have a Groq API key for relationship extraction\nprint(\"   \u2139\ufe0f Using local Llama 3.1-8B for relationship extraction\")\n#     print(\"   To enable relationships, add GROQ_API_KEY to Kaggle secrets\")\n\n# Check for available unprocessed filings\nprint(\"\\n\ud83d\udcca Checking for unprocessed filings...\")\navailable_filings = get_unprocessed_filings(limit=CONFIG[\"processing\"][\"filing_query_limit\"])\nprint(f\"   Found {len(available_filings)} unprocessed filings\")\n\nif available_filings:\n    print(\"\\n\ud83d\udccb Available filings to process:\")\n    for i, filing in enumerate(available_filings[:5], 1):\n        print(f\"   {i}. {filing['company_domain']} - {filing['filing_type']} ({filing['filing_date']})\")\n    \n    # Process the batch\n    print(f\"\\n\ud83d\udd04 Processing {min(CONFIG[\"processing\"][\"filing_batch_size\"], len(available_filings))} filings...\")\n    print(\"-\"*60)\n    \n    # Run the pipeline\n    batch_results = process_filings_batch(limit=CONFIG[\"processing\"][\"filing_batch_size\"])\n    \n    # Display results summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"\ud83d\udcca PROCESSING SUMMARY\")\n    print(\"=\"*80)\n    \n    if batch_results['success']:\n        print(f\"\u2705 Successfully processed {batch_results['successful_filings']}/{batch_results['filings_processed']} filings\")\n        print(f\"   \u2022 Total entities extracted: {batch_results['total_entities_extracted']:,}\")\n        print(f\"   \u2022 Total relationships found: {batch_results['total_relationships_found']:,}\")\n        print(f\"   \u2022 Total processing time: {batch_results['batch_processing_time']:.1f}s\")\n        print(f\"   \u2022 Average time per filing: {batch_results['avg_time_per_filing']:.1f}s\")\n        \n        # Show detailed results for each filing\n        print(f\"\\n\ud83d\udcc8 Detailed Results:\")\n        for i, result in enumerate(batch_results['results'], 1):\n            if result['success']:\n                print(f\"\\n   Filing {i}: {result['company_domain']} - {result['filing_type']}\")\n                print(f\"      \u2713 Sections: {result['sections_processed']}\")\n                print(f\"      \u2713 Entities: {result['entities_extracted']}\")\n                print(f\"      \u2713 Relationships: {result['relationships_found']}\")\n                print(f\"      \u2713 Time: {result['processing_time']:.1f}s\")\n            else:\n                print(f\"\\n   Filing {i}: FAILED - {result.get('error', 'Unknown error')}\")\n        \n        # Show pipeline statistics\n        print(f\"\\n\ud83d\udcca Pipeline Statistics:\")\n        print(f\"   \u2022 Documents processed (total): {entity_pipeline.pipeline_stats['documents_processed']}\")\n        print(f\"   \u2022 Entities extracted (total): {entity_pipeline.pipeline_stats['total_entities_extracted']}\")\n        print(f\"   \u2022 Storage transactions: {pipeline_storage.storage_stats['transactions_completed']} successful, {pipeline_storage.storage_stats['transactions_failed']} failed\")\n        print(f\"   \u2022 Merged entities: {pipeline_storage.storage_stats['merged_entities']}\")\n        print(f\"   \u2022 Single-model entities: {pipeline_storage.storage_stats['single_model_entities']}\")\n        \n    else:\n        print(f\"\u274c Processing failed: {batch_results.get('message', 'Unknown error')}\")\n    \n    # Generate analytics report\n    print(\"\\n\" + \"=\"*80)\n    generate_pipeline_analytics_report()\n    \nelse:\n    print(\"\\n\u26a0\ufe0f No unprocessed filings found in raw_data.sec_filings\")\n    print(\"   All available filings have already been processed\")\n    print(\"\\n\ud83d\udca1 To add new filings:\")\n    print(\"   1. Insert new records into raw_data.sec_filings with accession_number\")\n    print(\"   2. Make sure the accession_number is valid (20 characters)\")\n    print(\"   3. Run this cell again to process them\")\n\nprint(\"\\n\u2705 Pipeline execution complete!\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-13T18:14:18.239084Z",
          "iopub.execute_input": "2025-09-13T18:14:18.239404Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Cell 6: Execute Pipeline\n\nprint(f\"\\n\ud83c\udfaf PRODUCTION COMMANDS:\")\nprint(f\"   \u2022 Process new filings: batch_results = process_filings_batch(limit=5)\")\nprint(f\"   \u2022 Check results:       generate_pipeline_analytics_report()\")\nprint(f\"   \u2022 View statistics:     context_retriever.get_retrieval_statistics()\")\n\nprint(f\"\\n\u2705 EntityExtractionPipeline Production Interface Ready!\")\nprint(f\"\ud83d\udd27 SINGLE ENTRY POINT: process_filings_batch() - ensures all extractions use section-based pipeline\")\nprint(f\"\ud83d\udcca Database cleared - ready for fresh testing with guaranteed section extraction!\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Cell 6: TEST ENTITY STORAGE - Quick Verification Without Full Pipeline\n# This cell tests ONLY the storage functionality with mock entities\n\nprint(\"=\"*80)\nprint(\"\ud83e\uddea ENTITY STORAGE TEST - Verify Storage Works Without 90-Minute Run\")\nprint(\"=\"*80)\n\n# Create mock entities that match the expected structure\nprint(\"\\n\ud83d\udcdd Creating mock entities for storage test...\")\n\n# Create test entities with ALL required fields\ntest_entities = [\n    {\n        'extraction_id': str(uuid.uuid4()),\n        'company_domain': 'test.com',\n        'entity_text': 'Test Company Inc',\n        'entity_type': 'ORGANIZATION',\n        'entity_category': 'ORGANIZATION',  # Some code uses entity_category\n        'confidence_score': 0.95,\n        'char_start': 100,\n        'char_end': 117,\n        'character_start': 100,  # Some code uses character_start\n        'character_end': 117,\n        'surrounding_text': 'This is Test Company Inc in the context',\n        'models_detected': ['bert_base', 'roberta'],\n        'all_confidences': {'bert_base': 0.94, 'roberta': 0.96},\n        'primary_model': 'roberta',\n        'entity_variations': {'bert_base': 'Test Company Inc', 'roberta': 'Test Company Inc.'},\n        'is_merged': True,\n        'section_name': 'test_section',\n        'data_source': 'sec_filings',\n        'extraction_timestamp': datetime.now().isoformat(),\n        'original_label': 'ORG',\n        'model_source': 'roberta',  # Fallback field\n        'quality_score': 0.92,\n        'consensus_count': 2,\n        'detecting_models': ['bert_base', 'roberta'],\n        'consensus_score': 0.95,\n        'filing_id': 999999,\n        'sec_filing_ref': 'SEC_999999'\n    },\n    {\n        'extraction_id': str(uuid.uuid4()),\n        'company_domain': 'test.com',\n        'entity_text': 'John Smith',\n        'entity_type': 'PERSON',\n        'entity_category': 'PERSON',\n        'confidence_score': 0.88,\n        'char_start': 200,\n        'char_end': 210,\n        'character_start': 200,\n        'character_end': 210,\n        'surrounding_text': 'CEO John Smith announced',\n        'models_detected': ['bert_base'],\n        'all_confidences': {'bert_base': 0.88},\n        'primary_model': 'bert_base',\n        'entity_variations': {'bert_base': 'John Smith'},\n        'is_merged': False,\n        'section_name': 'test_section',\n        'data_source': 'sec_filings',\n        'extraction_timestamp': datetime.now().isoformat(),\n        'original_label': 'PER',\n        'model_source': 'bert_base',\n        'quality_score': 0.85,\n        'consensus_count': 1,\n        'detecting_models': ['bert_base'],\n        'consensus_score': 0.88,\n        'filing_id': 999999,\n        'sec_filing_ref': 'SEC_999999'\n    },\n    {\n        'extraction_id': str(uuid.uuid4()),\n        'company_domain': 'test.com',\n        'entity_text': 'FDA',\n        'entity_type': 'ORGANIZATION',\n        'entity_category': 'ORGANIZATION',\n        'confidence_score': 0.99,\n        'char_start': 300,\n        'char_end': 303,\n        'character_start': 300,\n        'character_end': 303,\n        'surrounding_text': 'approved by the FDA for clinical',\n        'models_detected': ['bert_base', 'roberta', 'biobert'],\n        'all_confidences': {'bert_base': 0.98, 'roberta': 0.99, 'biobert': 1.0},\n        'primary_model': 'biobert',\n        'entity_variations': {'bert_base': 'FDA', 'roberta': 'FDA', 'biobert': 'FDA'},\n        'is_merged': True,\n        'section_name': 'test_section',\n        'data_source': 'sec_filings',\n        'extraction_timestamp': datetime.now().isoformat(),\n        'original_label': 'ORG',\n        'model_source': 'biobert',\n        'quality_score': 0.99,\n        'consensus_count': 3,\n        'detecting_models': ['bert_base', 'roberta', 'biobert'],\n        'consensus_score': 0.99,\n        'filing_id': 999999,\n        'sec_filing_ref': 'SEC_999999'\n    }\n]\n\nprint(f\"\u2705 Created {len(test_entities)} test entities\")\nprint(\"\\n\ud83d\udcca Test entity details:\")\nfor i, entity in enumerate(test_entities, 1):\n    print(f\"   {i}. {entity['entity_text']} ({entity['entity_type']}) - confidence: {entity['confidence_score']:.2f}\")\n\n# Test storage\nprint(\"\\n\ud83d\udcbe Testing entity storage...\")\nprint(\"   Using PipelineEntityStorage from Cell 4...\")\n\ntry:\n    # Initialize storage (assuming NEON_CONFIG is available from Cell 0)\n    test_storage = PipelineEntityStorage(NEON_CONFIG)\n    print(\"   \u2705 Storage initialized successfully\")\n    \n    # Attempt to store test entities\n    filing_ref = \"SEC_TEST_999999\"\n    print(f\"\\n   \ud83d\udce4 Attempting to store {len(test_entities)} entities...\")\n    \n    success = test_storage.store_entities(test_entities, filing_ref)\n    \n    if success:\n        print(\"   \u2705 STORAGE SUCCESSFUL! Entities stored to database\")\n        \n        # Verify by querying the database\n        print(\"\\n   \ud83d\udd0d Verifying stored entities...\")\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        cursor.execute(\"\"\"\n            SELECT COUNT(*), \n                   COUNT(DISTINCT entity_text),\n                   AVG(confidence_score)::numeric(4,3),\n                   AVG(quality_score)::numeric(4,3),\n                   AVG(consensus_count)\n            FROM system_uno.sec_entities_raw\n            WHERE sec_filing_ref = %s\n        \"\"\", (filing_ref,))\n        \n        result = cursor.fetchone()\n        if result and result[0] > 0:\n            print(f\"   \u2705 VERIFIED: {result[0]} entities found in database\")\n            print(f\"      \u2022 Unique entities: {result[1]}\")\n            print(f\"      \u2022 Avg confidence: {result[2]}\")\n            print(f\"      \u2022 Avg quality: {result[3]}\")\n            print(f\"      \u2022 Avg consensus: {result[4]}\")\n        else:\n            print(\"   \u274c WARNING: Entities not found in database after storage\")\n        \n        # Clean up test data\n        print(\"\\n   \ud83e\uddf9 Cleaning up test data...\")\n        cursor.execute(\"DELETE FROM system_uno.sec_entities_raw WHERE sec_filing_ref = %s\", (filing_ref,))\n        conn.commit()\n        deleted = cursor.rowcount\n        print(f\"   \u2705 Cleaned up {deleted} test entities\")\n        \n        cursor.close()\n        conn.close()\n        \n    else:\n        print(\"   \u274c STORAGE FAILED! Check error messages above\")\n        print(\"   \u26a0\ufe0f  The entity storage is NOT working correctly\")\n        \nexcept Exception as e:\n    print(f\"   \u274c ERROR during storage test: {e}\")\n    print(\"   \u26a0\ufe0f  This error needs to be fixed before running the full pipeline\")\n    traceback.print_exc()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"\ud83c\udfc1 STORAGE TEST COMPLETE\")\nprint(\"=\"*80)\nprint(\"\\n\ud83d\udcdd Summary:\")\nprint(\"   \u2022 If you see '\u2705 STORAGE SUCCESSFUL', the storage is working\")\nprint(\"   \u2022 If you see '\u274c STORAGE FAILED', check the error and fix before running full pipeline\")\nprint(\"   \u2022 This test takes <10 seconds vs 90 minutes for full pipeline\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}