{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 1: GitHub Setup and Enhanced Import Configuration with PHASE 3 Improvements\n\n# Install required packages first\n!pip install edgartools transformers torch requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n\nimport os\nimport sys\nimport importlib\nimport importlib.util\nimport psycopg2\nfrom psycopg2.extras import execute_values\nfrom psycopg2 import pool\nimport time\nimport json\nimport pickle\nimport traceback\nfrom pathlib import Path\nfrom functools import wraps\nfrom contextlib import contextmanager\nfrom collections import OrderedDict\nfrom typing import Dict, List, Optional, Any\nfrom datetime import datetime\nfrom edgar import set_identity\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION - All settings in one place\n# ============================================================================\n\n# Use Kaggle secrets for all sensitive credentials\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n# Database Configuration (using Kaggle secrets for security)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"),\n    'user': user_secrets.get_secret(\"NEON_USER\"), \n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'sslmode': 'require'\n}\n\n# Master Configuration Dictionary - PHASE 3 ENHANCED\nCONFIG = {\n    # GitHub Settings\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': f\"https://{user_secrets.get_secret('GITHUB_TOKEN')}@github.com/amiralpert/SmartReach.git\",\n        'local_path': \"/kaggle/working/SmartReach\"\n    },\n    \n    # Database Settings\n    'database': NEON_CONFIG,\n    \n    # Connection Pool Settings - PHASE 3\n    'pool': {\n        'min_connections': 2,\n        'max_connections': 10,\n        'keepalives': 1,\n        'keepalives_idle': 30,\n        'keepalives_interval': 10,\n        'keepalives_count': 5\n    },\n    \n    # Connection Retry Settings\n    'retry': {\n        'max_attempts': 3,\n        'initial_delay': 1,  # seconds\n        'exponential_base': 2,\n        'max_delay': 30  # seconds\n    },\n    \n    # Model Configuration - PHASE 3 ENHANCED\n    'models': {\n        'confidence_threshold': 0.5,\n        'batch_size': 16,\n        'max_length': 512,\n        'warm_up_enabled': True,  # PHASE 3: Model warm-up\n        'warm_up_text': 'Pfizer announced FDA approval for new cancer drug targeting BRCA mutations.'\n    },\n    \n    # Cache Settings - PHASE 2 ENHANCED\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 100,  # Maximum cache size in MB\n        'ttl': 3600,  # seconds\n        'eviction_policy': 'LRU'  # Least Recently Used\n    },\n    \n    # Processing Settings - PHASE 3 ENHANCED  \n    'processing': {\n        'filing_batch_size': 3,\n        'entity_batch_size': 100,  # Max entities per database insert\n        'section_validation': True,  # Enforce section name validation\n        'debug_mode': False,\n        'max_insert_batch': 500,  # Maximum batch for database inserts\n        'deprecation_warnings': True,  # Show warnings for deprecated functions\n        'checkpoint_enabled': True,  # PHASE 3: Enable checkpointing\n        'checkpoint_dir': '/kaggle/working/checkpoints',  # PHASE 3: Checkpoint directory\n        'deduplication_threshold': 0.85  # PHASE 3: Similarity threshold for dedup\n    },\n    \n    # EdgarTools Settings\n    'edgar': {\n        'identity': \"SmartReach BizIntel amir@leanbio.consulting\"\n    }\n}\n\nif not CONFIG['github']['token']:\n    raise ValueError(\"❌ GITHUB_TOKEN is required in Kaggle secrets\")\n\nif not CONFIG['database']['password']:\n    raise ValueError(\"❌ NEON_PASSWORD is required in Kaggle secrets\")\n\nprint(\"✅ Configuration loaded from Kaggle secrets\")\nprint(f\"   Database: {CONFIG['database']['host']}\")\nprint(f\"   Processing: Batch size={CONFIG['processing']['filing_batch_size']}, Section validation={CONFIG['processing']['section_validation']}\")\nprint(f\"   Cache: Max size={CONFIG['cache']['max_size_mb']}MB, TTL={CONFIG['cache']['ttl']}s\")\nprint(f\"   Database batching: Max insert batch={CONFIG['processing']['max_insert_batch']}\")\nprint(f\"   Checkpointing: {'Enabled' if CONFIG['processing']['checkpoint_enabled'] else 'Disabled'}\")\n\n# ============================================================================\n# CONNECTION RETRY DECORATOR\n# ============================================================================\n\ndef retry_on_connection_error(func):\n    \"\"\"Decorator to retry database operations on connection errors\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        max_attempts = CONFIG['retry']['max_attempts']\n        delay = CONFIG['retry']['initial_delay']\n        \n        for attempt in range(max_attempts):\n            try:\n                return func(*args, **kwargs)\n            except (psycopg2.OperationalError, psycopg2.InterfaceError) as e:\n                if attempt == max_attempts - 1:\n                    error_msg = f\"ERROR [ConnectionRetry]: Failed after {max_attempts} attempts - {type(e).__name__}: {str(e)}\"\n                    print(error_msg)\n                    if logger:\n                        logger.log(error_msg)\n                    raise\n                \n                wait_time = min(delay * (CONFIG['retry']['exponential_base'] ** attempt), CONFIG['retry']['max_delay'])\n                print(f\"WARNING [ConnectionRetry]: Attempt {attempt + 1}/{max_attempts} failed. Retrying in {wait_time}s...\")\n                time.sleep(wait_time)\n        \n        return None\n    return wrapper\n\n# ============================================================================\n# PHASE 3: ENHANCED DATABASE MANAGER WITH CONNECTION POOLING\n# ============================================================================\n\nclass DatabaseManager:\n    \"\"\"Singleton database manager with connection pooling\"\"\"\n    \n    _instance = None\n    _pool = None\n    \n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super(DatabaseManager, cls).__new__(cls)\n        return cls._instance\n    \n    def __init__(self):\n        if DatabaseManager._pool is None:\n            self._initialize_pool()\n    \n    def _initialize_pool(self):\n        \"\"\"Initialize connection pool\"\"\"\n        try:\n            DatabaseManager._pool = psycopg2.pool.ThreadedConnectionPool(\n                CONFIG['pool']['min_connections'],\n                CONFIG['pool']['max_connections'],\n                **CONFIG['database'],\n                keepalives=CONFIG['pool']['keepalives'],\n                keepalives_idle=CONFIG['pool']['keepalives_idle'],\n                keepalives_interval=CONFIG['pool']['keepalives_interval'],\n                keepalives_count=CONFIG['pool']['keepalives_count']\n            )\n            log_info(\"DatabaseManager\", f\"Connection pool initialized with {CONFIG['pool']['max_connections']} max connections\")\n        except Exception as e:\n            log_error(\"DatabaseManager\", \"Failed to initialize connection pool\", e)\n            raise\n    \n    @contextmanager\n    def get_connection(self):\n        \"\"\"Get connection from pool with automatic return\"\"\"\n        conn = None\n        try:\n            conn = DatabaseManager._pool.getconn()\n            yield conn\n            conn.commit()\n        except Exception as e:\n            if conn:\n                conn.rollback()\n            raise e\n        finally:\n            if conn:\n                DatabaseManager._pool.putconn(conn)\n    \n    def close_all(self):\n        \"\"\"Close all connections in pool\"\"\"\n        if DatabaseManager._pool:\n            DatabaseManager._pool.closeall()\n            log_info(\"DatabaseManager\", \"All connections closed\")\n    \n    def get_pool_status(self) -> Dict:\n        \"\"\"Get current pool status\"\"\"\n        if DatabaseManager._pool:\n            return {\n                'minconn': DatabaseManager._pool.minconn,\n                'maxconn': DatabaseManager._pool.maxconn,\n                'closed': DatabaseManager._pool.closed\n            }\n        return {'status': 'not initialized'}\n\n# Initialize global database manager\nDB_MANAGER = DatabaseManager()\n\n# PHASE 2: Keep backward compatibility\n@contextmanager\ndef get_db_connection():\n    \"\"\"Legacy context manager - now uses DatabaseManager\"\"\"\n    with DB_MANAGER.get_connection() as conn:\n        yield conn\n\n# ============================================================================\n# PHASE 3: CHECKPOINT MANAGER FOR FAILURE RECOVERY\n# ============================================================================\n\nclass CheckpointManager:\n    \"\"\"Manage pipeline checkpoints for failure recovery\"\"\"\n    \n    def __init__(self, checkpoint_dir: str = None):\n        self.checkpoint_dir = Path(checkpoint_dir or CONFIG['processing']['checkpoint_dir'])\n        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n        self.current_checkpoint = None\n        log_info(\"CheckpointManager\", f\"Initialized with directory: {self.checkpoint_dir}\")\n    \n    def save_checkpoint(self, state: Dict, checkpoint_name: str = None) -> str:\n        \"\"\"Save pipeline state to checkpoint\"\"\"\n        try:\n            if not checkpoint_name:\n                checkpoint_name = f\"checkpoint_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n            \n            checkpoint_path = self.checkpoint_dir / f\"{checkpoint_name}.pkl\"\n            \n            # Add metadata\n            state['checkpoint_metadata'] = {\n                'created_at': datetime.now().isoformat(),\n                'pipeline_version': '3.0',\n                'config_hash': hash(str(CONFIG))\n            }\n            \n            with open(checkpoint_path, 'wb') as f:\n                pickle.dump(state, f)\n            \n            self.current_checkpoint = checkpoint_path\n            log_info(\"CheckpointManager\", f\"Saved checkpoint: {checkpoint_name}\")\n            return str(checkpoint_path)\n            \n        except Exception as e:\n            log_error(\"CheckpointManager\", \"Failed to save checkpoint\", e)\n            return None\n    \n    def load_checkpoint(self, checkpoint_path: str = None) -> Optional[Dict]:\n        \"\"\"Load pipeline state from checkpoint\"\"\"\n        try:\n            if not checkpoint_path:\n                checkpoint_path = self.current_checkpoint\n            \n            if not checkpoint_path:\n                # Find latest checkpoint\n                checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n                if not checkpoints:\n                    log_warning(\"CheckpointManager\", \"No checkpoints found\")\n                    return None\n                checkpoint_path = max(checkpoints, key=lambda p: p.stat().st_mtime)\n            \n            with open(checkpoint_path, 'rb') as f:\n                state = pickle.load(f)\n            \n            log_info(\"CheckpointManager\", f\"Loaded checkpoint: {Path(checkpoint_path).name}\")\n            return state\n            \n        except Exception as e:\n            log_error(\"CheckpointManager\", \"Failed to load checkpoint\", e)\n            return None\n    \n    def list_checkpoints(self) -> List[Dict]:\n        \"\"\"List available checkpoints\"\"\"\n        checkpoints = []\n        for cp_file in self.checkpoint_dir.glob(\"checkpoint_*.pkl\"):\n            try:\n                stats = cp_file.stat()\n                checkpoints.append({\n                    'name': cp_file.stem,\n                    'path': str(cp_file),\n                    'size_mb': stats.st_size / (1024 * 1024),\n                    'modified': datetime.fromtimestamp(stats.st_mtime).isoformat()\n                })\n            except:\n                continue\n        return sorted(checkpoints, key=lambda x: x['modified'], reverse=True)\n    \n    def cleanup_old_checkpoints(self, keep_last: int = 5):\n        \"\"\"Clean up old checkpoints\"\"\"\n        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n        checkpoints.sort(key=lambda p: p.stat().st_mtime, reverse=True)\n        \n        for cp_file in checkpoints[keep_last:]:\n            try:\n                cp_file.unlink()\n                log_info(\"CheckpointManager\", f\"Deleted old checkpoint: {cp_file.name}\")\n            except:\n                continue\n\n# Initialize global checkpoint manager\nCHECKPOINT_MANAGER = CheckpointManager()\n\n# ============================================================================\n# PHASE 2: SIZE-LIMITED LRU CACHE\n# ============================================================================\n\nclass SizeLimitedLRUCache:\n    \"\"\"LRU cache with size limit in MB\"\"\"\n    \n    def __init__(self, max_size_mb: int):\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.cache = OrderedDict()\n        self.current_size = 0\n        self.hits = 0\n        self.misses = 0\n    \n    def _estimate_size(self, value: str) -> int:\n        \"\"\"Estimate size of cached value in bytes\"\"\"\n        return len(value.encode('utf-8')) if isinstance(value, str) else sys.getsizeof(value)\n    \n    def get(self, key: str):\n        \"\"\"Get item from cache\"\"\"\n        if key in self.cache:\n            self.hits += 1\n            # Move to end (most recently used)\n            self.cache.move_to_end(key)\n            return self.cache[key]\n        self.misses += 1\n        return None\n    \n    def put(self, key: str, value, size: int = None):\n        \"\"\"Put item in cache with LRU eviction\"\"\"\n        if size is None:\n            size = self._estimate_size(value)\n        \n        # Remove old entries if needed\n        while self.current_size + size > self.max_size_bytes and self.cache:\n            evicted_key, evicted_value = self.cache.popitem(last=False)\n            self.current_size -= self._estimate_size(evicted_value)\n            log_info(\"Cache\", f\"Evicted {evicted_key} to maintain size limit\")\n        \n        # Add new entry\n        if key in self.cache:\n            self.current_size -= self._estimate_size(self.cache[key])\n        \n        self.cache[key] = value\n        self.current_size += size\n        self.cache.move_to_end(key)\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get cache statistics\"\"\"\n        hit_rate = (self.hits / (self.hits + self.misses) * 100) if (self.hits + self.misses) > 0 else 0\n        return {\n            'entries': len(self.cache),\n            'size_mb': self.current_size / (1024 * 1024),\n            'hits': self.hits,\n            'misses': self.misses,\n            'hit_rate': hit_rate\n        }\n\n# Initialize global cache for EdgarTools sections\nSECTION_CACHE = SizeLimitedLRUCache(CONFIG['cache']['max_size_mb'])\n\n# ============================================================================\n# PHASE 3: ENHANCED ERROR LOGGING WITH STACK TRACES\n# ============================================================================\n\ndef log_error(component: str, message: str, exception: Exception = None, context: dict = None):\n    \"\"\"Enhanced error logging with stack traces\"\"\"\n    if exception:\n        error_msg = f\"ERROR [{component}]: {message} - {type(exception).__name__}: {str(exception)}\"\n        # Add stack trace for debugging\n        if CONFIG['processing'].get('debug_mode', False):\n            error_msg += f\"\\nStack trace:\\n{traceback.format_exc()}\"\n    else:\n        error_msg = f\"ERROR [{component}]: {message}\"\n    \n    if context:\n        error_msg += f\" | Context: {context}\"\n    \n    print(error_msg)  # Auto-logger captures this\n    return error_msg\n\ndef log_warning(component: str, message: str, context: dict = None):\n    \"\"\"Standardized warning logging\"\"\"\n    warning_msg = f\"WARNING [{component}]: {message}\"\n    if context:\n        warning_msg += f\" | Context: {context}\"\n    print(warning_msg)\n    return warning_msg\n\ndef log_info(component: str, message: str):\n    \"\"\"Standardized info logging\"\"\"\n    info_msg = f\"INFO [{component}]: {message}\"\n    print(info_msg)\n    return info_msg\n\n# ============================================================================\n# PHASE 2: DEPRECATION WARNING SYSTEM\n# ============================================================================\n\ndef deprecated(replacement_func: str = None):\n    \"\"\"Decorator to mark functions as deprecated\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if CONFIG['processing']['deprecation_warnings']:\n                msg = f\"Function '{func.__name__}' is deprecated and will be removed.\"\n                if replacement_func:\n                    msg += f\" Use '{replacement_func}' instead.\"\n                log_warning(\"Deprecation\", msg)\n            return func(*args, **kwargs)\n        return wrapper\n    return decorator\n\n# ============================================================================\n# GITHUB SETUP\n# ============================================================================\n\nprint(\"\\n📦 Setting up GitHub repository...\")\nlocal_path = CONFIG['github']['local_path']\nrepo_url = CONFIG['github']['repo_url']\n\n# Clone or update repo with force pull\nif os.path.exists(local_path):\n    log_info(\"GitHub\", f\"Repository exists at {local_path}\")\n    log_info(\"GitHub\", \"Force updating from main branch\")\n    !cd {local_path} && git fetch origin\n    !cd {local_path} && git reset --hard origin/main\n    !cd {local_path} && git pull origin main\n    log_info(\"GitHub\", \"Repository updated successfully\")\n    \n    # Show current commit\n    !cd {local_path} && echo \"Current commit:\" && git log --oneline -1\nelse:\n    log_info(\"GitHub\", f\"Cloning repository to {local_path}\")\n    !git clone {repo_url} {local_path}\n    log_info(\"GitHub\", \"Repository cloned successfully\")\n\n# Clear any cached modules from previous runs\nmodules_to_clear = [key for key in sys.modules.keys() if 'auto_logger' in key.lower() or 'clean' in key.lower()]\nfor mod in modules_to_clear:\n    del sys.modules[mod]\n    log_info(\"ModuleCache\", f\"Cleared cached module: {mod}\")\n\n# Add to Python path for regular imports\nif f'{local_path}/BizIntel' in sys.path:\n    sys.path.remove(f'{local_path}/BizIntel')\nsys.path.insert(0, f'{local_path}/BizIntel')\n\nlog_info(\"Setup\", \"Python path configured for SEC entity extraction\")\n\n# Configure EdgarTools authentication - REQUIRED by SEC\nset_identity(CONFIG['edgar']['identity'])\nlog_info(\"EdgarTools\", f\"Identity configured: {CONFIG['edgar']['identity']}\")\n\n# ============================================================================\n# AUTO-LOGGER SETUP\n# ============================================================================\n\n@retry_on_connection_error\ndef setup_logger():\n    \"\"\"Set up the auto-logger with retry logic\"\"\"\n    with DB_MANAGER.get_connection() as logger_conn:\n        log_info(\"AutoLogger\", \"Database connected for clean logger\")\n        \n        # Import the redesigned clean auto-logger\n        logger_module_path = f\"{local_path}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\n        if os.path.exists(logger_module_path):\n            spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_module_path)\n            auto_logger_module = importlib.util.module_from_spec(spec)\n            sys.modules[\"auto_logger\"] = auto_logger_module\n            spec.loader.exec_module(auto_logger_module)\n\n            # Use the new clean logging setup\n            setup_clean_logging = auto_logger_module.setup_clean_logging\n            logger = setup_clean_logging(logger_conn, \"SEC_EntityExtraction\")\n            \n            log_info(\"AutoLogger\", \"Clean auto-logging enabled successfully\")\n            print(\"📋 Features:\")\n            print(\"   • One row per cell execution\")\n            print(\"   • Complete output capture\")\n            print(\"   • Proper cell numbers from # Cell N: comments\")\n            print(\"   • Full error tracebacks\")\n            print(\"   • Execution timing\")\n            return logger\n        else:\n            log_error(\"AutoLogger\", f\"Logger module not found at {logger_module_path}\")\n            return None\n\ntry:\n    logger = setup_logger()\nexcept Exception as e:\n    log_error(\"AutoLogger\", \"Logger setup failed\", e)\n    log_warning(\"AutoLogger\", \"Continuing without auto-logging\")\n    logger = None\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🚀 SEC ENTITY EXTRACTION ENGINE INITIALIZED - PHASE 3 PRODUCTION-READY\")\nprint(\"=\"*80)\nprint(f\"✅ GitHub: Repository at {CONFIG['github']['local_path']}\")\nprint(f\"✅ Database: Connected to {CONFIG['database']['host']}\")\nprint(f\"✅ EdgarTools: Configured as '{CONFIG['edgar']['identity']}'\")\nprint(f\"✅ Logging: {'Enabled' if logger else 'Disabled'}\")\nprint(f\"✅ Section Validation: {'ENFORCED' if CONFIG['processing']['section_validation'] else 'Disabled'}\")\nprint(f\"✅ PHASE 2 Enhancements:\")\nprint(f\"   • Database context manager: get_db_connection()\")\nprint(f\"   • Size-limited LRU cache: {CONFIG['cache']['max_size_mb']}MB\")\nprint(f\"   • Batch size limits: Max {CONFIG['processing']['max_insert_batch']} per insert\")\nprint(f\"   • Deprecation warnings: {'Enabled' if CONFIG['processing']['deprecation_warnings'] else 'Disabled'}\")\nprint(f\"✅ PHASE 3 PRODUCTION FEATURES:\")\nprint(f\"   • Connection Pool: {CONFIG['pool']['max_connections']} max connections\")\nprint(f\"   • Checkpoint System: Recovery from failures enabled\")\nprint(f\"   • Enhanced Error Logging: Stack traces included\")\nprint(f\"   • Model Warm-up: Enabled for faster first inference\")\nprint(f\"   • Entity Deduplication: {CONFIG['processing']['deduplication_threshold']} threshold\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 2: Database Connection Test with Retry Logic\n\nimport psycopg2\n\n# Test database connection with retry wrapper\n@retry_on_connection_error\ndef test_database_connection():\n    \"\"\"Test database connection with automatic retry on failure\"\"\"\n    try:\n        conn = psycopg2.connect(**CONFIG['database'])\n        cursor = conn.cursor()\n        \n        # Check SEC-related tables\n        cursor.execute('''\n            SELECT \n                (SELECT COUNT(*) FROM raw_data.sec_filings) as sec_filings,\n                (SELECT COUNT(*) FROM core.companies) as companies,\n                (SELECT COUNT(*) FROM system_uno.sec_entities_raw) as sec_entities_extracted,\n                (SELECT COUNT(DISTINCT company_domain) FROM raw_data.sec_filings) as companies_with_filings,\n                (SELECT COUNT(*) FROM raw_data.sec_filings WHERE url IS NOT NULL) as filings_with_urls\n        ''')\n        \n        counts = cursor.fetchone()\n        log_info(\"DatabaseTest\", \"Database connected successfully\")\n        print(f\"  SEC Filings: {counts[0]}\")\n        print(f\"  Companies: {counts[1]}\")\n        print(f\"  Extracted SEC Entities: {counts[2]}\")\n        print(f\"  Companies with SEC Filings: {counts[3]}\")\n        print(f\"  SEC Filings with URLs: {counts[4]}\")\n        \n        # Show sample SEC filing data\n        cursor.execute('''\n            SELECT company_domain, filing_type, COUNT(*) as count\n            FROM raw_data.sec_filings \n            GROUP BY company_domain, filing_type \n            ORDER BY company_domain, count DESC\n            LIMIT 10\n        ''')\n        \n        filing_stats = cursor.fetchall()\n        print(\"\\n📊 SEC Filing Distribution:\")\n        for stat in filing_stats:\n            print(f\"  {stat[0]}: {stat[1]} ({stat[2]} filings)\")\n        \n        # Check for section name issues\n        cursor.execute('''\n            SELECT \n                COUNT(*) FILTER (WHERE section_name IS NOT NULL AND section_name != '') as with_section,\n                COUNT(*) FILTER (WHERE section_name IS NULL OR section_name = '') as without_section,\n                COUNT(*) as total\n            FROM system_uno.sec_entities_raw\n            WHERE data_source = 'sec_filings'\n        ''')\n        \n        section_stats = cursor.fetchone()\n        if section_stats and section_stats[2] > 0:\n            success_rate = (section_stats[0] / section_stats[2]) * 100\n            if success_rate < 90:\n                log_warning(\"DatabaseTest\", \n                          f\"Section name success rate: {success_rate:.1f}%\",\n                          {\"with_section\": section_stats[0], \n                           \"without_section\": section_stats[1],\n                           \"total\": section_stats[2]})\n            else:\n                log_info(\"DatabaseTest\", f\"Section name success rate: {success_rate:.1f}% ✅\")\n        \n        cursor.close()\n        conn.close()\n        return True\n        \n    except Exception as e:\n        log_error(\"DatabaseTest\", \"Database connection test failed\", e)\n        return False\n\n# Test connection\nlog_info(\"DatabaseTest\", \"Starting database connection test...\")\nconnection_successful = test_database_connection()\n\nif connection_successful:\n    log_info(\"DatabaseTest\", \"✅ Database test completed successfully\")\nelse:\n    log_error(\"DatabaseTest\", \"❌ Database test failed - check configuration and retry settings\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Accession-Based SEC Content Extraction with EdgarTools - SIMPLIFIED\n\nimport edgar\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nimport requests\nimport re\nfrom typing import Dict, List, Optional, Tuple\nfrom bs4 import BeautifulSoup\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# REMOVED extract_accession_from_url - No longer needed since database has accession_number column\n\ndef get_filing_sections(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get structured sections from SEC filing using accession number\n    \n    PHASE 2 ENHANCED: Using cache for sections to avoid repeated API calls\n    \"\"\"\n    # Check cache first\n    cache_key = f\"{accession_number}#{filing_type or 'auto'}\"\n    cached_sections = SECTION_CACHE.get(cache_key)\n    if cached_sections:\n        log_info(\"Cache\", f\"Cache hit for {accession_number}\")\n        return cached_sections\n    \n    try:\n        # Find filing using accession number\n        filing = find(accession_number)\n        \n        if not filing:\n            raise ValueError(f\"Filing not found for accession: {accession_number}\")\n        \n        # Auto-detect filing type if not provided\n        if not filing_type:\n            filing_type = getattr(filing, 'form', '10-K')\n        \n        log_info(\"EdgarTools\", f\"Found {filing_type} for {getattr(filing, 'company', 'Unknown Company')}\")\n        \n        # Get structured HTML content\n        html_content = filing.html()\n        if not html_content:\n            raise ValueError(\"No HTML content available\")\n        \n        # Parse HTML to Document object\n        document = parse_html(html_content)\n        \n        # Extract sections using SectionExtractor\n        extractor = SectionExtractor(filing_type=filing_type)\n        sections = extractor.extract(document)\n        \n        log_info(\"EdgarTools\", f\"SectionExtractor found {len(sections)} sections\")\n        \n        # Convert sections to text dictionary\n        section_texts = {}\n        for section_name, section in sections.items():\n            try:\n                if hasattr(section, 'text'):\n                    text = section.text() if callable(section.text) else section.text\n                    if isinstance(text, str) and text.strip():\n                        section_texts[section_name] = text.strip()\n                        print(f\"      • {section_name}: {len(text):,} chars\")\n                elif hasattr(section, '__str__'):\n                    text = str(section).strip()\n                    if text:\n                        section_texts[section_name] = text\n                        print(f\"      • {section_name}: {len(text):,} chars (via str)\")\n            except Exception as section_e:\n                log_warning(\"EdgarTools\", f\"Could not extract section {section_name}\", {\"error\": str(section_e)})\n                continue\n        \n        # If SectionExtractor returns no sections, fall back to full document text\n        if not section_texts:\n            log_warning(\"EdgarTools\", \"No structured sections found, using full document fallback\")\n            full_text = document.text() if hasattr(document, 'text') and callable(document.text) else str(document)\n            if full_text and len(full_text.strip()) > 100:  # Only use if substantial content\n                section_texts['full_document'] = full_text.strip()\n                log_info(\"EdgarTools\", f\"Using full document: {len(full_text):,} chars\")\n        \n        # Cache the result\n        if section_texts and CONFIG['cache']['enabled']:\n            SECTION_CACHE.put(cache_key, section_texts)\n            log_info(\"Cache\", f\"Cached sections for {accession_number} ({len(section_texts)} sections)\")\n        \n        return section_texts\n        \n    except Exception as e:\n        log_error(\"EdgarTools\", \"Section extraction failed\", e)\n        return {}\n\ndef route_sections_to_models(sections: Dict[str, str], filing_type: str) -> Dict[str, List[str]]:\n    \"\"\"Route sections to appropriate NER models based on filing type\"\"\"\n    routing = {\n        'biobert': [],\n        'bert_base': [],\n        'roberta': [],\n        'finbert': []\n    }\n    \n    if filing_type.upper() in ['10-K', '10-Q']:\n        for section_name, section_text in sections.items():\n            # FinBERT gets financial statements exclusively\n            if 'financial' in section_name.lower() or 'statement' in section_name.lower():\n                routing['finbert'].append(section_name)\n            else:\n                # All other sections go to BERT/RoBERTa/BioBERT\n                routing['bert_base'].append(section_name)\n                routing['roberta'].append(section_name)\n                routing['biobert'].append(section_name)\n    \n    elif filing_type.upper() == '8-K':\n        # 8-K: all item sections go to all four models\n        for section_name in sections.keys():\n            routing['biobert'].append(section_name)\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['finbert'].append(section_name)\n    \n    else:\n        # Default routing for other filing types\n        for section_name in sections.keys():\n            routing['bert_base'].append(section_name)\n            routing['roberta'].append(section_name)\n            routing['biobert'].append(section_name)\n    \n    # Remove empty routing\n    routing = {model: sections_list for model, sections_list in routing.items() if sections_list}\n    \n    return routing\n\ndef process_sec_filing_with_sections(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing with section-based extraction\n    \n    SIMPLIFIED: Now uses accession_number directly from database\n    \"\"\"\n    try:\n        filing_id = filing_data.get('id')\n        accession_number = filing_data.get('accession_number')  # DIRECT FROM DATABASE\n        filing_type = filing_data.get('filing_type', '10-K')\n        company_domain = filing_data.get('company_domain', 'Unknown')\n        filing_url = filing_data.get('url')  # Still keep for reference\n        \n        log_info(\"FilingProcessor\", f\"Processing {filing_type} for {company_domain}\")\n        print(f\"   📄 Filing ID: {filing_id}\")\n        print(f\"   📑 Accession: {accession_number}\")\n        \n        # Validate accession number\n        if not accession_number:\n            raise ValueError(f\"Missing accession number for filing {filing_id}\")\n        \n        # Get structured sections using accession directly\n        sections = get_filing_sections(accession_number, filing_type)\n        if not sections:\n            raise ValueError(\"No sections extracted\")\n        \n        log_info(\"FilingProcessor\", f\"Extracted {len(sections)} sections\")\n        \n        # Route sections to models\n        model_routing = route_sections_to_models(sections, filing_type)\n        print(f\"   🎯 Model routing: {[f'{model}: {len(secs)} sections' for model, secs in model_routing.items()]}\")\n        \n        # Validate section names if configured\n        if CONFIG['processing']['section_validation']:\n            missing_sections = [name for name in sections.keys() if not name]\n            if missing_sections:\n                log_warning(\"FilingProcessor\", f\"Found {len(missing_sections)} sections without names\")\n        \n        # Show cache statistics\n        cache_stats = SECTION_CACHE.get_stats()\n        if cache_stats['hits'] > 0:\n            print(f\"   📊 Cache: {cache_stats['hit_rate']:.1f}% hit rate, {cache_stats['size_mb']:.1f}MB used\")\n        \n        return {\n            'filing_id': filing_id,\n            'company_domain': company_domain,\n            'filing_type': filing_type,\n            'accession_number': accession_number,\n            'url': filing_url,\n            'sections': sections,\n            'model_routing': model_routing,\n            'total_sections': len(sections),\n            'processing_status': 'success'\n        }\n        \n    except Exception as e:\n        log_error(\"FilingProcessor\", \"Filing processing failed\", e, \n                 {\"filing_id\": filing_data.get('id'), \"accession\": filing_data.get('accession_number')})\n        return {\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain', 'Unknown'),\n            'filing_type': filing_data.get('filing_type', 'Unknown'),\n            'accession_number': filing_data.get('accession_number'),\n            'error': str(e),\n            'processing_status': 'failed'\n        }\n\n@retry_on_connection_error\ndef get_unprocessed_filings(limit: int = 5) -> List[Dict]:\n    \"\"\"Get SEC filings that haven't been processed yet\n    \n    ENHANCED: Now retrieves accession_number directly from database\n    \"\"\"\n    with get_db_connection() as conn:  # PHASE 2: Using context manager\n        cursor = conn.cursor()\n        \n        cursor.execute(\"\"\"\n            SELECT \n                sf.id, \n                sf.company_domain, \n                sf.filing_type, \n                sf.accession_number,  -- DIRECT FROM DATABASE\n                sf.url, \n                sf.filing_date, \n                sf.title\n            FROM raw_data.sec_filings sf\n            LEFT JOIN system_uno.sec_entities_raw ser \n                ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n            WHERE sf.accession_number IS NOT NULL  -- Must have accession\n                AND ser.sec_filing_ref IS NULL     -- Not yet processed\n            ORDER BY sf.filing_date DESC\n            LIMIT %s\n        \"\"\", (limit,))\n        \n        filings = cursor.fetchall()\n        cursor.close()\n        \n        log_info(\"DatabaseQuery\", f\"Retrieved {len(filings)} unprocessed filings with accession numbers\")\n        \n        return [{\n            'id': filing[0],\n            'company_domain': filing[1],\n            'filing_type': filing[2],\n            'accession_number': filing[3],  # ACCESSION NUMBER INCLUDED\n            'url': filing[4],\n            'filing_date': filing[5],\n            'title': filing[6]\n        } for filing in filings]\n\n# Test the simplified extraction\nlog_info(\"Test\", \"Starting simplified section extraction test (using direct accession)\")\n\ntest_filings = get_unprocessed_filings(limit=1)\n\nif test_filings:\n    log_info(\"Test\", f\"Found {len(test_filings)} test filing(s)\")\n    \n    for filing in test_filings:\n        print(f\"\\n📋 Test Filing:\")\n        print(f\"   Company: {filing['company_domain']}\")\n        print(f\"   Type: {filing['filing_type']}\")\n        print(f\"   Accession: {filing['accession_number']}\")  # Direct from database\n        \n        result = process_sec_filing_with_sections(filing)\n        \n        if result['processing_status'] == 'success':\n            log_info(\"Test\", f\"Success: {result['total_sections']} sections extracted\")\n            print(f\"   📊 Available sections: {list(result['sections'].keys())}\")\n            \n            # Show model routing summary\n            for model, section_list in result['model_routing'].items():\n                print(f\"   🎯 {model}: {len(section_list)} sections\")\n                \n            # Show sample section content\n            if result['sections']:\n                first_section = list(result['sections'].keys())[0]\n                sample_text = result['sections'][first_section][:200] + \"...\"\n                print(f\"   📝 Sample from '{first_section}': {sample_text}\")\n        else:\n            log_error(\"Test\", \"Filing test failed\", None, {\"error\": result.get('error', 'Unknown error')})\nelse:\n    log_warning(\"Test\", \"No unprocessed filings found for testing\")\n\nprint(\"\\n✅ SIMPLIFIED SEC section extraction ready!\")\nprint(\"🔧 Improvements:\")\nprint(\"   ✂️ REMOVED: extract_accession_from_url() function (redundant)\")\nprint(\"   ✅ DIRECT: Accession number from database column\")\nprint(\"   🚀 FASTER: No URL parsing needed\")\nprint(\"   📊 CACHE: Section caching with LRU eviction\")\nprint(\"   🔒 SAFER: No regex extraction failures\")\nprint(\"   • Standardized error logging for auto-logger\")\nprint(\"   • Connection context manager (Phase 2)\")\nprint(\"   • Section name validation when configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: EntityExtractionPipeline Class and NER Model Loading\n\nimport torch\nimport time\nimport uuid\nimport json\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\nfrom abc import ABC, abstractmethod\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nprint(\"🚀 Loading EntityExtractionPipeline and NER Models...\")\n\nclass EntityExtractionPipeline:\n    \"\"\"Extensible pipeline for entity extraction from multiple data sources\"\"\"\n    \n    def __init__(self, db_config: Dict, model_config: Dict = None):\n        self.db_config = db_config\n        self.model_config = model_config or self._get_default_model_config()\n        self.loaded_models = {}\n        self.pipeline_stats = {\n            'documents_processed': 0,\n            'total_entities_extracted': 0,\n            'processing_time_total': 0,\n            'data_sources_supported': ['sec_filings', 'press_releases', 'patents']\n        }\n        \n        print(f\"🔧 EntityExtractionPipeline initialized\")\n        print(f\"   📊 Supported data sources: {self.pipeline_stats['data_sources_supported']}\")\n    \n    def _get_default_model_config(self) -> Dict:\n        \"\"\"Default model configuration for biotech entity extraction\"\"\"\n        return {\n            'models': {\n                'biobert': {\n                    'model_name': 'alvaroalon2/biobert_diseases_ner',\n                    'confidence_threshold': 0.5,\n                    'description': 'Biomedical entities (diseases, medications, treatments)'\n                },\n                'bert_base': {\n                    'model_name': 'dslim/bert-base-NER',\n                    'confidence_threshold': 0.5,\n                    'description': 'General entities (persons, organizations, locations)'\n                },\n                'finbert': {\n                    'model_name': 'ProsusAI/finbert',\n                    'confidence_threshold': 0.5,\n                    'description': 'Financial entities and metrics'\n                },\n                'roberta': {\n                    'model_name': 'Jean-Baptiste/roberta-large-ner-english',\n                    'confidence_threshold': 0.6,\n                    'description': 'High-precision general entities'\n                }\n            },\n            'routing_rules': {\n                'sec_filings': {\n                    '10-K': {\n                        'financial_sections': ['finbert'],\n                        'other_sections': ['biobert', 'bert_base', 'roberta']\n                    },\n                    '10-Q': {\n                        'financial_sections': ['finbert'],\n                        'other_sections': ['biobert', 'bert_base', 'roberta']\n                    },\n                    '8-K': {\n                        'all_sections': ['biobert', 'bert_base', 'roberta', 'finbert']\n                    }\n                },\n                'press_releases': {\n                    'all_content': ['biobert', 'bert_base', 'roberta']\n                },\n                'patents': {\n                    'all_content': ['biobert', 'bert_base', 'roberta']\n                }\n            },\n            'entity_type_mapping': {\n                # BioBERT mappings\n                'Disease': 'MEDICAL_CONDITION',\n                'Chemical': 'MEDICATION',\n                'CHEMICAL': 'MEDICATION',\n                'DISEASE': 'MEDICAL_CONDITION',\n                'DRUG': 'MEDICATION',\n                'Drug': 'MEDICATION',\n                'Compound': 'MEDICATION',\n                'Treatment': 'THERAPY',\n                'Therapy': 'THERAPY',\n                \n                # BERT-base mappings\n                'PER': 'PERSON',\n                'ORG': 'ORGANIZATION',\n                'LOC': 'LOCATION',\n                'MISC': 'MISCELLANEOUS',\n                \n                # Financial mappings\n                'MONEY': 'FINANCIAL',\n                'PERCENT': 'FINANCIAL',\n                'NUMBER': 'METRIC'\n            }\n        }\n    \n    def load_models(self) -> Dict:\n        \"\"\"Load all NER models specified in configuration\"\"\"\n        print(f\"📦 Loading {len(self.model_config['models'])} NER models...\")\n        \n        for model_key, model_info in self.model_config['models'].items():\n            try:\n                model_name = model_info['model_name']\n                print(f\"   🧠 Loading {model_key}: {model_name}\")\n                \n                tokenizer = AutoTokenizer.from_pretrained(model_name)\n                model = AutoModelForTokenClassification.from_pretrained(model_name)\n                ner_pipeline = pipeline(\n                    \"ner\", \n                    model=model, \n                    tokenizer=tokenizer,\n                    aggregation_strategy=\"average\", \n                    device=0 if torch.cuda.is_available() else -1\n                )\n                \n                self.loaded_models[model_key] = {\n                    'pipeline': ner_pipeline,\n                    'threshold': model_info['confidence_threshold'],\n                    'description': model_info['description'],\n                    'stats': {\n                        'texts_processed': 0,\n                        'entities_found': 0,\n                        'processing_time': 0\n                    }\n                }\n                \n                print(f\"      ✓ {model_key} loaded (threshold: {model_info['confidence_threshold']})\")\n                \n            except Exception as e:\n                print(f\"      ❌ {model_key} failed: {e}\")\n                # Use BERT-base as fallback for failed models\n                if model_key != 'bert_base' and 'bert_base' in self.loaded_models:\n                    self.loaded_models[model_key] = self.loaded_models['bert_base']\n                    print(f\"      🔄 Using BERT-base fallback for {model_key}\")\n        \n        loaded_count = len(self.loaded_models)\n        device = \"GPU\" if torch.cuda.is_available() else \"CPU\"\n        \n        print(f\"   ✅ Successfully loaded: {loaded_count}/4 models on {device}\")\n        \n        return self.loaded_models\n    \n    def get_routing_for_content(self, data_source: str, filing_type: str = None, section_name: str = None) -> List[str]:\n        \"\"\"Get model routing for specific content\"\"\"\n        routing_rules = self.model_config['routing_rules']\n        \n        if data_source not in routing_rules:\n            # Default to all text models for unknown data sources\n            return ['biobert', 'bert_base', 'roberta']\n        \n        source_rules = routing_rules[data_source]\n        \n        if data_source == 'sec_filings' and filing_type:\n            filing_rules = source_rules.get(filing_type, source_rules.get('10-K'))\n            \n            # Check if it's a financial section\n            if section_name and any(fin_word in section_name.lower() \n                                  for fin_word in ['financial', 'statement', 'balance', 'income', 'cash']):\n                return filing_rules.get('financial_sections', ['finbert'])\n            else:\n                return filing_rules.get('other_sections', ['biobert', 'bert_base', 'roberta'])\n        \n        elif data_source in ['press_releases', 'patents']:\n            return source_rules['all_content']\n        \n        # Default routing\n        return ['biobert', 'bert_base', 'roberta']\n    \n    def extract_entities_from_text(self, text: str, model_name: str, metadata: Dict = None) -> List[Dict]:\n        \"\"\"Extract entities from text using specific model\"\"\"\n        if model_name not in self.loaded_models or not text.strip():\n            return []\n        \n        try:\n            start_time = time.time()\n            model_info = self.loaded_models[model_name]\n            \n            # Run NER pipeline\n            raw_entities = model_info['pipeline'](text)\n            \n            # Process and filter results\n            processed_entities = []\n            for entity in raw_entities:\n                if entity['score'] >= model_info['threshold']:\n                    # Normalize entity type\n                    entity_type = self.model_config['entity_type_mapping'].get(\n                        entity['entity_group'], entity['entity_group']\n                    )\n                    \n                    processed_entity = {\n                        'entity_text': entity['word'].strip(),\n                        'entity_type': entity_type,\n                        'confidence_score': float(entity['score']),\n                        'char_start': entity['start'],\n                        'char_end': entity['end'],\n                        'model_source': model_name,\n                        'original_label': entity['entity_group'],\n                        'extraction_id': str(uuid.uuid4()),\n                        'extraction_timestamp': datetime.now().isoformat()\n                    }\n                    \n                    # Add metadata if provided\n                    if metadata:\n                        processed_entity.update(metadata)\n                    \n                    processed_entities.append(processed_entity)\n            \n            # Update statistics\n            processing_time = time.time() - start_time\n            model_info['stats']['texts_processed'] += 1\n            model_info['stats']['entities_found'] += len(processed_entities)\n            model_info['stats']['processing_time'] += processing_time\n            \n            return processed_entities\n            \n        except Exception as e:\n            print(f\"   ❌ {model_name} extraction failed: {e}\")\n            return []\n    \n    def process_sec_filing_sections(self, filing_result: Dict) -> List[Dict]:\n        \"\"\"Process SEC filing sections with appropriate model routing\"\"\"\n        if filing_result.get('processing_status') != 'success':\n            return []\n        \n        filing_id = filing_result['filing_id']\n        filing_type = filing_result['filing_type']\n        company_domain = filing_result['company_domain']\n        sections = filing_result['sections']\n        \n        print(f\"🔍 Processing {len(sections)} sections for {company_domain} {filing_type}\")\n        \n        all_entities = []\n        \n        for section_name, section_text in sections.items():\n            # Get routing for this section\n            applicable_models = self.get_routing_for_content('sec_filings', filing_type, section_name)\n            applicable_models = [m for m in applicable_models if m in self.loaded_models]\n            \n            if not applicable_models:\n                continue\n            \n            print(f\"   📑 Processing '{section_name}' with {len(applicable_models)} models\")\n            \n            # Metadata for this section\n            section_metadata = {\n                'filing_id': filing_id,\n                'company_domain': company_domain,\n                'filing_type': filing_type,\n                'section_name': section_name,\n                'sec_filing_ref': f'SEC_{filing_id}',\n                'data_source': 'sec_filings'\n            }\n            \n            # Process with each applicable model\n            section_entities = []\n            for model_name in applicable_models:\n                model_entities = self.extract_entities_from_text(section_text, model_name, section_metadata)\n                section_entities.extend(model_entities)\n                print(f\"      • {model_name}: {len(model_entities)} entities\")\n            \n            all_entities.extend(section_entities)\n        \n        # Merge entities at same positions\n        merged_entities = self.merge_position_overlaps(all_entities)\n        print(f\"   🔗 Merged: {len(all_entities)} → {len(merged_entities)} entities\")\n        \n        return merged_entities\n    \n    def merge_position_overlaps(self, entities: List[Dict]) -> List[Dict]:\n        \"\"\"Merge entities detected at same position by different models\"\"\"\n        if not entities:\n            return []\n        \n        # Group entities by position\n        position_groups = {}\n        \n        for entity in entities:\n            # Create position key\n            pos_key = f\"{entity.get('filing_id', 'unknown')}_{entity.get('section_name', 'unknown')}_{entity.get('char_start', 0)}_{entity.get('char_end', 0)}\"\n            \n            if pos_key not in position_groups:\n                position_groups[pos_key] = []\n            position_groups[pos_key].append(entity)\n        \n        merged_entities = []\n        \n        for pos_key, group in position_groups.items():\n            if len(group) == 1:\n                merged_entities.append(group[0])\n            else:\n                # Merge multiple detections\n                merged = self._merge_entity_group(group)\n                merged_entities.append(merged)\n        \n        return merged_entities\n    \n    def _merge_entity_group(self, entities: List[Dict]) -> Dict:\n        \"\"\"Merge entities from different models at same position\"\"\"\n        # Priority for biotech domain: BioBERT > FinBERT > RoBERTa > BERT-base\n        priority = {'biobert': 4, 'finbert': 3, 'roberta': 2, 'bert_base': 1}\n        \n        # Sort by priority, then by confidence\n        entities.sort(key=lambda x: (priority.get(x['model_source'], 0), x['confidence_score']), reverse=True)\n        \n        # Use best entity as base\n        best = entities[0].copy()\n        \n        # Add multi-model metadata\n        best['models_detected'] = [e['model_source'] for e in entities]\n        best['all_confidences'] = {e['model_source']: e['confidence_score'] for e in entities}\n        best['primary_model'] = best['model_source']\n        best['entity_variations'] = {e['model_source']: e['entity_text'] for e in entities}\n        best['is_merged'] = True\n        best['confidence_score'] = max(e['confidence_score'] for e in entities)\n        \n        return best\n    \n    def add_data_source_support(self, source_name: str, routing_config: Dict, processor_func: callable = None):\n        \"\"\"Dynamically add support for new data sources\"\"\"\n        # Add to routing rules\n        self.model_config['routing_rules'][source_name] = routing_config\n        \n        # Add to supported sources\n        if source_name not in self.pipeline_stats['data_sources_supported']:\n            self.pipeline_stats['data_sources_supported'].append(source_name)\n        \n        print(f\"✅ Added support for data source: {source_name}\")\n        print(f\"   📊 Routing config: {routing_config}\")\n    \n    def get_pipeline_statistics(self) -> Dict:\n        \"\"\"Get comprehensive pipeline statistics\"\"\"\n        return {\n            'pipeline_stats': self.pipeline_stats.copy(),\n            'model_stats': {\n                name: info['stats'].copy() \n                for name, info in self.loaded_models.items()\n            },\n            'loaded_models': list(self.loaded_models.keys()),\n            'supported_data_sources': self.pipeline_stats['data_sources_supported'],\n            'device': \"GPU\" if torch.cuda.is_available() else \"CPU\"\n        }\n\n# Initialize the EntityExtractionPipeline\nentity_pipeline = EntityExtractionPipeline(NEON_CONFIG)\n\n# Load all NER models\nloaded_models = entity_pipeline.load_models()\n\n# Test the pipeline with sample text\nif loaded_models:\n    test_text = \"Pfizer's COVID-19 vaccine generated $37 billion in revenue. The FDA approved treatment for Alzheimer's disease in Boston.\"\n    print(f\"\\n🧪 Testing models with sample text...\")\n    \n    for model_name, model_info in loaded_models.items():\n        try:\n            test_entities = entity_pipeline.extract_entities_from_text(test_text, model_name)\n            print(f\"   {model_name}: Found {len(test_entities)} entities\")\n            for entity in test_entities[:2]:\n                print(f\"      • {entity['entity_type']}: '{entity['entity_text']}' ({entity['confidence_score']:.3f})\")\n        except Exception as e:\n            print(f\"   {model_name}: Test failed - {e}\")\n\nprint(f\"\\n✅ EntityExtractionPipeline ready!\")\nprint(f\"   🎯 Loaded models: {list(loaded_models.keys())}\")\nprint(f\"   🔧 Supported sources: {entity_pipeline.pipeline_stats['data_sources_supported']}\")\nprint(f\"   💻 Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\nprint(f\"   📊 Ready for biotech-optimized entity extraction!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: GPU-Optimized Entity Storage and Database Integration\n\nimport psycopg2\nfrom psycopg2.extras import execute_values\nimport numpy as np\n\nclass PipelineEntityStorage:\n    \"\"\"Enhanced storage system for EntityExtractionPipeline results\"\"\"\n    \n    def __init__(self, db_config: Dict):\n        self.db_config = db_config\n        self.storage_stats = {\n            'total_entities_stored': 0,\n            'filings_processed': 0,\n            'merged_entities': 0,\n            'single_model_entities': 0,\n            'failed_inserts': 0\n        }\n        \n        # Ensure enhanced table structure\n        self._ensure_enhanced_table()\n    \n    def _ensure_enhanced_table(self):\n        \"\"\"Ensure table has all required columns for pipeline storage\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Check existing columns\n            cursor.execute(\"\"\"\n                SELECT column_name \n                FROM information_schema.columns \n                WHERE table_schema = 'system_uno' \n                AND table_name = 'sec_entities_raw'\n            \"\"\")\n            \n            existing_columns = {row[0] for row in cursor.fetchall()}\n            \n            # Required columns for pipeline\n            required_columns = {\n                'models_detected': 'TEXT[]',\n                'all_confidences': 'JSONB',\n                'primary_model': 'TEXT',\n                'entity_variations': 'JSONB',\n                'is_merged': 'BOOLEAN DEFAULT FALSE',\n                'section_name': 'TEXT',\n                'data_source': 'TEXT DEFAULT \\'sec_filings\\'',\n                'extraction_timestamp': 'TIMESTAMP DEFAULT CURRENT_TIMESTAMP',\n                'original_label': 'TEXT'\n            }\n            \n            # Add missing columns\n            for col_name, col_type in required_columns.items():\n                if col_name not in existing_columns:\n                    cursor.execute(f'ALTER TABLE system_uno.sec_entities_raw ADD COLUMN {col_name} {col_type}')\n                    print(f\"   ✓ Added column: {col_name}\")\n            \n            # Create indexes for performance\n            index_queries = [\n                'CREATE INDEX IF NOT EXISTS idx_sec_entities_pipeline_position ON system_uno.sec_entities_raw (sec_filing_ref, section_name, character_start, character_end)',\n                'CREATE INDEX IF NOT EXISTS idx_sec_entities_models ON system_uno.sec_entities_raw USING GIN (models_detected)',\n                'CREATE INDEX IF NOT EXISTS idx_sec_entities_source ON system_uno.sec_entities_raw (data_source, primary_model)',\n                'CREATE INDEX IF NOT EXISTS idx_sec_entities_merged ON system_uno.sec_entities_raw (is_merged, entity_category)'\n            ]\n            \n            for idx_query in index_queries:\n                cursor.execute(idx_query)\n            \n            conn.commit()\n            cursor.close()\n            conn.close()\n            print(\"✓ Enhanced table structure verified\")\n            \n        except Exception as e:\n            print(f\"⚠️ Table enhancement failed: {e}\")\n    \n    def store_pipeline_entities(self, entities: List[Dict]) -> bool:\n        \"\"\"Store entities from EntityExtractionPipeline\"\"\"\n        if not entities:\n            return True\n        \n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            print(f\"💾 Storing {len(entities)} pipeline entities...\")\n            \n            # Debug: check section names in entities\n            section_names = [e.get('section_name', 'MISSING') for e in entities]\n            unique_sections = set(section_names)\n            print(f\"   🔍 Section names in entities: {unique_sections}\")\n            \n            # Prepare batch insert data\n            insert_data = []\n            merged_count = 0\n            \n            for entity in entities:\n                # Convert entity data for database\n                models_detected = entity.get('models_detected', [entity.get('model_source', 'unknown')])\n                if not isinstance(models_detected, list):\n                    models_detected = [str(models_detected)]\n                \n                all_confidences = entity.get('all_confidences', {entity.get('model_source', 'unknown'): entity.get('confidence_score', 0.0)})\n                entity_variations = entity.get('entity_variations', {entity.get('model_source', 'unknown'): entity.get('entity_text', '')})\n                \n                is_merged = entity.get('is_merged', False)\n                if is_merged:\n                    merged_count += 1\n                \n                # Ensure section_name is properly captured\n                section_name = entity.get('section_name', '')\n                if not section_name:\n                    # Debug missing section name\n                    print(f\"   ⚠️ Missing section name for entity: {entity.get('entity_text', 'unknown')[:30]}...\")\n                \n                insert_tuple = (\n                    entity.get('extraction_id', str(uuid.uuid4())),\n                    entity.get('company_domain', ''),\n                    entity.get('entity_text', '').strip()[:1000],\n                    entity.get('entity_type', 'UNKNOWN'),\n                    float(entity.get('confidence_score', 0.0)),\n                    int(entity.get('char_start', 0)),\n                    int(entity.get('char_end', 0)),\n                    entity.get('surrounding_text', '')[:2000] if entity.get('surrounding_text') else '',\n                    entity.get('sec_filing_ref', ''),\n                    models_detected,\n                    json.dumps(all_confidences),\n                    entity.get('primary_model', entity.get('model_source', 'unknown')),\n                    json.dumps(entity_variations),\n                    is_merged,\n                    section_name,  # Store section name properly\n                    entity.get('data_source', 'sec_filings'),\n                    entity.get('extraction_timestamp'),\n                    entity.get('original_label', '')\n                )\n                \n                insert_data.append(insert_tuple)\n            \n            # Batch insert with enhanced schema\n            insert_query = \"\"\"\n                INSERT INTO system_uno.sec_entities_raw \n                (extraction_id, company_domain, entity_text, entity_category, \n                 confidence_score, character_start, character_end, surrounding_text, \n                 sec_filing_ref, models_detected, all_confidences, primary_model,\n                 entity_variations, is_merged, section_name, data_source,\n                 extraction_timestamp, original_label)\n                VALUES %s\n                ON CONFLICT (extraction_id) DO UPDATE SET\n                    models_detected = EXCLUDED.models_detected,\n                    all_confidences = EXCLUDED.all_confidences,\n                    primary_model = EXCLUDED.primary_model,\n                    entity_variations = EXCLUDED.entity_variations,\n                    is_merged = EXCLUDED.is_merged,\n                    section_name = EXCLUDED.section_name\n            \"\"\"\n            \n            execute_values(cursor, insert_query, insert_data, page_size=100)\n            rows_affected = cursor.rowcount\n            \n            conn.commit()\n            cursor.close()\n            conn.close()\n            \n            # Update statistics\n            self.storage_stats['total_entities_stored'] += len(entities)\n            self.storage_stats['merged_entities'] += merged_count\n            self.storage_stats['single_model_entities'] += len(entities) - merged_count\n            \n            print(f\"   ✅ Stored {rows_affected} entities\")\n            print(f\"   🔗 Merged: {merged_count}, Single-model: {len(entities) - merged_count}\")\n            print(f\"   📑 Sections represented: {len(unique_sections)} unique section names\")\n            \n            return True\n            \n        except Exception as e:\n            print(f\"   ❌ Storage failed: {e}\")\n            self.storage_stats['failed_inserts'] += len(entities)\n            return False\n    \n    def get_storage_verification(self, sec_filing_ref: str) -> Dict:\n        \"\"\"Verify stored entities for a specific filing\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Enhanced verification with pipeline-specific fields\n            cursor.execute(\"\"\"\n                SELECT \n                    entity_category,\n                    COUNT(*) as total_count,\n                    AVG(confidence_score) as avg_confidence,\n                    COUNT(*) FILTER (WHERE is_merged = true) as merged_count,\n                    COUNT(DISTINCT primary_model) as models_used,\n                    COUNT(DISTINCT section_name) as sections_processed,\n                    MAX(extraction_timestamp) as latest_extraction\n                FROM system_uno.sec_entities_raw\n                WHERE sec_filing_ref = %s\n                GROUP BY entity_category\n                ORDER BY total_count DESC\n            \"\"\", (sec_filing_ref,))\n            \n            category_stats = cursor.fetchall()\n            \n            # Model performance breakdown\n            cursor.execute(\"\"\"\n                SELECT \n                    primary_model,\n                    COUNT(*) as entities,\n                    AVG(confidence_score) as avg_conf,\n                    COUNT(DISTINCT section_name) as sections\n                FROM system_uno.sec_entities_raw\n                WHERE sec_filing_ref = %s\n                GROUP BY primary_model\n                ORDER BY entities DESC\n            \"\"\", (sec_filing_ref,))\n            \n            model_stats = cursor.fetchall()\n            \n            cursor.close()\n            conn.close()\n            \n            return {\n                'filing_ref': sec_filing_ref,\n                'total_entities': sum(stat[1] for stat in category_stats),\n                'categories': [{\n                    'category': stat[0],\n                    'count': stat[1],\n                    'avg_confidence': float(stat[2]),\n                    'merged_count': stat[3],\n                    'models_used': stat[4],\n                    'sections_processed': stat[5]\n                } for stat in category_stats],\n                'model_performance': [{\n                    'model': stat[0],\n                    'entities': stat[1],\n                    'avg_confidence': float(stat[2]),\n                    'sections': stat[3]\n                } for stat in model_stats]\n            }\n            \n        except Exception as e:\n            print(f\"❌ Verification failed: {e}\")\n            return {}\n\n# Initialize enhanced storage\npipeline_storage = PipelineEntityStorage(NEON_CONFIG)\n\n# Complete pipeline processing function\ndef process_filing_with_pipeline(filing_data: Dict) -> Dict:\n    \"\"\"Process single filing through complete pipeline\"\"\"\n    try:\n        start_time = time.time()\n        \n        # Step 1: Extract sections using accession-based method\n        section_result = process_sec_filing_with_sections(filing_data)\n        \n        if section_result['processing_status'] != 'success':\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': section_result.get('error', 'Section extraction failed'),\n                'processing_time': time.time() - start_time\n            }\n        \n        # Step 2: Process sections through EntityExtractionPipeline\n        entities = entity_pipeline.process_sec_filing_sections(section_result)\n        \n        if not entities:\n            return {\n                'success': False,\n                'filing_id': filing_data.get('id'),\n                'error': 'No entities extracted',\n                'processing_time': time.time() - start_time\n            }\n        \n        # Step 3: Store entities\n        storage_success = pipeline_storage.store_pipeline_entities(entities)\n        \n        # Step 4: Get verification\n        verification = pipeline_storage.get_storage_verification(f\"SEC_{filing_data.get('id')}\")\n        \n        processing_time = time.time() - start_time\n        \n        result = {\n            'success': storage_success,\n            'filing_id': filing_data.get('id'),\n            'company_domain': filing_data.get('company_domain'),\n            'filing_type': filing_data.get('filing_type'),\n            'sections_processed': section_result.get('total_sections', 0),\n            'entities_extracted': len(entities),\n            'entities_stored': verification.get('total_entities', 0),\n            'processing_time': round(processing_time, 2),\n            'verification': verification,\n            'sample_entities': entities[:3]  # First 3 entities as sample\n        }\n        \n        return result\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'filing_id': filing_data.get('id'),\n            'error': str(e),\n            'processing_time': time.time() - start_time\n        }\n\ndef process_filings_batch(limit: int = 3) -> Dict:\n    \"\"\"Process multiple filings in batch\"\"\"\n    print(f\"🚀 Processing batch of {limit} SEC filings with complete pipeline...\")\n    \n    batch_start = time.time()\n    \n    # Get unprocessed filings\n    filings = get_unprocessed_filings(limit)\n    \n    if not filings:\n        return {'success': False, 'message': 'No filings to process'}\n    \n    print(f\"📄 Found {len(filings)} filings to process\")\n    \n    # Process each filing\n    results = []\n    successful = 0\n    total_entities = 0\n    \n    for i, filing in enumerate(filings, 1):\n        print(f\"\\n📄 [{i}/{len(filings)}] Processing {filing['filing_type']} for {filing['company_domain']}\")\n        \n        result = process_filing_with_pipeline(filing)\n        results.append(result)\n        \n        if result['success']:\n            successful += 1\n            total_entities += result.get('entities_extracted', 0)\n            print(f\"   ✅ Success: {result['entities_extracted']} entities extracted in {result['processing_time']}s\")\n            \n            # Show sample entities with section names\n            for entity in result.get('sample_entities', []):\n                models = '+'.join(entity.get('models_detected', [entity.get('model_source', 'unknown')]))\n                section = entity.get('section_name', 'NO_SECTION')\n                print(f\"      • {entity['entity_type']}: '{entity['entity_text']}' ({models}, {entity['confidence_score']:.3f}, {section})\")\n        else:\n            print(f\"   ❌ Failed: {result.get('error', 'Unknown error')}\")\n        \n        # Brief pause between filings\n        if i < len(filings):\n            time.sleep(1)\n    \n    batch_time = time.time() - batch_start\n    \n    # Update pipeline statistics\n    entity_pipeline.pipeline_stats['documents_processed'] += successful\n    entity_pipeline.pipeline_stats['total_entities_extracted'] += total_entities\n    entity_pipeline.pipeline_stats['processing_time_total'] += batch_time\n    \n    # Update storage statistics\n    pipeline_storage.storage_stats['filings_processed'] += successful\n    \n    batch_summary = {\n        'success': successful > 0,\n        'filings_processed': len(filings),\n        'successful_filings': successful,\n        'failed_filings': len(filings) - successful,\n        'total_entities_extracted': total_entities,\n        'batch_processing_time': round(batch_time, 2),\n        'avg_time_per_filing': round(batch_time / len(filings), 2),\n        'results': results\n    }\n    \n    return batch_summary\n\n# Test the complete pipeline\nprint(\"🧪 Testing complete pipeline with sample filing...\")\n\ntest_filings = get_unprocessed_filings(limit=1)\n\nif test_filings:\n    test_result = process_filing_with_pipeline(test_filings[0])\n    \n    if test_result['success']:\n        print(f\"✅ Pipeline test successful!\")\n        print(f\"   📊 Sections processed: {test_result['sections_processed']}\")\n        print(f\"   🔍 Entities extracted: {test_result['entities_extracted']}\")\n        print(f\"   💾 Entities stored: {test_result['entities_stored']}\")\n        print(f\"   ⏱️ Processing time: {test_result['processing_time']}s\")\n        \n        # Show verification details\n        if test_result['verification']:\n            print(f\"   📈 Verification:\")\n            for cat in test_result['verification']['categories'][:3]:\n                print(f\"      • {cat['category']}: {cat['count']} entities\")\n    else:\n        print(f\"❌ Pipeline test failed: {test_result.get('error')}\")\nelse:\n    print(\"📭 No test filings available\")\n\nprint(f\"\\n✅ Complete EntityExtractionPipeline with enhanced storage ready!\")\nprint(f\"🎯 Usage: batch_results = process_filings_batch(limit=5)\")\nprint(f\"📊 Features: Section extraction → NER processing → Database storage\")\nprint(f\"🔧 Enhanced: Multi-model merging, performance tracking, detailed verification\")\nprint(f\"🏷️ Fixed: Section name attribution and storage verification\")\n\n# Context Retrieval System for Relationship Extraction\nclass ContextRetrievalSystem:\n    \"\"\"System to retrieve context around entities for relationship extraction\"\"\"\n    \n    def __init__(self, db_config: Dict):\n        self.db_config = db_config\n        self.retrieval_stats = {\n            'contexts_retrieved': 0,\n            'section_fetches': 0,\n            'failed_retrievals': 0,\n            'cache_hits': 0\n        }\n        # Simple cache for section content to avoid repeated API calls\n        self._section_cache = {}\n    \n    def get_entity_context(self, entity_id: str, context_window: int = 500) -> Dict:\n        \"\"\"Get context around a specific entity using its position and section\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Get entity details with section and position\n            cursor.execute(\"\"\"\n                SELECT \n                    entity_text, section_name, character_start, character_end,\n                    sec_filing_ref, company_domain, entity_category,\n                    confidence_score, primary_model\n                FROM system_uno.sec_entities_raw\n                WHERE extraction_id = %s\n                  AND section_name IS NOT NULL \n                  AND section_name != ''\n            \"\"\", (entity_id,))\n            \n            entity_data = cursor.fetchone()\n            cursor.close()\n            conn.close()\n            \n            if not entity_data:\n                return {\n                    'success': False, \n                    'error': 'Entity not found or missing section name',\n                    'entity_id': entity_id\n                }\n            \n            entity_text, section_name, char_start, char_end, sec_filing_ref, company_domain, category, confidence, model = entity_data\n            \n            # Extract filing ID from reference\n            filing_id = sec_filing_ref.replace('SEC_', '') if sec_filing_ref else None\n            if not filing_id:\n                return {\n                    'success': False,\n                    'error': 'Could not extract filing ID',\n                    'entity_id': entity_id\n                }\n            \n            # Get filing URL to extract accession\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                SELECT url, filing_type, title\n                FROM raw_data.sec_filings\n                WHERE id = %s\n            \"\"\", (filing_id,))\n            \n            filing_info = cursor.fetchone()\n            cursor.close()\n            conn.close()\n            \n            if not filing_info:\n                return {\n                    'success': False,\n                    'error': f'Filing {filing_id} not found',\n                    'entity_id': entity_id\n                }\n            \n            filing_url, filing_type, filing_title = filing_info\n            \n            # Get section content\n            section_content = self._get_section_content(filing_url, filing_type, section_name)\n            \n            if not section_content:\n                return {\n                    'success': False,\n                    'error': f'Could not retrieve section {section_name}',\n                    'entity_id': entity_id\n                }\n            \n            # Extract context around entity position\n            context_start = max(0, char_start - context_window)\n            context_end = min(len(section_content), char_end + context_window)\n            \n            context_text = section_content[context_start:context_end]\n            \n            # Find entity position within context\n            entity_pos_in_context = char_start - context_start\n            \n            self.retrieval_stats['contexts_retrieved'] += 1\n            \n            return {\n                'success': True,\n                'entity_id': entity_id,\n                'entity_text': entity_text,\n                'entity_category': category,\n                'context_text': context_text,\n                'entity_position_in_context': entity_pos_in_context,\n                'entity_length': char_end - char_start,\n                'context_window_used': context_window,\n                'section_name': section_name,\n                'filing_info': {\n                    'filing_id': filing_id,\n                    'company_domain': company_domain,\n                    'filing_type': filing_type,\n                    'filing_title': filing_title\n                },\n                'metadata': {\n                    'confidence_score': confidence,\n                    'primary_model': model,\n                    'original_char_start': char_start,\n                    'original_char_end': char_end,\n                    'section_length': len(section_content)\n                }\n            }\n            \n        except Exception as e:\n            self.retrieval_stats['failed_retrievals'] += 1\n            return {\n                'success': False,\n                'error': str(e),\n                'entity_id': entity_id\n            }\n    \n    def _get_section_content(self, filing_url: str, filing_type: str, section_name: str) -> str:\n        \"\"\"Get section content using EdgarTools, with caching\"\"\"\n        cache_key = f\"{filing_url}#{section_name}\"\n        \n        # Check cache first\n        if cache_key in self._section_cache:\n            self.retrieval_stats['cache_hits'] += 1\n            return self._section_cache[cache_key]\n        \n        try:\n            from edgar import find\n            from edgar.documents import parse_html\n            from edgar.documents.extractors.section_extractor import SectionExtractor\n            import re\n            \n            # Extract accession from URL\n            accession_match = re.search(r'(\\d{10}-\\d{2}-\\d{6})', filing_url)\n            if not accession_match:\n                compressed_match = re.search(r'/(\\d{18})/', filing_url)\n                if compressed_match:\n                    accession = compressed_match.group(1)\n                    accession_number = f\"{accession[:10]}-{accession[10:12]}-{accession[12:]}\"\n                else:\n                    return None\n            else:\n                accession_number = accession_match.group(1)\n            \n            # Get filing and extract sections\n            filing = find(accession_number)\n            if not filing:\n                return None\n            \n            html_content = filing.html()\n            if not html_content:\n                return None\n            \n            document = parse_html(html_content)\n            extractor = SectionExtractor(filing_type=filing_type)\n            sections = extractor.extract(document)\n            \n            self.retrieval_stats['section_fetches'] += 1\n            \n            # Find the specific section\n            if section_name in sections:\n                section_obj = sections[section_name]\n                if hasattr(section_obj, 'text'):\n                    content = section_obj.text() if callable(section_obj.text) else section_obj.text\n                else:\n                    content = str(section_obj)\n                \n                # Cache the result\n                self._section_cache[cache_key] = content\n                return content\n            \n            return None\n            \n        except Exception as e:\n            print(f\"⚠️ Section retrieval failed for {section_name}: {e}\")\n            return None\n    \n    def get_co_occurring_entities(self, filing_id: str, section_name: str, distance_threshold: int = 1000) -> List[Dict]:\n        \"\"\"Get entities that co-occur within a distance threshold in the same section\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Get all entities in the same section\n            cursor.execute(\"\"\"\n                SELECT \n                    extraction_id, entity_text, entity_category, character_start, character_end,\n                    confidence_score, primary_model\n                FROM system_uno.sec_entities_raw\n                WHERE sec_filing_ref = %s\n                  AND section_name = %s\n                  AND section_name IS NOT NULL\n                ORDER BY character_start\n            \"\"\", (f'SEC_{filing_id}', section_name))\n            \n            entities = cursor.fetchall()\n            cursor.close()\n            conn.close()\n            \n            if len(entities) < 2:\n                return []\n            \n            # Find co-occurring pairs\n            co_occurrences = []\n            \n            for i in range(len(entities)):\n                for j in range(i + 1, len(entities)):\n                    entity1 = entities[i]\n                    entity2 = entities[j]\n                    \n                    # Calculate distance between entities\n                    distance = entity2[3] - entity1[4]  # start of entity2 - end of entity1\n                    \n                    if 0 <= distance <= distance_threshold:\n                        co_occurrences.append({\n                            'entity1': {\n                                'id': entity1[0],\n                                'text': entity1[1],\n                                'category': entity1[2],\n                                'start': entity1[3],\n                                'end': entity1[4],\n                                'confidence': entity1[5],\n                                'model': entity1[6]\n                            },\n                            'entity2': {\n                                'id': entity2[0],\n                                'text': entity2[1],\n                                'category': entity2[2],\n                                'start': entity2[3],\n                                'end': entity2[4],\n                                'confidence': entity2[5],\n                                'model': entity2[6]\n                            },\n                            'distance': distance,\n                            'section_name': section_name,\n                            'filing_id': filing_id\n                        })\n            \n            return co_occurrences\n            \n        except Exception as e:\n            print(f\"❌ Co-occurrence search failed: {e}\")\n            return []\n    \n    def get_context_for_entity_pair(self, entity1_id: str, entity2_id: str, context_window: int = 300) -> Dict:\n        \"\"\"Get shared context for a pair of entities\"\"\"\n        try:\n            # Get context for both entities\n            context1 = self.get_entity_context(entity1_id, context_window)\n            context2 = self.get_entity_context(entity2_id, context_window)\n            \n            if not (context1['success'] and context2['success']):\n                return {\n                    'success': False,\n                    'error': 'Could not retrieve context for one or both entities'\n                }\n            \n            # Verify they're from same section\n            if context1['section_name'] != context2['section_name']:\n                return {\n                    'success': False,\n                    'error': 'Entities are not in the same section'\n                }\n            \n            # Find overlapping context or create combined context\n            entity1_start = context1['metadata']['original_char_start']\n            entity1_end = context1['metadata']['original_char_end']\n            entity2_start = context2['metadata']['original_char_start']\n            entity2_end = context2['metadata']['original_char_end']\n            \n            # Calculate combined context boundaries\n            combined_start = min(entity1_start, entity2_start) - context_window\n            combined_end = max(entity1_end, entity2_end) + context_window\n            \n            # Get section content and extract combined context\n            filing_url_query = f\"\"\"\n                SELECT url, filing_type, title \n                FROM raw_data.sec_filings \n                WHERE id = %s\n            \"\"\"\n            \n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            cursor.execute(filing_url_query, (context1['filing_info']['filing_id'],))\n            filing_info = cursor.fetchone()\n            cursor.close()\n            conn.close()\n            \n            if not filing_info:\n                return {'success': False, 'error': 'Filing not found'}\n            \n            section_content = self._get_section_content(\n                filing_info[0], filing_info[1], context1['section_name']\n            )\n            \n            if not section_content:\n                return {'success': False, 'error': 'Section content not found'}\n            \n            # Extract combined context\n            combined_start = max(0, combined_start)\n            combined_end = min(len(section_content), combined_end)\n            combined_context = section_content[combined_start:combined_end]\n            \n            # Calculate relative positions within combined context\n            entity1_rel_start = entity1_start - combined_start\n            entity1_rel_end = entity1_end - combined_start\n            entity2_rel_start = entity2_start - combined_start\n            entity2_rel_end = entity2_end - combined_start\n            \n            return {\n                'success': True,\n                'entity1': context1['entity_text'],\n                'entity2': context2['entity_text'],\n                'combined_context': combined_context,\n                'entity1_position': {'start': entity1_rel_start, 'end': entity1_rel_end},\n                'entity2_position': {'start': entity2_rel_start, 'end': entity2_rel_end},\n                'distance_between_entities': abs(entity1_start - entity2_start),\n                'section_name': context1['section_name'],\n                'filing_info': context1['filing_info']\n            }\n            \n        except Exception as e:\n            return {\n                'success': False,\n                'error': str(e)\n            }\n    \n    def get_retrieval_statistics(self) -> Dict:\n        \"\"\"Get context retrieval system statistics\"\"\"\n        cache_size = len(self._section_cache)\n        return {\n            **self.retrieval_stats,\n            'cache_size': cache_size,\n            'cache_hit_rate': self.retrieval_stats['cache_hits'] / max(1, self.retrieval_stats['section_fetches']) * 100\n        }\n\n# Initialize context retrieval system\ncontext_retriever = ContextRetrievalSystem(NEON_CONFIG)\n\nprint(f\"\\n🔍 ContextRetrievalSystem initialized!\")\nprint(f\"   📊 Features: Entity context extraction, co-occurrence detection\")\nprint(f\"   🔧 Methods: get_entity_context(), get_co_occurring_entities(), get_context_for_entity_pair()\")\nprint(f\"   💾 Caching: Automatic section content caching to reduce API calls\")\nprint(f\"   📈 Ready for relationship extraction preparation!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Cell 6: Production Analytics and Monitoring Interface\n\ndef generate_pipeline_analytics_report() -> None:\n    \"\"\"Generate comprehensive analytics report for the pipeline\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"📊 ENTITYEXTRACTIONPIPELINE ANALYTICS DASHBOARD\")\n    print(\"=\"*80)\n    \n    # Database overview with enhanced metrics\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Enhanced database statistics\n        cursor.execute(\"\"\"\n            SELECT \n                COUNT(*) as total_entities,\n                COUNT(DISTINCT company_domain) as companies,\n                COUNT(DISTINCT sec_filing_ref) as filings,\n                COUNT(DISTINCT entity_category) as entity_types,\n                AVG(confidence_score) as avg_confidence,\n                COUNT(*) FILTER (WHERE is_merged = true) as merged_entities,\n                COUNT(DISTINCT primary_model) as active_models,\n                COUNT(DISTINCT section_name) as sections_processed,\n                COUNT(*) FILTER (WHERE section_name IS NOT NULL AND section_name != '') as entities_with_sections,\n                MAX(extraction_timestamp) as last_extraction\n            FROM system_uno.sec_entities_raw\n            WHERE data_source = 'sec_filings'\n        \"\"\")\n        \n        db_overview = cursor.fetchone()\n        \n        if db_overview and db_overview[0] > 0:\n            total_entities = db_overview[0]\n            entities_with_sections = db_overview[8]\n            section_success_rate = (entities_with_sections / total_entities * 100) if total_entities > 0 else 0\n            \n            print(f\"\\n📈 DATABASE OVERVIEW:\")\n            print(f\"   Total Entities Extracted: {db_overview[0]:,}\")\n            print(f\"   Companies Processed: {db_overview[1]:,}\")\n            print(f\"   SEC Filings Analyzed: {db_overview[2]:,}\")\n            print(f\"   Entity Categories Found: {db_overview[3]:,}\")\n            print(f\"   Average Confidence Score: {db_overview[4]:.3f}\")\n            print(f\"   Multi-Model Entities: {db_overview[5]:,} ({db_overview[5]/db_overview[0]*100:.1f}%)\")\n            print(f\"   Active Models: {db_overview[6]:,}\")\n            print(f\"   Unique Sections Found: {db_overview[7]:,}\")\n            print(f\"   🎯 SECTION SUCCESS RATE: {entities_with_sections:,}/{total_entities:,} ({section_success_rate:.1f}%)\")\n            print(f\"   Last Extraction: {db_overview[9] or 'Never'}\")\n            \n            # Alert if section success rate is low\n            if section_success_rate < 90 and total_entities > 10:\n                print(f\"   🚨 WARNING: Section success rate is {section_success_rate:.1f}% - Pipeline routing issue!\")\n            elif section_success_rate >= 90:\n                print(f\"   ✅ EXCELLENT: Section success rate is {section_success_rate:.1f}% - Pipeline working correctly!\")\n        else:\n            print(f\"\\n📈 DATABASE OVERVIEW: No entities found - database is clean for testing\")\n        \n        cursor.close()\n        conn.close()\n                \n    except Exception as e:\n        print(f\"   ❌ Could not retrieve analytics: {e}\")\n    \n    # Pipeline statistics\n    try:\n        pipeline_stats = entity_pipeline.get_pipeline_statistics()\n        \n        print(f\"\\n🔧 PIPELINE STATISTICS:\")\n        print(f\"   Documents Processed: {pipeline_stats['pipeline_stats']['documents_processed']:,}\")\n        print(f\"   Total Entities Found: {pipeline_stats['pipeline_stats']['total_entities_extracted']:,}\")\n        print(f\"   Processing Time: {pipeline_stats['pipeline_stats']['processing_time_total']:.2f}s\")\n        print(f\"   Device: {pipeline_stats['device']}\")\n        print(f\"   Loaded Models: {', '.join(pipeline_stats['loaded_models'])}\")\n        print(f\"   Supported Sources: {', '.join(pipeline_stats['supported_data_sources'])}\")\n        \n        # Individual model statistics\n        print(f\"\\n📊 INDIVIDUAL MODEL PERFORMANCE:\")\n        for model_name, stats in pipeline_stats['model_stats'].items():\n            if stats['texts_processed'] > 0:\n                entities_per_text = stats['entities_found'] / stats['texts_processed']\n                avg_time = stats['processing_time'] / stats['texts_processed']\n                print(f\"   {model_name:>12}: {stats['texts_processed']:>4} texts | {stats['entities_found']:>5} entities | {entities_per_text:>4.1f} avg/text | {avg_time:>4.2f}s avg\")\n        \n        # Storage statistics\n        storage_stats = pipeline_storage.storage_stats\n        if storage_stats['total_entities_stored'] > 0:\n            print(f\"\\n💾 STORAGE STATISTICS:\")\n            print(f\"   Entities Stored: {storage_stats['total_entities_stored']:,}\")\n            print(f\"   Filings Processed: {storage_stats['filings_processed']:,}\")\n            print(f\"   Merged Entities: {storage_stats['merged_entities']:,}\")\n            print(f\"   Single-Model Entities: {storage_stats['single_model_entities']:,}\")\n            print(f\"   Failed Inserts: {storage_stats['failed_inserts']:,}\")\n            \n            merge_rate = (storage_stats['merged_entities'] / storage_stats['total_entities_stored'] * 100) if storage_stats['total_entities_stored'] > 0 else 0\n            print(f\"   Multi-Model Detection Rate: {merge_rate:.1f}%\")\n    \n    except Exception as e:\n        print(f\"\\n🔧 Pipeline statistics unavailable: {e}\")\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"✅ EntityExtractionPipeline Analytics Complete!\")\n    print(\"=\"*80)\n\nprint(f\"\\n🎯 PRODUCTION COMMANDS:\")\nprint(f\"   • Process new filings: batch_results = process_filings_batch(limit=5)\")\nprint(f\"   • Check results:       generate_pipeline_analytics_report()\")\nprint(f\"   • View statistics:     context_retriever.get_retrieval_statistics()\")\n\nprint(f\"\\n✅ EntityExtractionPipeline Production Interface Ready!\")\nprint(f\"🔧 SINGLE ENTRY POINT: process_filings_batch() - ensures all extractions use section-based pipeline\")\nprint(f\"📊 Database cleared - ready for fresh testing with guaranteed section extraction!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}