{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# Cell -1: Minimal Logging Setup (BEFORE PACKAGE INSTALLATION)\n#\n# Purpose: Initialize basic logging system to capture Cell 0 package installation\n# This cell must run BEFORE Cell 0 to enable console log visibility\n\nimport sys\nimport psycopg2\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"🔧 Setting up minimal logging system...\")\n\n# ============================================================================\n# MINIMAL DATABASE CONNECTION FOR LOGGING\n# ============================================================================\n\n# Get database credentials\nuser_secrets = UserSecretsClient()\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"), \n    'user': user_secrets.get_secret(\"NEON_USER\"),\n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'port': 5432,\n    'sslmode': 'require'\n}\n\nprint(\"✅ Database credentials loaded for logging\")\n\n# ============================================================================\n# GLOBAL LOGGER STATE MANAGEMENT\n# ============================================================================\n\n# Track current active logger globally to prevent overlaps\n_current_logger = None\n\ndef stop_current_logging():\n    \"\"\"Properly stop and cleanup current logger\"\"\"\n    global _current_logger\n    if _current_logger:\n        sys.stdout = _current_logger.original_stdout\n        sys.stderr = _current_logger.original_stderr\n        _current_logger = None\n\n# ============================================================================\n# MINIMAL REAL-TIME CONSOLE LOGGING SYSTEM (COMPATIBLE WITH FLAIR/GLIREL)\n# ============================================================================\n\nclass RealTimeKaggleLogger:\n    \"\"\"Minimal logger for Cell 0 package installation visibility - compatible with Flair logging\"\"\"\n    def __init__(self, cell_number):\n        self.cell_number = cell_number\n        self.original_stdout = sys.stdout\n        self.original_stderr = sys.stderr\n        self._closed = False\n        \n    def write(self, text):\n        if self._closed:\n            return\n        \n        # Write to original console immediately\n        try:\n            self.original_stdout.write(text)\n            self.original_stdout.flush()\n        except (ValueError, AttributeError):\n            # Handle case where original stdout is closed\n            pass\n        \n        # Save to database (non-blocking)\n        if text.strip():  # Only log non-empty lines\n            try:\n                with psycopg2.connect(**NEON_CONFIG) as conn:\n                    with conn.cursor() as cursor:\n                        cursor.execute(\"\"\"\n                            INSERT INTO core.console_logs (cell_number, console_output) \n                            VALUES (%s, %s)\n                        \"\"\", (self.cell_number, text.strip()))\n                        conn.commit()\n            except:\n                pass  # Don't let logging errors break execution\n                \n    def flush(self):\n        if self._closed:\n            return\n        try:\n            self.original_stdout.flush()\n        except (ValueError, AttributeError):\n            pass\n    \n    def close(self):\n        \"\"\"Close method required by logging system (Flair/GLiREL compatibility)\"\"\"\n        if not self._closed:\n            self._closed = True\n            # Don't actually close stdout/stderr as they may be needed elsewhere\n    \n    def isatty(self):\n        \"\"\"Check if this is a terminal (required by some logging systems)\"\"\"\n        try:\n            return self.original_stdout.isatty()\n        except (ValueError, AttributeError):\n            return False\n\ndef start_cell_logging(cell_number):\n    \"\"\"Start clean logging for a cell (stops previous logger first)\"\"\"\n    global _current_logger\n    \n    # Clean stop of any existing logger to prevent duplicates\n    stop_current_logging()\n    \n    # Start fresh logger for this cell only\n    _current_logger = RealTimeKaggleLogger(cell_number)\n    sys.stdout = _current_logger\n    sys.stderr = _current_logger\n    \n    # Log cell start\n    print(f\"=== CELL {cell_number} START ===\")\n\n# Make functions globally available\nglobals()['start_cell_logging'] = start_cell_logging\nglobals()['stop_current_logging'] = stop_current_logging\n\nprint(\"✅ Minimal logging system initialized with Flair/GLiREL compatibility\")\n\n# ============================================================================\n# CLEAR EXISTING CONSOLE LOGS\n# ============================================================================\n\ntry:\n    with psycopg2.connect(**NEON_CONFIG) as conn:\n        with conn.cursor() as cursor:\n            cursor.execute(\"DELETE FROM core.console_logs\")\n            conn.commit()\n    print(\"🧹 Console logs cleared for fresh debugging session\")\nexcept Exception as e:\n    print(f\"⚠️ Could not clear console logs: {e}\")\n\n# ============================================================================\n# START LOGGING FOR THIS CELL\n# ============================================================================\n\nstart_cell_logging(-1)\nprint(\"✅ Cell -1 complete - Logging system ready for Cell 0 package installation\")\nprint(\"   📊 Console logs will now capture Cell 0 execution details\")\nprint(\"   🔧 Logger management: Prevents duplicates across cells\")\nprint(\"   🔗 Flair/GLiREL compatibility: Added close() and isatty() methods\")",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 0: Package Installation and Consolidated Imports\n# \n# Purpose: Install packages and consolidate all imports following PEP 8 order\n# Initialize basic logging - all other setup in Cell 1\n\n# ============================================================================\n# START LOGGING FOR THIS CELL (using Cell -1 setup)\n# ============================================================================\n\nstart_cell_logging(0)\nprint(\"🔧 Starting Cell 0 - Package installation and imports with logging enabled\")\n\n# ============================================================================\n# PACKAGE INSTALLATION\n# ============================================================================\n\nprint(\"🔧 Installing required packages with compatible versions...\")\n!pip install edgartools torch==2.6.0 transformers==4.41.0 accelerate==0.24.1 huggingface_hub requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n!pip install -U bitsandbytes --quiet\n!pip install psycopg2-binary --quiet\n!pip install gliner==0.2.5 glirel==0.1.0 --quiet\n\nprint(\"✅ All packages installed successfully with resolved dependencies\")\n\n# ============================================================================\n# CONSOLIDATED IMPORTS - ALL IMPORTS FOR THE NOTEBOOK\n# ============================================================================\n\nprint(\"📦 Starting consolidated imports...\")\n\n# Standard library imports (alphabetical order)\nimport importlib\nimport importlib.util\nimport json\nimport os\nimport pickle\nimport re\nimport signal\nimport sys\nimport time\nimport traceback\nimport uuid\nimport warnings\n\n# Standard library from imports (alphabetical order)\nfrom collections import OrderedDict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom functools import wraps\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Set, Tuple\n\nprint(\"✅ Standard library imports complete\")\n\n# Third-party imports (alphabetical order)\nimport edgar\nimport numpy as np\nimport psycopg2\nimport requests\nimport torch\n\n# Third-party from imports (alphabetical order by module)\nfrom bs4 import BeautifulSoup\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nfrom huggingface_hub import login\nfrom psycopg2 import pool\nfrom psycopg2.extras import execute_values\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoModelForTokenClassification, \n    pipeline,\n    BitsAndBytesConfig\n)\n\nprint(\"✅ Third-party imports complete\")\n\n# Environment imports\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"✅ Environment imports complete\")\n\n# ============================================================================\n# GITHUB REPOSITORY SETUP\n# ============================================================================\n\nprint(\"🔧 Setting up GitHub repository access...\")\n\n# Get GitHub token and setup repository\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n\n# Clone/update repo for module access\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\nif os.path.exists(LOCAL_PATH):\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n    print(\"✅ Repository updated\")\nelse:\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n    print(\"✅ Repository cloned\")\n\n# Add to Python path\nif f'{LOCAL_PATH}/BizIntel' not in sys.path:\n    sys.path.insert(0, f'{LOCAL_PATH}/BizIntel')\n\nprint(\"✅ GitHub repository setup complete\")\n\nprint(\"=\"*80)\nprint(\"🎉 CELL 0 COMPLETE - Package installation + imports + GitHub setup\")\nprint(\"=\"*80)\nprint(\"   📦 Standard library: importlib, json, os, pickle, re, signal, sys, time, etc.\")\nprint(\"   🔗 Third-party: edgar, numpy, psycopg2, requests, torch, transformers, bs4\")\nprint(\"   🌐 Environment: kaggle_secrets\")\nprint(\"   📂 GitHub: Repository cloned/updated and added to Python path\")\nprint(\"   🧪 GLiNER: Installed (gliner==0.2.5, glirel==0.1.0) with compatible versions\")\nprint(\"   📊 Console logging: Active and capturing all output\")\nprint(\"   ✅ Dependency conflicts resolved - torch 2.6.0, transformers 4.41.0\")",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 1: GitHub Setup and Simplified Configuration\n\n# ============================================================================\n# START LOGGING FOR THIS CELL\n# ============================================================================\n\nstart_cell_logging(1)\n\n# ============================================================================\n# GITHUB SETUP AND PATH CONFIGURATION\n# ============================================================================\n\nprint(\"🔄 Setting up GitHub repository...\")\n\n# GitHub configuration\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\n# Clone or update the repository\nif os.path.exists(LOCAL_PATH):\n    print(\"   📂 Repository exists, pulling latest changes...\")\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n    print(\"   ✅ Repository updated\")\nelse:\n    print(\"   📥 Cloning repository...\")\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n    print(\"   ✅ Repository cloned\")\n\n# Add paths for module imports\nbizintel_path = f'{LOCAL_PATH}/BizIntel'\nscripts_path = f'{LOCAL_PATH}/BizIntel/Scripts'\n\nif bizintel_path not in sys.path:\n    sys.path.insert(0, bizintel_path)\nif scripts_path not in sys.path:\n    sys.path.insert(0, scripts_path)\n\nprint(f\"   ✅ Added {bizintel_path} to Python path\")\nprint(f\"   ✅ Added {scripts_path} to Python path\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    SEC_FILINGS_PROMPT,\n    SizeLimitedLRUCache,\n    log_error,\n    log_warning, \n    log_info,\n    get_db_connection\n)\n\nprint(\"✅ Imported modular EntityExtractionEngine components\")\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION\n# ============================================================================\n\n# Neon database configuration (from secrets)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"), \n    'user': user_secrets.get_secret(\"NEON_USER\"),\n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'port': 5432,\n    'sslmode': 'require'\n}\n\n# Complete centralized configuration\nCONFIG = {\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': 'https://github.com/amiralpert/SmartReach.git',\n        'local_path': '/kaggle/working/SmartReach',\n        'branch': 'main'\n    },\n    'database': {\n        'connection_pool_size': 5,\n        'max_connections': 10,\n        'connection_timeout': 30,\n        'query_timeout': 60,\n        'retry_attempts': 3,\n        'batch_size': 100\n    },\n    'models': {\n        'confidence_threshold': 0.75,\n        'warm_up_enabled': True,\n        'warm_up_text': 'Test entity extraction with biotechnology company.',\n        'device_preference': 'auto',  # 'auto', 'cuda', 'cpu'\n        'model_timeout': 30\n    },\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 512,\n        'ttl_hours': 24,\n        'cleanup_interval': 3600\n    },\n    'processing': {\n        'filing_batch_size': 3,\n        'entity_batch_size': 50,\n        'max_section_length': 50000,\n        'enable_parallel': True,\n        'max_workers': 4,\n        'section_validation': True,\n        'filing_query_limit': 10,\n        'enable_relationships': True,\n        'relationship_batch_size': 15,\n        'context_window_chars': 400\n    },\n    'entity_extraction': {\n        'max_chunk_size': 2000,  # Conservative limit for transformer models (512 tokens)\n        'chunk_overlap': 200,     # Overlap between chunks to catch entities at boundaries\n        'max_chunks_per_section': 50,  # Limit chunks to prevent excessive processing\n        'enable_chunking': True   # Enable/disable text chunking for large documents\n    },\n    'gliner': {\n        'enabled': True,\n        'entity_model': 'urchade/gliner_medium-v2.1',\n        'relation_model': 'jackboyla/glirel-base',\n        'model_size': 'medium',  # 'small', 'medium', 'large'\n        'entity_labels': [\n            'Person', 'Filing Company', 'Private Company', 'Public Company',\n            'Government Agency', 'Date', 'Money', 'Location', 'Product',\n            'Technology', 'Financial Instrument', 'Law', 'Patent',\n            'Drug', 'Disease', 'Regulatory Body'\n        ],\n        'confidence_threshold': 0.7,\n        'relation_threshold': 0.6,\n        'enable_relationships': True,\n        'relation_types': [\n            'employed_by', 'subsidiary_of', 'owns', 'part_of',\n            'located_in', 'affiliated_with', 'contracts_with',\n            'acquired_by', 'merged_with', 'partner_of'\n        ],\n        'max_text_length': 50000,\n        'normalization': {\n            'enable_coreference': True,\n            'similarity_threshold': 0.85,\n            'company_normalization': True\n        },\n        'output': {\n            'verbose': False,\n            'save_full_text': True,\n            'include_context': True\n        }\n    },\n    'llama': {\n        'enabled': True,\n        'model_name': 'meta-llama/Llama-3.1-8B-Instruct',\n        'batch_size': 15,\n        'max_new_tokens': 50,\n        'context_window': 400,\n        'temperature': 0.3,\n        'entity_context_window': 400,\n        'test_max_tokens': 50,\n        'min_confidence_filter': 0.8,\n        'timeout_seconds': 30,\n        'SEC_FilingsPrompt': SEC_FILINGS_PROMPT,  # Now imported from module\n    },\n    'edgar': {\n        'identity': 'SmartReach BizIntel amir.alpert@gmail.com',\n        'rate_limit_delay': 0.1,\n        'max_retries': 3,\n        'timeout_seconds': 30\n    }\n}\n\n# Error checking for required secrets\nrequired_secrets = ['NEON_HOST', 'NEON_DATABASE', 'NEON_USER', 'NEON_PASSWORD', 'GITHUB_TOKEN']\nmissing_secrets = []\n\nfor secret in required_secrets:\n    try:\n        value = user_secrets.get_secret(secret)\n        if not value:\n            missing_secrets.append(secret)\n    except Exception as e:\n        missing_secrets.append(secret)\n\nif missing_secrets:\n    print(f\"❌ Missing required secrets: {missing_secrets}\")\n    print(\"   Please add these secrets in Kaggle's Settings > Secrets\")\n    raise ValueError(\"Missing required secrets\")\n\nprint(\"✅ All required secrets validated\")\n\n# Configuration validation and display\nprint(\"\\n🔧 Configuration Summary:\")\nprint(f\"   • Database: {NEON_CONFIG['host']} / {NEON_CONFIG['database']}\")\nprint(f\"   • Models: {len(['biobert', 'bert', 'roberta', 'finbert'])} NER models + Llama 3.1-8B\")\nprint(f\"   • GLiNER: {'Enabled' if CONFIG['gliner']['enabled'] else 'Disabled'} - {CONFIG['gliner']['model_size']} model\")\nprint(f\"   • Processing: {CONFIG['processing']['filing_batch_size']} filings/batch\")\nprint(f\"   • Cache: {CONFIG['cache']['max_size_mb']}MB limit\")\nprint(f\"   • Relationships: {'Enabled' if CONFIG['processing']['enable_relationships'] else 'Disabled'}\")\nprint(f\"   • Text Chunking: {CONFIG['entity_extraction']['max_chunk_size']} chars/chunk with {CONFIG['entity_extraction']['chunk_overlap']} overlap\")\n\n# ============================================================================\n# INITIALIZE COMPONENTS\n# ============================================================================\n\n# Initialize global cache for section extraction using imported class\nSECTION_CACHE = SizeLimitedLRUCache(max_size_mb=CONFIG['cache']['max_size_mb'])\n\n# Create database connection function with NEON_CONFIG\ndef get_db_connection_configured():\n    \"\"\"Database connection using our configuration\"\"\"\n    return get_db_connection(NEON_CONFIG)\n\n# ============================================================================\n# MODULE CLEARING AND EDGARTOOLS SETUP\n# ============================================================================\n\nprint(\"\\n🧹 Clearing modules and setting up EdgarTools...\")\n\n# Clear any existing modules to ensure fresh imports\nmodules_to_clear = [mod for mod in sys.modules.keys() if 'SmartReach' in mod]\nfor module in modules_to_clear:\n    del sys.modules[module]\n\n# Configure EdgarTools identity\nset_identity(CONFIG['edgar']['identity'])\nprint(f\"   ✅ EdgarTools identity set: {CONFIG['edgar']['identity']}\")\n\n# ============================================================================\n# FINAL INITIALIZATION MESSAGES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"🎉 CELL 1 INITIALIZATION COMPLETE\")\nprint(\"=\"*80)\n\nprint(f\"✅ GitHub repository ready at: {LOCAL_PATH}\")\nprint(f\"✅ Database connection configured: {NEON_CONFIG['host']}\")\nprint(f\"✅ Configuration loaded with {len(CONFIG)} main sections\")\nprint(f\"✅ Modular components imported from EntityExtractionEngine\")\nprint(f\"✅ Size-limited cache initialized: {CONFIG['cache']['max_size_mb']}MB limit\")\nprint(f\"✅ EdgarTools identity configured\")\nprint(f\"✅ Logging functions available: log_error, log_warning, log_info\")\nprint(f\"✅ Database context manager available: get_db_connection_configured()\")\nprint(f\"✅ Llama 3.1-8B relationship extraction prompt configured\")\nprint(f\"✅ GLiNER configuration: {CONFIG['gliner']['model_size']} model with {len(CONFIG['gliner']['entity_labels'])} entity types\")\nprint(f\"✅ Entity extraction chunking: {CONFIG['entity_extraction']['max_chunks_per_section']} chunks max\")\nprint(f\"✅ Console logging active from Cell -1 - using core.console_logs table\")\n\nprint(f\"\\n🚀 Ready to proceed to Cell 2 for EdgarTools section extraction!\")",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Database Functions and ORM-like Models with Batching - SIMPLIFIED\n\n# Start real-time console logging for this cell\nstart_cell_logging(2)\n\n# Basic startup check - restart kernel if issues persist\nprint(\"Starting Cell 2 - EdgarTools section extraction\")\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    TimeoutError,\n    get_filing_sections,\n    route_sections_to_models, \n    process_sec_filing_with_sections,\n    get_unprocessed_filings\n)\n\nprint(\"✅ Imported EdgarTools processing components from EntityExtractionEngine\")\n\n# ============================================================================\n# WRAPPER FUNCTIONS FOR CONFIGURED COMPONENTS\n# ============================================================================\n\ndef get_filing_sections_configured(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get filing sections using global configuration and cache\"\"\"\n    return get_filing_sections(accession_number, filing_type, SECTION_CACHE, CONFIG)\n\ndef process_sec_filing_configured(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing using global configuration and cache\"\"\"\n    return process_sec_filing_with_sections(filing_data, SECTION_CACHE, CONFIG)\n\ndef get_unprocessed_filings_configured(limit: int = 5) -> List[Dict]:\n    \"\"\"Get unprocessed filings using configured database connection\"\"\"\n    return get_unprocessed_filings(get_db_connection_configured, limit)\n\n# ============================================================================\n# EDGAR FILING EXTRACTION\n# ============================================================================\n\n# Extract sections from unprocessed SEC filings\nlog_info(\"EdgarExtraction\", \"Starting section extraction with timeout protection\")\n\nunprocessed_filings = get_unprocessed_filings_configured(limit=1)\n\nif unprocessed_filings:\n    print(f\"\\n📄 Processing filing: {unprocessed_filings[0]['company_domain']} - {unprocessed_filings[0]['filing_type']}\")\n    print(f\"   Accession: {unprocessed_filings[0]['accession_number']}\")\n    \n    filing_result = process_sec_filing_configured(unprocessed_filings[0])\n    \n    if filing_result['processing_status'] == 'success':\n        log_info(\"EdgarExtraction\", f\"✅ Successfully extracted {filing_result['total_sections']} sections\")\n    elif filing_result['processing_status'] == 'timeout':\n        log_warning(\"EdgarExtraction\", f\"⏱️ Processing timed out - filing may be too large or slow\")\n    elif filing_result['processing_status'] == 'skipped':\n        log_info(\"EdgarExtraction\", f\"⏭️ Skipped problematic filing\")\n    else:\n        log_error(\"EdgarExtraction\", f\"❌ Section extraction failed: {filing_result.get('error')}\")\nelse:\n    log_info(\"EdgarExtraction\", \"No unprocessed filings available (all may be processed or problematic)\")\n\nprint(\"✅ Cell 2 complete - EdgarTools section extraction with timeout protection ready\")",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:10:05.069606Z",
     "iopub.status.busy": "2025-09-13T18:10:05.069316Z",
     "iopub.status.idle": "2025-09-13T18:10:12.336615Z",
     "shell.execute_reply": "2025-09-13T18:10:12.336382Z",
     "shell.execute_reply.started": "2025-09-13T18:10:05.069581Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Entity Extraction Pipeline with GLiNER\n\n# Start real-time console logging for this cell\nstart_cell_logging(3)\n\nprint(\"🔬 Initializing GLiNER Entity Extraction Pipeline\")\n\n# ============================================================================\n# IMPORT GLINER COMPONENTS\n# ============================================================================\n\nfrom EntityExtractionEngine import (\n    GLiNEREntityExtractor,\n    GLiNEREntity,\n    GLiNERRelationship,\n    GLiNEREntityStorage,\n    GLiNERLlamaBridge,\n    create_gliner_storage,\n    create_gliner_llama_bridge,\n    GLINER_AVAILABLE\n)\n\nif not GLINER_AVAILABLE:\n    print(\"❌ GLiNER components not available\")\n    print(\"   Ensure Cell 0 ran successfully with GLiNER package installation\")\n    raise ImportError(\"GLiNER not available\")\n\nprint(\"✅ GLiNER components imported successfully\")\n\n# ============================================================================\n# INITIALIZE GLINER EXTRACTOR\n# ============================================================================\n\nprint(\"\\n🔧 Initializing GLiNER extractor...\")\n\ntry:\n    # Initialize GLiNER with configuration from Cell 1\n    gliner_extractor = GLiNEREntityExtractor(\n        model_size=CONFIG['gliner']['model_size'],\n        labels=CONFIG['gliner']['entity_labels'],\n        threshold=CONFIG['gliner']['confidence_threshold'],\n        enable_relationships=CONFIG['gliner']['enable_relationships'],\n        debug=CONFIG['gliner']['output']['verbose']\n    )\n    \n    print(\"🔬 Extractor: Initialized\")\n    \nexcept Exception as e:\n    print(f\"❌ Failed to initialize GLiNER: {e}\")\n    print(\"   Note: GLiNER models will be downloaded on first use (may take a moment)\")\n    # Create a lazy-loading wrapper\n    gliner_extractor = None\n\n# ============================================================================\n# INITIALIZE STORAGE COMPONENTS\n# ============================================================================\n\nprint(\"🔧 Initializing storage components...\")\n\n# Initialize GLiNER-specific storage\ngliner_storage = create_gliner_storage(NEON_CONFIG)\ngliner_bridge = create_gliner_llama_bridge(CONFIG)\n\nprint(\"💾 Storage: Ready for database and memory operations\")\n\n# ============================================================================\n# CREATE WRAPPER FOR COMPATIBILITY\n# ============================================================================\n\nclass GLiNERPipelineWrapper:\n    \"\"\"Wrapper to make GLiNER compatible with existing pipeline interface\"\"\"\n    \n    def __init__(self, extractor, storage, bridge):\n        self.extractor = extractor\n        self.storage = storage\n        self.bridge = bridge\n    \n    def extract_entities(self, text, section_name, filing_context=None):\n        \"\"\"Extract entities using GLiNER (compatible with old interface)\"\"\"\n        print(f\"🐛 Cell3 Debug - Processing section: '{section_name}'\")\n        print(f\"🐛 Cell3 Debug - Text length: {len(text)} chars\")\n        \n        if not self.extractor:\n            raise RuntimeError(\"GLiNER extractor not initialized\")\n        \n        # Use GLiNER extraction\n        result = self.extractor.extract_with_relationships(\n            text,\n            filing_context or {},\n            include_full_text=True\n        )\n        \n        # Store in memory bridge\n        if filing_context and 'accession' in filing_context:\n            self.bridge.store_gliner_results(filing_context['accession'], result)\n        \n        # Add section name to each entity record before returning\n        entity_records = result.get('entity_records', [])\n        print(f\"🐛 Cell3 Debug - Found {len(entity_records)} entities BEFORE adding section names\")\n        \n        for i, record in enumerate(entity_records):\n            record['section_name'] = section_name  # Add section name to entity record\n            print(f\"🐛 Cell3 Debug - Added section_name '{section_name}' to entity {i}: '{record.get('entity_text', 'NO_TEXT')}'\")\n        \n        print(f\"🐛 Cell3 Debug - Returning {len(entity_records)} entities WITH section names\")\n        return entity_records\n    \n    def process_filing(self, filing_data, sections):\n        \"\"\"Process entire filing with GLiNER\"\"\"\n        print(f\"🐛 Cell3 Debug - process_filing called with {len(sections)} sections: {list(sections.keys())}\")\n        \n        all_entities = []\n        \n        for section_name, section_text in sections.items():\n            print(f\"🐛 Cell3 Debug - Processing section '{section_name}' ({len(section_text)} chars)\")\n            \n            if not section_text or len(section_text.strip()) < 100:\n                print(f\"🐛 Cell3 Debug - Skipping section '{section_name}' - too short\")\n                continue\n            \n            filing_context = {\n                'accession': filing_data.get('accession_number', ''),\n                'company': filing_data.get('company_domain', ''),\n                'section': section_name,\n                'filing_type': filing_data.get('filing_type', ''),\n                'filing_date': filing_data.get('filing_date', '')\n            }\n            \n            print(f\"🐛 Cell3 Debug - About to call extract_entities for '{section_name}'\")\n            entities = self.extract_entities(section_text, section_name, filing_context)\n            print(f\"🐛 Cell3 Debug - Got {len(entities)} entities back from extract_entities\")\n            \n            all_entities.extend(entities)\n        \n        print(f\"🐛 Cell3 Debug - Total entities collected: {len(all_entities)}\")\n        return all_entities\n\n# Create the wrapper for pipeline compatibility\nentity_pipeline = GLiNERPipelineWrapper(gliner_extractor, gliner_storage, gliner_bridge)\n\nprint(\"🔗 Pipeline wrapper: Compatible with existing architecture\")\n\n# ============================================================================\n# PROCESS SEC FILING SECTIONS FROM CELL 2\n# ============================================================================\n\n# Use actual variables from Cell 2\nif 'filing_result' in globals() and filing_result.get('processing_status') == 'success':\n    \n    # Get actual SEC sections extracted by EdgarTools\n    sections = filing_result['sections']\n    filing_data = unprocessed_filings[0] if unprocessed_filings else {}\n    \n    print(f\"\\n🔍 Processing {len(sections)} SEC filing sections with GLiNER+GLiREL...\")\n    \n    # Process sections with GLiNER entity extraction + GLiREL relationship extraction  \n    all_entities = entity_pipeline.process_filing(filing_data, sections)\n    \n    # Store results in database\n    gliner_storage.store_gliner_entities({\n        'entity_records': all_entities,\n        'filing': filing_data  \n    }, filing_data)\n    \n    print(f\"✅ Processing complete: {len(all_entities)} entities stored\")\n\nelse:\n    print(\"⚠️ No SEC filing sections available from Cell 2\")\n\nprint(\"\\n✅ Cell 3 complete - GLiNER entity extraction pipeline ready\")\nprint(\"📝 Usage: entity_pipeline.extract_entities(text, section, context)\")",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Relationship Extractor with Local Llama 3.1-8B - MODULARIZED\n\n# Start real-time console logging for this cell\nstart_cell_logging(5)\n\nprint(\"🦙 Loading Relationship Extractor with Local Llama 3.1-8B...\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    RelationshipExtractor,\n    SemanticRelationshipStorage,\n    PipelineEntityStorage,\n    process_filings_batch,\n    generate_pipeline_analytics_report\n)\n\nprint(\"✅ Imported relationship processing components from EntityExtractionEngine\")\n\n# ============================================================================\n# INITIALIZE GLOBAL OBJECTS\n# ============================================================================\n\nprint(\"🔧 Initializing pipeline components...\")\n\n# Initialize relationship extraction and storage components\nrelationship_extractor = RelationshipExtractor(CONFIG)\nsemantic_storage = SemanticRelationshipStorage(CONFIG['database'])\npipeline_storage = PipelineEntityStorage(CONFIG['database'])\n\nprint(\"✅ Pipeline components initialized:\")\nprint(f\"   🦙 Llama model status: {'✅ Loaded' if relationship_extractor.model else '❌ Failed'}\")\nprint(f\"   💾 Storage systems: ✅ Entity & ✅ Relationship storage initialized\")\n\n# ============================================================================\n# WRAPPER FUNCTIONS FOR CONFIGURED PROCESSING\n# ============================================================================\n\ndef process_filings_batch_configured(limit: int = None) -> Dict:\n    \"\"\"Process multiple SEC filings using configured pipeline components\"\"\"\n    return process_filings_batch(\n        entity_pipeline, relationship_extractor, pipeline_storage, \n        semantic_storage, CONFIG, limit\n    )\n\nprint(\"✅ Cell 5 complete - Relationship extraction and storage ready\")\nprint(f\"   🎯 Batch processing: process_filings_batch_configured() function ready\")\nprint(f\"   📊 Analytics: generate_pipeline_analytics_report() function ready\")",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Main Processing Pipeline with Relationship Extraction - MODULARIZED\n\n# Start real-time console logging for this cell\nstart_cell_logging(6)\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import execute_main_pipeline\n\nprint(\"✅ Imported main pipeline orchestrator from EntityExtractionEngine\")\n\n# ============================================================================\n# EXECUTE MAIN PIPELINE\n# ============================================================================\n\n# Execute the complete SEC filing processing pipeline\nresults = execute_main_pipeline(\n    entity_pipeline, \n    relationship_extractor, \n    pipeline_storage, \n    semantic_storage, \n    CONFIG\n)\n\nprint(\"✅ Cell 6 complete - Main pipeline execution finished\")",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:14:18.239404Z",
     "iopub.status.busy": "2025-09-13T18:14:18.239084Z"
    },
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}