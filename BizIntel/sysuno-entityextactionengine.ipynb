{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# Cell 6: Batch Processing and Monitoring Dashboard\n\ndef run_batch_extraction(batch_size: int = 5, max_filings: int = None) -> Dict[str, Any]:\n    \"\"\"Run batch entity extraction on multiple SEC filings\"\"\"\n    \n    print(f\"üöÄ Starting batch entity extraction...\")\n    print(f\"   üì¶ Batch size: {batch_size}\")\n    print(f\"   üìä Max filings: {max_filings or 'unlimited'}\")\n    \n    # Get filings to process\n    filings_to_process = fetcher.get_filings_to_process(limit=max_filings or 100)\n    \n    if not filings_to_process:\n        print(\"üì≠ No unprocessed filings found\")\n        return {'success': False, 'message': 'No filings to process'}\n    \n    print(f\"   üìÑ Found {len(filings_to_process)} filings to process\")\n    \n    # Initialize batch tracking\n    batch_results = {\n        'total_filings': len(filings_to_process),\n        'successful_filings': 0,\n        'failed_filings': 0,\n        'total_entities_extracted': 0,\n        'processing_start_time': datetime.now(),\n        'results': []\n    }\n    \n    # Process filings in batches\n    for i in range(0, len(filings_to_process), batch_size):\n        batch_filings = filings_to_process[i:i + batch_size]\n        batch_number = (i // batch_size) + 1\n        total_batches = (len(filings_to_process) + batch_size - 1) // batch_size\n        \n        print(f\\\"\\\\nüì¶ Processing batch {batch_number}/{total_batches} ({len(batch_filings)} filings)\\\")\\n        \\n        for filing in batch_filings:\\n            try:\\n                result = process_single_filing(filing)\\n                \\n                if result['success']:\\n                    batch_results['successful_filings'] += 1\\n                    batch_results['total_entities_extracted'] += result['entities_extracted']\\n                    print(f\\\"   ‚úÖ {filing['company_domain']} - {filing['filing_type']}: {result['entities_extracted']} entities\\\")\\n                else:\\n                    batch_results['failed_filings'] += 1\\n                    print(f\\\"   ‚ùå {filing['company_domain']} - {filing['filing_type']}: {result.get('error', 'Unknown error')}\\\")\\n                \\n                batch_results['results'].append(result)\\n                \\n            except Exception as e:\\n                batch_results['failed_filings'] += 1\\n                error_result = {\\n                    'success': False,\\n                    'filing_id': filing['id'],\\n                    'company_domain': filing['company_domain'],\\n                    'filing_type': filing['filing_type'],\\n                    'error': str(e)\\n                }\\n                batch_results['results'].append(error_result)\\n                print(f\\\"   ‚ùå {filing['company_domain']} - {filing['filing_type']}: Exception: {e}\\\")\\n        \\n        # Short delay between batches\\n        if i + batch_size < len(filings_to_process):\\n            print(\\\"   ‚è∏Ô∏è Brief pause between batches...\\\")\\n            time.sleep(2)\\n    \\n    # Finalize results\\n    batch_results['processing_end_time'] = datetime.now()\\n    batch_results['total_processing_time'] = str(batch_results['processing_end_time'] - batch_results['processing_start_time'])\\n    batch_results['success_rate'] = batch_results['successful_filings'] / batch_results['total_filings'] if batch_results['total_filings'] > 0 else 0\\n    \\n    return batch_results\\n\\ndef generate_extraction_report(batch_results: Dict[str, Any] = None) -> None:\\n    \\\"\\\"\\\"Generate comprehensive extraction report\\\"\\\"\\\"\\n    print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n    print(\\\"üìä SEC ENTITY EXTRACTION REPORT\\\")\\n    print(\\\"=\\\"*60)\\n    \\n    # Current database status\\n    try:\\n        conn = psycopg2.connect(**NEON_CONFIG)\\n        cursor = conn.cursor()\\n        \\n        cursor.execute('''\\n            SELECT \\n                COUNT(*) as total_entities,\\n                COUNT(DISTINCT company_domain) as companies_processed,\\n                COUNT(DISTINCT sec_filing_ref) as filings_processed,\\n                COUNT(DISTINCT entity_category) as entity_types,\\n                AVG(confidence_score) as avg_confidence\\n            FROM system_uno.sec_entities_raw\\n        ''')\\n        \\n        db_stats = cursor.fetchone()\\n        \\n        cursor.execute('''\\n            SELECT entity_category, COUNT(*) as count, AVG(confidence_score) as avg_conf\\n            FROM system_uno.sec_entities_raw\\n            GROUP BY entity_category\\n            ORDER BY count DESC\\n            LIMIT 10\\n        ''')\\n        \\n        entity_breakdown = cursor.fetchall()\\n        cursor.close()\\n        conn.close()\\n        \\n        print(f\\\"\\\\nüìà DATABASE STATISTICS:\\\")\\n        print(f\\\"   Total Entities Extracted: {db_stats[0]:,}\\\")\\n        print(f\\\"   Companies Processed: {db_stats[1]}\\\")\\n        print(f\\\"   SEC Filings Processed: {db_stats[2]}\\\")\\n        print(f\\\"   Entity Types Found: {db_stats[3]}\\\")\\n        print(f\\\"   Average Confidence: {db_stats[4]:.3f}\\\")\\n        \\n        print(f\\\"\\\\nüè∑Ô∏è ENTITY TYPE BREAKDOWN:\\\")\\n        for entity_type in entity_breakdown:\\n            print(f\\\"   {entity_type[0]}: {entity_type[1]:,} entities (avg conf: {entity_type[2]:.3f})\\\")\\n            \\n    except Exception as e:\\n        print(f\\\"   ‚ùå Could not retrieve database statistics: {e}\\\")\\n    \\n    # Batch processing results\\n    if batch_results:\\n        print(f\\\"\\\\n‚ö° BATCH PROCESSING RESULTS:\\\")\\n        print(f\\\"   Total Filings Processed: {batch_results['total_filings']}\\\")\\n        print(f\\\"   Successful: {batch_results['successful_filings']} ({batch_results['success_rate']*100:.1f}%)\\\")\\n        print(f\\\"   Failed: {batch_results['failed_filings']}\\\")\\n        print(f\\\"   Total Entities Extracted: {batch_results['total_entities_extracted']:,}\\\")\\n        print(f\\\"   Processing Time: {batch_results['total_processing_time']}\\\")\\n    \\n    # Extractor statistics\\n    if extractor:\\n        extractor_stats = extractor.get_extraction_summary()\\n        print(f\\\"\\\\nüîç EXTRACTION STATISTICS:\\\")\\n        print(f\\\"   API Calls Made: {extractor_stats['extraction_stats']['api_calls_made']:,}\\\")\\n        print(f\\\"   Chunks Processed: {extractor_stats['extraction_stats']['total_chunks_processed']:,}\\\")\\n        print(f\\\"   Entities Found: {extractor_stats['extraction_stats']['total_entities_found']:,}\\\")\\n        print(f\\\"   Errors: {extractor_stats['extraction_stats']['errors']}\\\")\\n    \\n    # Storage statistics\\n    storage_stats = storage.get_storage_summary()\\n    print(f\\\"\\\\nüíæ STORAGE STATISTICS:\\\")\\n    print(f\\\"   Entities Stored: {storage_stats['successful_inserts']:,}\\\")\\n    print(f\\\"   Duplicates Skipped: {storage_stats['duplicate_entities']:,}\\\")\\n    print(f\\\"   Failed Inserts: {storage_stats['failed_inserts']:,}\\\")\\n    \\n    print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n    print(\\\"‚úÖ Report generated successfully!\\\")\\n    print(\\\"=\\\"*60)\\n\\n# Generate initial report\\ngenerate_extraction_report()\\n\\n# Ready for batch processing\\nprint(\\\"\\\\nüéØ BATCH PROCESSING COMMANDS:\\\")\\nprint(\\\"   ‚Ä¢ To process 5 filings: batch_results = run_batch_extraction(batch_size=5, max_filings=5)\\\")\\nprint(\\\"   ‚Ä¢ To process 10 filings: batch_results = run_batch_extraction(batch_size=3, max_filings=10)\\\")\\nprint(\\\"   ‚Ä¢ To generate new report: generate_extraction_report(batch_results)\\\")\\nprint(\\\"\\\\n‚úÖ SEC Entity Extraction Engine fully operational!\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 5: Store Extraction Results in system_uno.sec_entities_raw\n\nclass SECEntityStorage:\n    \"\"\"Store extracted entities in the Neon database\"\"\"\n    \n    def __init__(self, db_config):\n        self.db_config = db_config\n        self.storage_stats = {\n            'total_entities_stored': 0,\n            'successful_inserts': 0,\n            'failed_inserts': 0,\n            'duplicate_entities': 0\n        }\n    \n    def store_entities(self, entities: List[Dict[str, Any]]) -> bool:\n        \"\"\"Store a list of entities in the database\"\"\"\n        if not entities:\n            print(\"‚ö†Ô∏è No entities to store\")\n            return True\n        \n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            print(f\"üíæ Storing {len(entities)} entities in database...\")\n            \n            # Prepare batch insert data\n            insert_data = []\n            for entity in entities:\n                insert_data.append((\n                    entity['extraction_id'],\n                    entity['company_domain'],\n                    entity['entity_text'],\n                    entity['entity_category'],\n                    entity['confidence_score'],\n                    entity['character_start'],\n                    entity['character_end'],\n                    entity['surrounding_text'],\n                    entity['sec_filing_ref']\n                ))\n            \n            # Batch insert using execute_values for better performance\n            insert_query = '''\n                INSERT INTO system_uno.sec_entities_raw \n                (extraction_id, company_domain, entity_text, entity_category, \n                 confidence_score, character_start, character_end, surrounding_text, sec_filing_ref)\n                VALUES %s\n                ON CONFLICT (extraction_id) DO NOTHING\n            '''\n            \n            from psycopg2.extras import execute_values\n            execute_values(cursor, insert_query, insert_data, page_size=100)\n            \n            # Get number of rows actually inserted\n            rows_inserted = cursor.rowcount\n            \n            conn.commit()\n            cursor.close()\n            conn.close()\n            \n            # Update statistics\n            self.storage_stats['total_entities_stored'] += len(entities)\n            self.storage_stats['successful_inserts'] += rows_inserted\n            self.storage_stats['duplicate_entities'] += len(entities) - rows_inserted\n            \n            print(f\"   ‚úì Stored {rows_inserted} entities ({len(entities) - rows_inserted} duplicates skipped)\")\n            return True\n            \n        except Exception as e:\n            print(f\"   ‚úó Storage failed: {e}\")\n            self.storage_stats['failed_inserts'] += len(entities)\n            return False\n    \n    def get_storage_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of storage operations\"\"\"\n        return self.storage_stats\n    \n    def verify_storage(self, sec_filing_ref: str) -> Dict[str, Any]:\n        \"\"\"Verify entities were stored correctly for a filing\"\"\"\n        try:\n            conn = psycopg2.connect(**self.db_config)\n            cursor = conn.cursor()\n            \n            # Count entities by category for this filing\n            cursor.execute('''\n                SELECT entity_category, COUNT(*) as count, AVG(confidence_score) as avg_confidence\n                FROM system_uno.sec_entities_raw\n                WHERE sec_filing_ref = %s\n                GROUP BY entity_category\n                ORDER BY count DESC\n            ''', (sec_filing_ref,))\n            \n            results = cursor.fetchall()\n            cursor.close()\n            conn.close()\n            \n            verification = {\n                'total_entities': sum(result[1] for result in results),\n                'entity_breakdown': [\n                    {\n                        'category': result[0],\n                        'count': result[1],\n                        'avg_confidence': float(result[2])\n                    } for result in results\n                ]\n            }\n            \n            return verification\n            \n        except Exception as e:\n            print(f\"   ‚úó Verification failed: {e}\")\n            return {}\n\n# Initialize storage handler\nstorage = SECEntityStorage(NEON_CONFIG)\nprint(\"‚úì SEC Entity Storage initialized!\")\n\n# Function to process a single filing end-to-end\ndef process_single_filing(filing_data: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Complete processing pipeline for a single SEC filing\"\"\"\n    print(f\"\\nüöÄ Processing filing: {filing_data['company_domain']} - {filing_data['filing_type']}\")\n    print(f\"   üìÖ Date: {filing_data['filing_date']}\")\n    print(f\"   üîó URL: {filing_data['url']}\")\n    \n    # Step 1: Fetch content\n    content = fetcher.fetch_filing_content(filing_data['url'])\n    if not content:\n        return {'success': False, 'error': 'Failed to fetch content'}\n    \n    # Step 2: Extract entities\n    if not extractor:\n        return {'success': False, 'error': 'Entity extractor not available'}\n    \n    entities = extractor.extract_entities_from_filing(filing_data, content)\n    if not entities:\n        return {'success': False, 'error': 'No entities extracted'}\n    \n    # Step 3: Store entities\n    success = storage.store_entities(entities)\n    if not success:\n        return {'success': False, 'error': 'Failed to store entities'}\n    \n    # Step 4: Verify storage\n    sec_filing_ref = f\"SEC_{filing_data['id']}\"\n    verification = storage.verify_storage(sec_filing_ref)\n    \n    result = {\n        'success': True,\n        'filing_id': filing_data['id'],\n        'company_domain': filing_data['company_domain'],\n        'filing_type': filing_data['filing_type'],\n        'content_length': len(content),\n        'entities_extracted': len(entities),\n        'verification': verification\n    }\n    \n    print(f\"   ‚úÖ Successfully processed {len(entities)} entities\")\n    return result\n\n# Test processing a single filing\nif 'test_filing' in locals() and test_filing:\n    print(\"\\nüß™ Testing complete processing pipeline...\")\n    test_result = process_single_filing(test_filing)\n    \n    if test_result['success']:\n        print(f\"   üìä Results: {test_result['entities_extracted']} entities extracted\")\n        if test_result['verification']:\n            for entity_type in test_result['verification']['entity_breakdown']:\n                print(f\"      {entity_type['category']}: {entity_type['count']} (avg confidence: {entity_type['avg_confidence']:.3f})\")\n    else:\n        print(f\"   ‚ùå Test failed: {test_result.get('error', 'Unknown error')}\")\n\nprint(\"\\n‚úÖ Entity Storage Pipeline ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 4: Entity Extraction Pipeline Using Microsoft Biomed NLP\n\nclass SECEntityExtractor:\n    \"\"\"Extract biomedical entities from SEC filings using Azure Text Analytics\"\"\"\n    \n    def __init__(self, text_analytics_client):\n        self.client = text_analytics_client\n        self.extraction_stats = {\n            'total_chunks_processed': 0,\n            'total_entities_found': 0,\n            'api_calls_made': 0,\n            'errors': 0\n        }\n    \n    def extract_entities_from_text(self, text: str, chunk_offset: int = 0) -> List[Dict[str, Any]]:\n        \"\"\"Extract entities from a single text chunk using Microsoft Biomed NLP\"\"\"\n        if not self.client:\n            print(\"‚ö†Ô∏è Azure Text Analytics client not available\")\n            return []\n        \n        try:\n            # Call Azure Text Analytics for Health\n            documents = [text]\n            result = self.client.analyze_healthcare_entities(documents)\n            \n            self.extraction_stats['api_calls_made'] += 1\n            \n            entities = []\n            \n            # Process the results\n            for doc_result in result:\n                if doc_result.is_error:\n                    print(f\"‚úó API Error: {doc_result.error}\")\n                    self.extraction_stats['errors'] += 1\n                    continue\n                \n                for entity in doc_result.entities:\n                    # Extract surrounding context\n                    start_pos = max(0, entity.offset - EXTRACTION_CONFIG['context_window'])\n                    end_pos = min(len(text), entity.offset + entity.length + EXTRACTION_CONFIG['context_window'])\n                    surrounding_text = text[start_pos:end_pos]\n                    \n                    entity_data = {\n                        'entity_text': entity.text,\n                        'entity_category': entity.category,\n                        'confidence_score': entity.confidence_score,\n                        'character_start': chunk_offset + entity.offset,\n                        'character_end': chunk_offset + entity.offset + entity.length,\n                        'surrounding_text': surrounding_text,\n                        'subcategory': getattr(entity, 'subcategory', None),\n                        'assertion': getattr(entity, 'assertion', None)\n                    }\n                    entities.append(entity_data)\n                    self.extraction_stats['total_entities_found'] += 1\n            \n            self.extraction_stats['total_chunks_processed'] += 1\n            \n            # Rate limiting\n            time.sleep(EXTRACTION_CONFIG['rate_limit_delay'])\n            \n            return entities\n            \n        except Exception as e:\n            print(f\"‚úó Entity extraction failed: {e}\")\n            self.extraction_stats['errors'] += 1\n            return []\n    \n    def extract_entities_from_filing(self, filing_data: Dict[str, Any], filing_content: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract entities from a complete SEC filing\"\"\"\n        print(f\"üîç Extracting entities from {filing_data['company_domain']} - {filing_data['filing_type']}\")\n        \n        # Split content into chunks\n        chunks = fetcher.chunk_text(filing_content, EXTRACTION_CONFIG['max_text_length'])\n        print(f\"   üìÑ Processing {len(chunks)} chunks...\")\n        \n        all_entities = []\n        \n        for i, chunk in enumerate(chunks):\n            print(f\"   üî¢ Chunk {i+1}/{len(chunks)} ({len(chunk['text'])} chars)\")\n            \n            entities = self.extract_entities_from_text(chunk['text'], chunk['start'])\n            \n            # Add filing metadata to each entity\n            for entity in entities:\n                entity.update({\n                    'filing_id': filing_data['id'],\n                    'company_domain': filing_data['company_domain'],\n                    'filing_type': filing_data['filing_type'],\n                    'filing_date': filing_data['filing_date'],\n                    'sec_filing_ref': f\"SEC_{filing_data['id']}\",\n                    'extraction_id': str(uuid.uuid4()),\n                    'chunk_number': i,\n                    'total_chunks': len(chunks)\n                })\n            \n            all_entities.extend(entities)\n            \n            print(f\"      Found {len(entities)} entities\")\n        \n        print(f\"   ‚úì Total entities found: {len(all_entities)}\")\n        return all_entities\n    \n    def get_extraction_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary statistics of extraction process\"\"\"\n        return {\n            'extraction_stats': self.extraction_stats,\n            'supported_entity_types': EXTRACTION_CONFIG['supported_entities'],\n            'config': EXTRACTION_CONFIG\n        }\n\n# Initialize entity extractor\nif text_analytics_client:\n    extractor = SECEntityExtractor(text_analytics_client)\n    print(\"‚úì SEC Entity Extractor initialized!\")\n    \n    # Test extraction on sample content if we have it\n    if 'content' in locals() and content:\n        print(\"\\nüß™ Testing entity extraction...\")\n        sample_text = content[:1000] if len(content) > 1000 else content\n        test_entities = extractor.extract_entities_from_text(sample_text)\n        \n        print(f\"   üìä Found {len(test_entities)} entities in sample text\")\n        \n        # Show sample entities\n        for entity in test_entities[:3]:\n            print(f\"   üîç {entity['entity_category']}: '{entity['entity_text']}' (confidence: {entity['confidence_score']:.3f})\")\n    \nelse:\n    print(\"‚ùå Cannot initialize entity extractor - Azure client not available\")\n    extractor = None\n\nprint(\"\\n‚úÖ Entity Extraction Pipeline ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: SEC Filing URL Fetching and Text Extraction\n\nclass SECFilingFetcher:\n    \"\"\"Fetches and processes SEC filing content from EDGAR URLs\"\"\"\n    \n    def __init__(self):\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'SmartReach BizIntel Entity Extraction (contact@smartreach.com)',\n            'Accept-Encoding': 'gzip, deflate',\n            'Host': 'www.sec.gov'\n        })\n        \n    def fetch_filing_content(self, url: str) -> Optional[str]:\n        \"\"\"Fetch and extract text content from SEC filing URL\"\"\"\n        try:\n            print(f\"üì• Fetching: {url}\")\n            \n            # Add delay to respect SEC rate limits\n            time.sleep(0.1)\n            \n            response = self.session.get(url, timeout=30)\n            response.raise_for_status()\n            \n            # Parse HTML content\n            soup = BeautifulSoup(response.content, 'lxml')\n            \n            # Remove script and style elements\n            for script in soup([\"script\", \"style\"]):\n                script.extract()\n                \n            # Extract text content\n            text = soup.get_text()\n            \n            # Clean up whitespace\n            lines = (line.strip() for line in text.splitlines())\n            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n            text = ' '.join(chunk for chunk in chunks if chunk)\n            \n            print(f\"  ‚úì Extracted {len(text):,} characters\")\n            return text\n            \n        except requests.RequestException as e:\n            print(f\"  ‚úó Request failed: {e}\")\n            return None\n        except Exception as e:\n            print(f\"  ‚úó Processing failed: {e}\")\n            return None\n    \n    def get_filings_to_process(self, limit: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Get SEC filings from database that need processing\"\"\"\n        try:\n            conn = psycopg2.connect(**NEON_CONFIG)\n            cursor = conn.cursor()\n            \n            # Get filings that haven't been processed yet\n            cursor.execute('''\n                SELECT sf.id, sf.company_domain, sf.filing_type, sf.url, sf.filing_date, sf.title\n                FROM raw_data.sec_filings sf\n                LEFT JOIN system_uno.sec_entities_raw ser ON ser.sec_filing_ref = CONCAT('SEC_', sf.id)\n                WHERE sf.url IS NOT NULL \n                AND ser.sec_filing_ref IS NULL\n                ORDER BY sf.filing_date DESC\n                LIMIT %s\n            ''', (limit,))\n            \n            filings = cursor.fetchall()\n            cursor.close()\n            conn.close()\n            \n            return [{\n                'id': filing[0],\n                'company_domain': filing[1],\n                'filing_type': filing[2],\n                'url': filing[3],\n                'filing_date': filing[4],\n                'title': filing[5]\n            } for filing in filings]\n            \n        except Exception as e:\n            print(f\"‚úó Database query failed: {e}\")\n            return []\n    \n    def chunk_text(self, text: str, max_length: int = 5120, overlap: int = 200) -> List[Dict[str, Any]]:\n        \"\"\"Split text into chunks that fit Azure API limits\"\"\"\n        if len(text) <= max_length:\n            return [{'text': text, 'start': 0, 'end': len(text)}]\n        \n        chunks = []\n        start = 0\n        \n        while start < len(text):\n            end = min(start + max_length, len(text))\n            \n            # Try to break at sentence boundary\n            if end < len(text):\n                last_period = text.rfind('.', start, end)\n                if last_period > start + max_length // 2:\n                    end = last_period + 1\n            \n            chunk_text = text[start:end].strip()\n            if chunk_text:\n                chunks.append({\n                    'text': chunk_text,\n                    'start': start,\n                    'end': end\n                })\n            \n            start = max(end - overlap, start + 1)\n        \n        return chunks\n\n# Initialize the fetcher\nfetcher = SECFilingFetcher()\n\n# Test fetching a single filing\ntest_filings = fetcher.get_filings_to_process(limit=1)\nif test_filings:\n    test_filing = test_filings[0]\n    print(f\"üß™ Testing with filing: {test_filing['company_domain']} - {test_filing['filing_type']}\")\n    print(f\"   URL: {test_filing['url']}\")\n    \n    content = fetcher.fetch_filing_content(test_filing['url'])\n    if content:\n        chunks = fetcher.chunk_text(content)\n        print(f\"   üìÑ Content: {len(content):,} characters\")\n        print(f\"   üî¢ Chunks: {len(chunks)} pieces\")\n        print(f\"   üìù First 200 chars: {content[:200]}...\")\n    else:\n        print(\"   ‚ùå Failed to fetch content\")\nelse:\n    print(\"üì≠ No unprocessed filings found\")\n\nprint(\"\\n‚úÖ SEC Filing Fetcher ready!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Cell 0: GitHub Setup and Auto-Logging\n\nimport os\nimport sys\nimport importlib\nimport importlib.util\nimport psycopg2\n\n# GitHub credentials - use Kaggle secrets for security\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\ngithub_token = user_secrets.get_secret(\"GITHUB_TOKEN\")\nrepo_url = f\"https://{github_token}@github.com/amiralpert/SmartReach.git\"\nlocal_path = \"/kaggle/working/SmartReach\"\n\nprint(\"üì¶ Setting up GitHub repository...\")\n\n# Clone or update repo with force pull\nif os.path.exists(local_path):\n    print(f\"üìÇ Repository exists at {local_path}\")\n    print(\"üîÑ Force updating from GitHub...\")\n    !cd {local_path} && git fetch origin\n    !cd {local_path} && git reset --hard origin/main\n    !cd {local_path} && git pull origin main\n    print(\"‚úÖ Repository updated\")\n    \n    # Show current commit\n    !cd {local_path} && echo \"Current commit:\" && git log --oneline -1\nelse:\n    print(f\"üì• Cloning repository to {local_path}\")\n    !git clone {repo_url} {local_path}\n    print(\"‚úÖ Repository cloned\")\n\n# Clear any cached modules from previous runs\nmodules_to_clear = [key for key in sys.modules.keys() if 'sec_' in key.lower() or 'entity' in key.lower()]\nfor mod in modules_to_clear:\n    del sys.modules[mod]\n    print(f\"  Cleared cached module: {mod}\")\n\n# Add to Python path for regular imports\nif f'{local_path}/BizIntel' in sys.path:\n    sys.path.remove(f'{local_path}/BizIntel')\nsys.path.insert(0, f'{local_path}/BizIntel')\n\nprint(\"‚úì Python path configured for SEC entity extraction!\")\n\n# Set up database configuration\nNEON_CONFIG = {\n    'host': 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech',\n    'database': 'BizIntelSmartReach',\n    'user': 'neondb_owner',\n    'password': 'npg_aTFt6Pug3Kpy',\n    'sslmode': 'require'\n}\n\n# Try to set up logger, but don't fail if there are issues\ntry:\n    # Create separate connection for logger\n    logger_conn = psycopg2.connect(**NEON_CONFIG)\n    print(\"‚úì Database connected for logger\")\n\n    # Import auto-logger using direct file import\n    logger_module_path = f\"{local_path}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\n    if os.path.exists(logger_module_path):\n        spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_module_path)\n        auto_logger_module = importlib.util.module_from_spec(spec)\n        sys.modules[\"auto_logger\"] = auto_logger_module\n        spec.loader.exec_module(auto_logger_module)\n\n        setup_auto_logging = auto_logger_module.setup_auto_logging\n        logger = setup_auto_logging(logger_conn, \"SEC_EntityExtraction\")\n        print(\"‚úì Auto-logging enabled!\")\n    else:\n        print(f\"‚úó Auto-logger not found at {logger_module_path}\")\n        logger = None\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Logger setup failed: {e}\")\n    print(\"  Continuing without auto-logging...\")\n    logger = None\n\nprint(\"\\n‚úÖ Setup complete. SEC Entity Extraction Engine ready to use.\")",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 1: Neon Database Configuration\nNEON_CONFIG = {\n    'host': 'ep-royal-star-ad1gn0d4-pooler.c-2.us-east-1.aws.neon.tech',\n    'database': 'BizIntelSmartReach',\n    'user': 'neondb_owner',\n    'password': 'npg_aTFt6Pug3Kpy',\n    'sslmode': 'require'\n}\n\n# Test database connection\ndef test_database_connection():\n    try:\n        conn = psycopg2.connect(**NEON_CONFIG)\n        cursor = conn.cursor()\n        \n        # Check SEC-related tables\n        cursor.execute('''\n            SELECT \n                (SELECT COUNT(*) FROM raw_data.sec_filings) as sec_filings,\n                (SELECT COUNT(*) FROM core.companies) as companies,\n                (SELECT COUNT(*) FROM system_uno.sec_entities_raw) as sec_entities_extracted,\n                (SELECT COUNT(DISTINCT company_domain) FROM raw_data.sec_filings) as companies_with_filings,\n                (SELECT COUNT(*) FROM raw_data.sec_filings WHERE url IS NOT NULL) as filings_with_urls\n        ''')\n        \n        counts = cursor.fetchone()\n        print(\"‚úì Database connected successfully!\")\n        print(f\"  SEC Filings: {counts[0]}\")\n        print(f\"  Companies: {counts[1]}\")\n        print(f\"  Extracted SEC Entities: {counts[2]}\")\n        print(f\"  Companies with SEC Filings: {counts[3]}\")\n        print(f\"  SEC Filings with URLs: {counts[4]}\")\n        \n        # Show sample SEC filing data\n        cursor.execute('''\n            SELECT company_domain, filing_type, COUNT(*) as count\n            FROM raw_data.sec_filings \n            GROUP BY company_domain, filing_type \n            ORDER BY company_domain, count DESC\n            LIMIT 10\n        ''')\n        \n        filing_stats = cursor.fetchall()\n        print(\"\\nüìä SEC Filing Distribution:\")\n        for stat in filing_stats:\n            print(f\"  {stat[0]}: {stat[1]} ({stat[2]} filings)\")\n        \n        cursor.close()\n        conn.close()\n        return True\n        \n    except Exception as e:\n        print(f\"‚úó Database connection failed: {e}\")\n        return False\n\n# Test connection\ntest_database_connection()",
   "metadata": {
    "trusted": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Cell 2: Microsoft Biomed NLP Setup and Dependencies\n\n# Install required packages\n!pip install azure-ai-textanalytics==5.3.0 requests beautifulsoup4 lxml uuid\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport uuid\nimport json\nimport time\nfrom datetime import datetime\nfrom typing import List, Dict, Any, Optional\nimport re\n\n# Azure Text Analytics for Health (Microsoft Biomed NLP)\nfrom azure.ai.textanalytics import TextAnalyticsClient\nfrom azure.core.credentials import AzureKeyCredential\n\nprint(\"üì¶ Packages installed successfully!\")\n\n# Azure credentials - use Kaggle secrets for security\ntry:\n    azure_key = user_secrets.get_secret(\"AZURE_TEXT_ANALYTICS_KEY\")\n    azure_endpoint = user_secrets.get_secret(\"AZURE_TEXT_ANALYTICS_ENDPOINT\")\n    \n    # Initialize Text Analytics client\n    credential = AzureKeyCredential(azure_key)\n    text_analytics_client = TextAnalyticsClient(\n        endpoint=azure_endpoint,\n        credential=credential\n    )\n    \n    print(\"‚úì Azure Text Analytics client initialized!\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Azure setup failed: {e}\")\n    print(\"  Please ensure AZURE_TEXT_ANALYTICS_KEY and AZURE_TEXT_ANALYTICS_ENDPOINT are set in Kaggle secrets\")\n    text_analytics_client = None\n\n# Configuration for entity extraction\nEXTRACTION_CONFIG = {\n    'max_text_length': 5120,  # Azure Text Analytics limit\n    'context_window': 500,    # Characters before/after entity for context\n    'batch_size': 10,         # Process 10 documents at a time\n    'rate_limit_delay': 1.0,  # Seconds between API calls\n    'supported_entities': [\n        'MEDICATION', 'MEDICAL_CONDITION', 'TREATMENT', \n        'EXAMINATION', 'BODY_STRUCTURE', 'HEALTHCARE_PROFESSION',\n        'DOSAGE', 'ROUTE_OR_MODE', 'FREQUENCY'\n    ]\n}\n\nprint(f\"üìã Configuration loaded:\")\nprint(f\"  Max text length: {EXTRACTION_CONFIG['max_text_length']} characters\")\nprint(f\"  Context window: {EXTRACTION_CONFIG['context_window']} characters\")\nprint(f\"  Batch size: {EXTRACTION_CONFIG['batch_size']} documents\")\nprint(f\"  Supported entities: {len(EXTRACTION_CONFIG['supported_entities'])} types\")\n\nprint(\"\\n‚úÖ Microsoft Biomed NLP setup complete!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}