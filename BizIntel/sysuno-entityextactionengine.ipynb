{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 0: Consolidated Imports and Auto-Logger Bootstrap\n",
    "# \n",
    "# Purpose: All imports consolidated here following PEP 8 order\n",
    "# Initialize basic logging - all other setup in Cell 1\n",
    "\n",
    "# ============================================================================\n",
    "# CONSOLIDATED IMPORTS - ALL IMPORTS FOR THE NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library imports (alphabetical order)\n",
    "import importlib\n",
    "import importlib.util\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import signal\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "import uuid\n",
    "import warnings\n",
    "\n",
    "# Standard library from imports (alphabetical order)\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Any, Set, Tuple\n",
    "\n",
    "# Third-party imports (alphabetical order)\n",
    "import edgar\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# Third-party from imports (alphabetical order by module)\n",
    "from bs4 import BeautifulSoup\n",
    "from edgar import Filing, find, set_identity, Company\n",
    "from edgar.documents import parse_html\n",
    "from edgar.documents.extractors.section_extractor import SectionExtractor\n",
    "from huggingface_hub import login\n",
    "from IPython import get_ipython\n",
    "from ipykernel.iostream import OutStream\n",
    "from psycopg2 import pool\n",
    "from psycopg2.extras import execute_values\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# Environment imports\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-LOGGER BOOTSTRAP (USING CONSOLIDATED IMPORTS)\n",
    "# ============================================================================\n",
    "\n",
    "# Get GitHub token for logger access\n",
    "user_secrets = UserSecretsClient()\n",
    "GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "\n",
    "print(\"🔧 Setting up consolidated imports and logger bootstrap...\")\n",
    "\n",
    "# Clone/update repo for logger access\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\n",
    "LOCAL_PATH = \"/kaggle/working/SmartReach\"\n",
    "\n",
    "if os.path.exists(LOCAL_PATH):\n",
    "    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n",
    "else:\n",
    "    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n",
    "\n",
    "# Add to path\n",
    "if f'{LOCAL_PATH}/BizIntel' not in sys.path:\n",
    "    sys.path.insert(0, f'{LOCAL_PATH}/BizIntel')\n",
    "\n",
    "# Initialize logger with minimal setup\n",
    "logger_path = f\"{LOCAL_PATH}/BizIntel/Scripts/KaggleLogger/auto_logger.py\"\n",
    "if os.path.exists(logger_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"auto_logger\", logger_path)\n",
    "    auto_logger = importlib.util.module_from_spec(spec)\n",
    "    sys.modules[\"auto_logger\"] = auto_logger\n",
    "    spec.loader.exec_module(auto_logger)\n",
    "    \n",
    "    # Simple logger setup - database manager will be provided by Cell 1\n",
    "    logger = None  # Will be properly initialized after Cell 1 runs\n",
    "    print(\"✅ Auto-logger module loaded\")\n",
    "else:\n",
    "    logger = None\n",
    "    print(\"⚠️  Logger module not found - continuing without logging\")\n",
    "\n",
    "print(\"✅ Cell 0: All imports consolidated (33+ imports) + bootstrap complete\")\n",
    "print(\"   📦 Standard library: importlib, json, os, pickle, re, signal, sys, time, etc.\")\n",
    "print(\"   🔗 Third-party: edgar, numpy, psycopg2, requests, torch, transformers, bs4\")\n",
    "print(\"   🌐 Environment: kaggle_secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 1: GitHub Setup and Simplified Configuration\n",
    "\n",
    "# Install required packages first\n",
    "!pip install edgartools transformers torch accelerate huggingface_hub requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n",
    "!pip install -U bitsandbytes --quiet\n",
    "\n",
    "print(\"🔧 Installing additional packages...\")\n",
    "!pip install psycopg2-binary --quiet\n",
    "!pip install accelerate --quiet\n",
    "\n",
    "print(\"✅ All packages installed successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# GITHUB SETUP AND PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🔄 Setting up GitHub repository...\")\n",
    "\n",
    "# GitHub configuration\n",
    "user_secrets = UserSecretsClient()\n",
    "GITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n",
    "REPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\n",
    "LOCAL_PATH = \"/kaggle/working/SmartReach\"\n",
    "\n",
    "# Clone or update the repository\n",
    "if os.path.exists(LOCAL_PATH):\n",
    "    print(\"   📂 Repository exists, pulling latest changes...\")\n",
    "    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n",
    "    print(\"   ✅ Repository updated\")\n",
    "else:\n",
    "    print(\"   📥 Cloning repository...\")\n",
    "    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n",
    "    print(\"   ✅ Repository cloned\")\n",
    "\n",
    "# Add paths for module imports\n",
    "bizintel_path = f'{LOCAL_PATH}/BizIntel'\n",
    "scripts_path = f'{LOCAL_PATH}/BizIntel/Scripts'\n",
    "\n",
    "if bizintel_path not in sys.path:\n",
    "    sys.path.insert(0, bizintel_path)\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.insert(0, scripts_path)\n",
    "\n",
    "print(f\"   ✅ Added {bizintel_path} to Python path\")\n",
    "print(f\"   ✅ Added {scripts_path} to Python path\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT MODULAR COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Import from our modular EntityExtractionEngine\n",
    "from EntityExtractionEngine import (\n",
    "    SEC_FILINGS_PROMPT,\n",
    "    SizeLimitedLRUCache,\n",
    "    log_error,\n",
    "    log_warning, \n",
    "    log_info,\n",
    "    get_db_connection\n",
    ")\n",
    "\n",
    "print(\"✅ Imported modular EntityExtractionEngine components\")\n",
    "\n",
    "# ============================================================================\n",
    "# CENTRALIZED CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Neon database configuration (from secrets)\n",
    "NEON_CONFIG = {\n",
    "    'host': user_secrets.get_secret(\"NEON_HOST\"),\n",
    "    'database': user_secrets.get_secret(\"NEON_DATABASE\"), \n",
    "    'user': user_secrets.get_secret(\"NEON_USER\"),\n",
    "    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n",
    "    'port': 5432,\n",
    "    'sslmode': 'require'\n",
    "}\n",
    "\n",
    "# Complete centralized configuration\n",
    "CONFIG = {\n",
    "    'github': {\n",
    "        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n",
    "        'repo_url': 'https://github.com/amiralpert/SmartReach.git',\n",
    "        'local_path': '/kaggle/working/SmartReach',\n",
    "        'branch': 'main'\n",
    "    },\n",
    "    'database': {\n",
    "        'connection_pool_size': 5,\n",
    "        'max_connections': 10,\n",
    "        'connection_timeout': 30,\n",
    "        'query_timeout': 60,\n",
    "        'retry_attempts': 3,\n",
    "        'batch_size': 100\n",
    "    },\n",
    "    'models': {\n",
    "        'confidence_threshold': 0.75,\n",
    "        'warm_up_enabled': True,\n",
    "        'warm_up_text': 'Test entity extraction with biotechnology company.',\n",
    "        'device_preference': 'auto',  # 'auto', 'cuda', 'cpu'\n",
    "        'model_timeout': 30\n",
    "    },\n",
    "    'cache': {\n",
    "        'enabled': True,\n",
    "        'max_size_mb': 512,\n",
    "        'ttl_hours': 24,\n",
    "        'cleanup_interval': 3600\n",
    "    },\n",
    "    'processing': {\n",
    "        'filing_batch_size': 3,\n",
    "        'entity_batch_size': 50,\n",
    "        'max_section_length': 50000,\n",
    "        'enable_parallel': True,\n",
    "        'max_workers': 4,\n",
    "        'section_validation': True,\n",
    "        'filing_query_limit': 10,\n",
    "        'enable_relationships': True,\n",
    "        'relationship_batch_size': 15,\n",
    "        'context_window_chars': 400\n",
    "    },\n",
    "    'llama': {\n",
    "        'enabled': True,\n",
    "        'model_name': 'meta-llama/Llama-3.1-8B-Instruct',\n",
    "        'batch_size': 15,\n",
    "        'max_new_tokens': 50,\n",
    "        'context_window': 400,\n",
    "        'temperature': 0.3,\n",
    "        'entity_context_window': 400,\n",
    "        'test_max_tokens': 50,\n",
    "        'min_confidence_filter': 0.8,\n",
    "        'timeout_seconds': 30,\n",
    "        'SEC_FilingsPrompt': SEC_FILINGS_PROMPT,  # Now imported from module\n",
    "    },\n",
    "    'edgar': {\n",
    "        'identity': 'SmartReach BizIntel amir.alpert@gmail.com',\n",
    "        'rate_limit_delay': 0.1,\n",
    "        'max_retries': 3,\n",
    "        'timeout_seconds': 30\n",
    "    }\n",
    "}\n",
    "\n",
    "# Error checking for required secrets\n",
    "required_secrets = ['NEON_HOST', 'NEON_DATABASE', 'NEON_USER', 'NEON_PASSWORD', 'GITHUB_TOKEN']\n",
    "missing_secrets = []\n",
    "\n",
    "for secret in required_secrets:\n",
    "    try:\n",
    "        value = user_secrets.get_secret(secret)\n",
    "        if not value:\n",
    "            missing_secrets.append(secret)\n",
    "    except Exception as e:\n",
    "        missing_secrets.append(secret)\n",
    "\n",
    "if missing_secrets:\n",
    "    print(f\"❌ Missing required secrets: {missing_secrets}\")\n",
    "    print(\"   Please add these secrets in Kaggle's Settings > Secrets\")\n",
    "    raise ValueError(\"Missing required secrets\")\n",
    "\n",
    "print(\"✅ All required secrets validated\")\n",
    "\n",
    "# Configuration validation and display\n",
    "print(\"\\n🔧 Configuration Summary:\")\n",
    "print(f\"   • Database: {NEON_CONFIG['host']} / {NEON_CONFIG['database']}\")\n",
    "print(f\"   • Models: {len(['biobert', 'bert', 'roberta', 'finbert'])} NER models + Llama 3.1-8B\")\n",
    "print(f\"   • Processing: {CONFIG['processing']['filing_batch_size']} filings/batch\")\n",
    "print(f\"   • Cache: {CONFIG['cache']['max_size_mb']}MB limit\")\n",
    "print(f\"   • Relationships: {'Enabled' if CONFIG['processing']['enable_relationships'] else 'Disabled'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize global cache for section extraction using imported class\n",
    "SECTION_CACHE = SizeLimitedLRUCache(max_size_mb=CONFIG['cache']['max_size_mb'])\n",
    "\n",
    "# Create database connection function with NEON_CONFIG\n",
    "def get_db_connection_configured():\n",
    "    \"\"\"Database connection using our configuration\"\"\"\n",
    "    return get_db_connection(NEON_CONFIG)\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE CLEARING AND EDGARTOOLS SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n🧹 Clearing modules and setting up EdgarTools...\")\n",
    "\n",
    "# Clear any existing modules to ensure fresh imports\n",
    "modules_to_clear = [mod for mod in sys.modules.keys() if 'SmartReach' in mod]\n",
    "for module in modules_to_clear:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Configure EdgarTools identity\n",
    "set_identity(CONFIG['edgar']['identity'])\n",
    "print(f\"   ✅ EdgarTools identity set: {CONFIG['edgar']['identity']}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL INITIALIZATION MESSAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 CELL 1 INITIALIZATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"✅ GitHub repository ready at: {LOCAL_PATH}\")\n",
    "print(f\"✅ Database connection configured: {NEON_CONFIG['host']}\")\n",
    "print(f\"✅ Configuration loaded with {len(CONFIG)} main sections\")\n",
    "print(f\"✅ Modular components imported from EntityExtractionEngine\")\n",
    "print(f\"✅ Size-limited cache initialized: {CONFIG['cache']['max_size_mb']}MB limit\")\n",
    "print(f\"✅ EdgarTools identity configured\")\n",
    "print(f\"✅ Logging functions available: log_error, log_warning, log_info\")\n",
    "print(f\"✅ Database context manager available: get_db_connection_configured()\")\n",
    "print(f\"✅ Llama 3.1-8B relationship extraction prompt configured\")\n",
    "\n",
    "print(f\"\\n🚀 Ready to proceed to Cell 2 for EdgarTools section extraction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:10:05.069606Z",
     "iopub.status.busy": "2025-09-13T18:10:05.069316Z",
     "iopub.status.idle": "2025-09-13T18:10:12.336615Z",
     "shell.execute_reply": "2025-09-13T18:10:12.336382Z",
     "shell.execute_reply.started": "2025-09-13T18:10:05.069581Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 2: Database Functions and ORM-like Models with Batching - SIMPLIFIED\n",
    "\n",
    "# Basic startup check - restart kernel if issues persist\n",
    "print(\"Starting Cell 2 - EdgarTools section extraction\")\n",
    "\n",
    "# Ensure identity is set\n",
    "set_identity(CONFIG['edgar']['identity'])\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT MODULAR COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Import from our modular EntityExtractionEngine\n",
    "from EntityExtractionEngine import (\n",
    "    TimeoutError,\n",
    "    get_filing_sections,\n",
    "    route_sections_to_models, \n",
    "    process_sec_filing_with_sections,\n",
    "    get_unprocessed_filings\n",
    ")\n",
    "\n",
    "print(\"✅ Imported EdgarTools processing components from EntityExtractionEngine\")\n",
    "\n",
    "# ============================================================================\n",
    "# WRAPPER FUNCTIONS FOR CONFIGURED COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "def get_filing_sections_configured(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n",
    "    \"\"\"Get filing sections using global configuration and cache\"\"\"\n",
    "    return get_filing_sections(accession_number, filing_type, SECTION_CACHE, CONFIG)\n",
    "\n",
    "def process_sec_filing_configured(filing_data: Dict) -> Dict:\n",
    "    \"\"\"Process SEC filing using global configuration and cache\"\"\"\n",
    "    return process_sec_filing_with_sections(filing_data, SECTION_CACHE, CONFIG)\n",
    "\n",
    "def get_unprocessed_filings_configured(limit: int = 5) -> List[Dict]:\n",
    "    \"\"\"Get unprocessed filings using configured database connection\"\"\"\n",
    "    return get_unprocessed_filings(get_db_connection_configured, limit)\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "# Test the simplified extraction with timeout protection\n",
    "log_info(\"Test\", \"Starting section extraction test with timeout protection\")\n",
    "\n",
    "test_filings = get_unprocessed_filings_configured(limit=1)\n",
    "\n",
    "if test_filings:\n",
    "    print(f\"\\n🧪 Testing with filing: {test_filings[0]['company_domain']} - {test_filings[0]['filing_type']}\")\n",
    "    print(f\"   Accession: {test_filings[0]['accession_number']}\")\n",
    "    \n",
    "    test_result = process_sec_filing_configured(test_filings[0])\n",
    "    \n",
    "    if test_result['processing_status'] == 'success':\n",
    "        log_info(\"Test\", f\"✅ Successfully extracted {test_result['total_sections']} sections\")\n",
    "    elif test_result['processing_status'] == 'timeout':\n",
    "        log_warning(\"Test\", f\"⏱️ Processing timed out - filing may be too large or slow\")\n",
    "    elif test_result['processing_status'] == 'skipped':\n",
    "        log_info(\"Test\", f\"⏭️ Skipped problematic filing\")\n",
    "    else:\n",
    "        log_error(\"Test\", f\"❌ Section extraction failed: {test_result.get('error')}\")\n",
    "else:\n",
    "    log_info(\"Test\", \"No test filings available (all may be processed or problematic)\")\n",
    "\n",
    "print(\"✅ Cell 2 complete - EdgarTools section extraction with timeout protection ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:10:12.338823Z",
     "iopub.status.busy": "2025-09-13T18:10:12.338593Z",
     "iopub.status.idle": "2025-09-13T18:10:58.440614Z",
     "shell.execute_reply": "2025-09-13T18:10:58.440277Z",
     "shell.execute_reply.started": "2025-09-13T18:10:12.338803Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 3: Optimized Entity Extraction Pipeline - Uses Cell 2's Pre-Extracted Sections\n",
    "\n",
    "print(\"🚀 Loading Optimized EntityExtractionPipeline (Handler Classes Eliminated)...\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT MODULAR COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Import from our modular EntityExtractionEngine\n",
    "from EntityExtractionEngine import EntityExtractionPipeline\n",
    "\n",
    "print(\"✅ Imported EntityExtractionPipeline from EntityExtractionEngine\")\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize the entity extraction pipeline\n",
    "entity_pipeline = EntityExtractionPipeline(CONFIG)\n",
    "\n",
    "print(f\"✅ EntityExtractionPipeline initialized:\")\n",
    "stats = entity_pipeline.get_extraction_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   • {key}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# WRAPPER FUNCTION FOR CONFIGURED PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_filing_entities_configured(filing_data: Dict) -> List[Dict]:\n",
    "    \"\"\"Process filing entities using configured pipeline and Cell 2 functions\"\"\"\n",
    "    return entity_pipeline.process_filing_entities(filing_data, process_sec_filing_configured)\n",
    "\n",
    "print(\"✅ Cell 3 complete - Optimized entity extraction ready (handler classes eliminated)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 4: Relationship Extractor with Local Llama 3.1-8B - MODULARIZED\n",
    "\n",
    "print(\"🦙 Loading Relationship Extractor with Local Llama 3.1-8B...\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT MODULAR COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Import from our modular EntityExtractionEngine\n",
    "from EntityExtractionEngine import (\n",
    "    RelationshipExtractor,\n",
    "    SemanticRelationshipStorage,\n",
    "    PipelineEntityStorage,\n",
    "    process_filings_batch,\n",
    "    generate_pipeline_analytics_report\n",
    ")\n",
    "\n",
    "print(\"✅ Imported relationship processing components from EntityExtractionEngine\")\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE GLOBAL OBJECTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🔧 Initializing pipeline components...\")\n",
    "\n",
    "# Initialize relationship extraction and storage components\n",
    "relationship_extractor = RelationshipExtractor(CONFIG)\n",
    "semantic_storage = SemanticRelationshipStorage(CONFIG['database'])\n",
    "pipeline_storage = PipelineEntityStorage(CONFIG['database'])\n",
    "\n",
    "print(\"✅ Pipeline components initialized:\")\n",
    "print(f\"   🦙 Llama model status: {'✅ Loaded' if relationship_extractor.model else '❌ Failed'}\")\n",
    "print(f\"   💾 Storage systems: ✅ Entity & ✅ Relationship storage initialized\")\n",
    "\n",
    "# ============================================================================\n",
    "# WRAPPER FUNCTIONS FOR CONFIGURED PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_filings_batch_configured(limit: int = None) -> Dict:\n",
    "    \"\"\"Process multiple SEC filings using configured pipeline components\"\"\"\n",
    "    return process_filings_batch(\n",
    "        entity_pipeline, relationship_extractor, pipeline_storage, \n",
    "        semantic_storage, CONFIG, limit\n",
    "    )\n",
    "\n",
    "print(\"✅ Cell 4 complete - Relationship extraction and storage ready\")\n",
    "print(f\"   🎯 Batch processing: process_filings_batch_configured() function ready\")\n",
    "print(f\"   📊 Analytics: generate_pipeline_analytics_report() function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:14:18.239404Z",
     "iopub.status.busy": "2025-09-13T18:14:18.239084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cell 5: Main Processing Pipeline with Relationship Extraction - MODULARIZED\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORT MODULAR COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "# Import from our modular EntityExtractionEngine\n",
    "from EntityExtractionEngine import execute_main_pipeline\n",
    "\n",
    "print(\"✅ Imported main pipeline orchestrator from EntityExtractionEngine\")\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE MAIN PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "# Execute the complete SEC filing processing pipeline\n",
    "results = execute_main_pipeline(\n",
    "    entity_pipeline, \n",
    "    relationship_extractor, \n",
    "    pipeline_storage, \n",
    "    semantic_storage, \n",
    "    CONFIG\n",
    ")\n",
    "\n",
    "print(\"✅ Cell 5 complete - Main pipeline execution finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
