{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Cell -1: Minimal Logging Setup (BEFORE PACKAGE INSTALLATION)\n#\n# Purpose: Initialize basic logging system to capture Cell 0 package installation\n# This cell must run BEFORE Cell 0 to enable console log visibility\n\nimport sys\nimport psycopg2\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"ðŸ”§ Setting up minimal logging system...\")\n\n# ============================================================================\n# MINIMAL DATABASE CONNECTION FOR LOGGING\n# ============================================================================\n\n# Get database credentials\nuser_secrets = UserSecretsClient()\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"), \n    'user': user_secrets.get_secret(\"NEON_USER\"),\n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'port': 5432,\n    'sslmode': 'require'\n}\n\nprint(\"âœ… Database credentials loaded for logging\")\n\n# ============================================================================\n# GLOBAL LOGGER STATE MANAGEMENT\n# ============================================================================\n\n# Track current active logger globally to prevent overlaps\n_current_logger = None\n\ndef stop_current_logging():\n    \"\"\"Properly stop and cleanup current logger\"\"\"\n    global _current_logger\n    if _current_logger:\n        sys.stdout = _current_logger.original_stdout\n        sys.stderr = _current_logger.original_stderr\n        _current_logger = None\n\n# ============================================================================\n# MINIMAL REAL-TIME CONSOLE LOGGING SYSTEM\n# ============================================================================\n\nclass RealTimeKaggleLogger:\n    \"\"\"Minimal logger for Cell 0 package installation visibility\"\"\"\n    def __init__(self, cell_number):\n        self.cell_number = cell_number\n        self.original_stdout = sys.stdout\n        self.original_stderr = sys.stderr\n        \n    def write(self, text):\n        # Write to original console immediately\n        self.original_stdout.write(text)\n        self.original_stdout.flush()\n        \n        # Save to database (non-blocking)\n        if text.strip():  # Only log non-empty lines\n            try:\n                with psycopg2.connect(**NEON_CONFIG) as conn:\n                    with conn.cursor() as cursor:\n                        cursor.execute(\"\"\"\n                            INSERT INTO core.console_logs (cell_number, console_output) \n                            VALUES (%s, %s)\n                        \"\"\", (self.cell_number, text.strip()))\n                        conn.commit()\n            except:\n                pass  # Don't let logging errors break execution\n                \n    def flush(self):\n        self.original_stdout.flush()\n\ndef start_cell_logging(cell_number):\n    \"\"\"Start clean logging for a cell (stops previous logger first)\"\"\"\n    global _current_logger\n    \n    # Clean stop of any existing logger to prevent duplicates\n    stop_current_logging()\n    \n    # Start fresh logger for this cell only\n    _current_logger = RealTimeKaggleLogger(cell_number)\n    sys.stdout = _current_logger\n    sys.stderr = _current_logger\n    \n    # Log cell start\n    print(f\"=== CELL {cell_number} START ===\")\n\n# Make functions globally available\nglobals()['start_cell_logging'] = start_cell_logging\nglobals()['stop_current_logging'] = stop_current_logging\n\nprint(\"âœ… Minimal logging system initialized with proper logger management\")\n\n# ============================================================================\n# CLEAR EXISTING CONSOLE LOGS\n# ============================================================================\n\ntry:\n    with psycopg2.connect(**NEON_CONFIG) as conn:\n        with conn.cursor() as cursor:\n            cursor.execute(\"DELETE FROM core.console_logs\")\n            conn.commit()\n    print(\"ðŸ§¹ Console logs cleared for fresh debugging session\")\nexcept Exception as e:\n    print(f\"âš ï¸ Could not clear console logs: {e}\")\n\n# ============================================================================\n# START LOGGING FOR THIS CELL\n# ============================================================================\n\nstart_cell_logging(-1)\nprint(\"âœ… Cell -1 complete - Logging system ready for Cell 0 package installation\")\nprint(\"   ðŸ“Š Console logs will now capture Cell 0 execution details\")\nprint(\"   ðŸ”§ Logger management: Prevents duplicates across cells\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_0",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 0: Package Installation and Consolidated Imports\n# \n# Purpose: Install packages and consolidate all imports following PEP 8 order\n# Initialize basic logging - all other setup in Cell 1\n\n# ============================================================================\n# START LOGGING FOR THIS CELL (using Cell -1 setup)\n# ============================================================================\n\nstart_cell_logging(0)\nprint(\"ðŸ”§ Starting Cell 0 - Package installation and imports with logging enabled\")\n\n# ============================================================================\n# PACKAGE INSTALLATION\n# ============================================================================\n\nprint(\"ðŸ”§ Installing required packages with compatible versions...\")\n!pip install edgartools torch==2.6.0 transformers==4.41.0 accelerate==0.24.1 huggingface_hub requests beautifulsoup4 'lxml[html_clean]' uuid numpy newspaper3k --quiet\n!pip install -U bitsandbytes --quiet\n!pip install psycopg2-binary --quiet\n!pip install gliner==0.2.5 glirel==0.1.0 --quiet\n\nprint(\"âœ… All packages installed successfully with resolved dependencies\")\n\n# ============================================================================\n# CONSOLIDATED IMPORTS - ALL IMPORTS FOR THE NOTEBOOK\n# ============================================================================\n\nprint(\"ðŸ“¦ Starting consolidated imports...\")\n\n# Standard library imports (alphabetical order)\nimport importlib\nimport importlib.util\nimport json\nimport os\nimport pickle\nimport re\nimport signal\nimport sys\nimport time\nimport traceback\nimport uuid\nimport warnings\n\n# Standard library from imports (alphabetical order)\nfrom collections import OrderedDict\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom contextlib import contextmanager\nfrom datetime import datetime\nfrom functools import wraps\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Set, Tuple\n\nprint(\"âœ… Standard library imports complete\")\n\n# Third-party imports (alphabetical order)\nimport edgar\nimport numpy as np\nimport psycopg2\nimport requests\nimport torch\n\n# Third-party from imports (alphabetical order by module)\nfrom bs4 import BeautifulSoup\nfrom edgar import Filing, find, set_identity, Company\nfrom edgar.documents import parse_html\nfrom edgar.documents.extractors.section_extractor import SectionExtractor\nfrom huggingface_hub import login\nfrom psycopg2 import pool\nfrom psycopg2.extras import execute_values\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    AutoModelForTokenClassification, \n    pipeline,\n    BitsAndBytesConfig\n)\n\nprint(\"âœ… Third-party imports complete\")\n\n# Environment imports\nfrom kaggle_secrets import UserSecretsClient\n\nprint(\"âœ… Environment imports complete\")\n\n# ============================================================================\n# GITHUB REPOSITORY SETUP\n# ============================================================================\n\nprint(\"ðŸ”§ Setting up GitHub repository access...\")\n\n# Get GitHub token and setup repository\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\n\n# Clone/update repo for module access\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\nif os.path.exists(LOCAL_PATH):\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n    print(\"âœ… Repository updated\")\nelse:\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n    print(\"âœ… Repository cloned\")\n\n# Add to Python path\nif f'{LOCAL_PATH}/BizIntel' not in sys.path:\n    sys.path.insert(0, f'{LOCAL_PATH}/BizIntel')\n\nprint(\"âœ… GitHub repository setup complete\")\n\nprint(\"=\"*80)\nprint(\"ðŸŽ‰ CELL 0 COMPLETE - Package installation + imports + GitHub setup\")\nprint(\"=\"*80)\nprint(\"   ðŸ“¦ Standard library: importlib, json, os, pickle, re, signal, sys, time, etc.\")\nprint(\"   ðŸ”— Third-party: edgar, numpy, psycopg2, requests, torch, transformers, bs4\")\nprint(\"   ðŸŒ Environment: kaggle_secrets\")\nprint(\"   ðŸ“‚ GitHub: Repository cloned/updated and added to Python path\")\nprint(\"   ðŸ§ª GLiNER: Installed (gliner==0.2.5, glirel==0.1.0) with compatible versions\")\nprint(\"   ðŸ“Š Console logging: Active and capturing all output\")\nprint(\"   âœ… Dependency conflicts resolved - torch 2.6.0, transformers 4.41.0\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_1",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 1: GitHub Setup and Simplified Configuration\n\n# ============================================================================\n# START LOGGING FOR THIS CELL\n# ============================================================================\n\nstart_cell_logging(1)\n\n# ============================================================================\n# GITHUB SETUP AND PATH CONFIGURATION\n# ============================================================================\n\nprint(\"ðŸ”„ Setting up GitHub repository...\")\n\n# GitHub configuration\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\")\nREPO_URL = f\"https://{GITHUB_TOKEN}@github.com/amiralpert/SmartReach.git\"\nLOCAL_PATH = \"/kaggle/working/SmartReach\"\n\n# Clone or update the repository\nif os.path.exists(LOCAL_PATH):\n    print(\"   ðŸ“‚ Repository exists, pulling latest changes...\")\n    !cd {LOCAL_PATH} && git pull origin main > /dev/null 2>&1\n    print(\"   âœ… Repository updated\")\nelse:\n    print(\"   ðŸ“¥ Cloning repository...\")\n    !git clone {REPO_URL} {LOCAL_PATH} > /dev/null 2>&1\n    print(\"   âœ… Repository cloned\")\n\n# Add paths for module imports\nbizintel_path = f'{LOCAL_PATH}/BizIntel'\nscripts_path = f'{LOCAL_PATH}/BizIntel/Scripts'\n\nif bizintel_path not in sys.path:\n    sys.path.insert(0, bizintel_path)\nif scripts_path not in sys.path:\n    sys.path.insert(0, scripts_path)\n\nprint(f\"   âœ… Added {bizintel_path} to Python path\")\nprint(f\"   âœ… Added {scripts_path} to Python path\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    SEC_FILINGS_PROMPT,\n    SizeLimitedLRUCache,\n    log_error,\n    log_warning, \n    log_info,\n    get_db_connection\n)\n\nprint(\"âœ… Imported modular EntityExtractionEngine components\")\n\n# ============================================================================\n# CENTRALIZED CONFIGURATION\n# ============================================================================\n\n# Neon database configuration (from secrets)\nNEON_CONFIG = {\n    'host': user_secrets.get_secret(\"NEON_HOST\"),\n    'database': user_secrets.get_secret(\"NEON_DATABASE\"), \n    'user': user_secrets.get_secret(\"NEON_USER\"),\n    'password': user_secrets.get_secret(\"NEON_PASSWORD\"),\n    'port': 5432,\n    'sslmode': 'require'\n}\n\n# Complete centralized configuration\nCONFIG = {\n    'github': {\n        'token': user_secrets.get_secret(\"GITHUB_TOKEN\"),\n        'repo_url': 'https://github.com/amiralpert/SmartReach.git',\n        'local_path': '/kaggle/working/SmartReach',\n        'branch': 'main'\n    },\n    'database': {\n        'connection_pool_size': 5,\n        'max_connections': 10,\n        'connection_timeout': 30,\n        'query_timeout': 60,\n        'retry_attempts': 3,\n        'batch_size': 100\n    },\n    'models': {\n        'confidence_threshold': 0.75,\n        'warm_up_enabled': True,\n        'warm_up_text': 'Test entity extraction with biotechnology company.',\n        'device_preference': 'auto',  # 'auto', 'cuda', 'cpu'\n        'model_timeout': 30\n    },\n    'cache': {\n        'enabled': True,\n        'max_size_mb': 512,\n        'ttl_hours': 24,\n        'cleanup_interval': 3600\n    },\n    'processing': {\n        'filing_batch_size': 3,\n        'entity_batch_size': 50,\n        'max_section_length': 50000,\n        'enable_parallel': True,\n        'max_workers': 4,\n        'section_validation': True,\n        'filing_query_limit': 10,\n        'enable_relationships': True,\n        'relationship_batch_size': 15,\n        'context_window_chars': 400\n    },\n    'entity_extraction': {\n        'max_chunk_size': 2000,  # Conservative limit for transformer models (512 tokens)\n        'chunk_overlap': 200,     # Overlap between chunks to catch entities at boundaries\n        'max_chunks_per_section': 50,  # Limit chunks to prevent excessive processing\n        'enable_chunking': True   # Enable/disable text chunking for large documents\n    },\n    'gliner': {\n        'enabled': True,\n        'entity_model': 'urchade/gliner_medium-v2.1',\n        'relation_model': 'jackboyla/glirel-base',\n        'model_size': 'medium',  # 'small', 'medium', 'large'\n        'entity_labels': [\n            'Person', 'Filing Company', 'Private Company', 'Public Company',\n            'Government Agency', 'Date', 'Money', 'Location', 'Product',\n            'Technology', 'Financial Instrument', 'Law', 'Patent',\n            'Drug', 'Disease', 'Regulatory Body'\n        ],\n        'confidence_threshold': 0.7,\n        'relation_threshold': 0.6,\n        'enable_relationships': True,\n        'relation_types': [\n            'employed_by', 'subsidiary_of', 'owns', 'part_of',\n            'located_in', 'affiliated_with', 'contracts_with',\n            'acquired_by', 'merged_with', 'partner_of'\n        ],\n        'max_text_length': 50000,\n        'normalization': {\n            'enable_coreference': True,\n            'similarity_threshold': 0.85,\n            'company_normalization': True\n        },\n        'output': {\n            'verbose': False,\n            'save_full_text': True,\n            'include_context': True\n        }\n    },\n    'llama': {\n        'enabled': True,\n        'model_name': 'meta-llama/Llama-3.1-8B-Instruct',\n        'batch_size': 15,\n        'max_new_tokens': 50,\n        'context_window': 400,\n        'temperature': 0.3,\n        'entity_context_window': 400,\n        'test_max_tokens': 50,\n        'min_confidence_filter': 0.8,\n        'timeout_seconds': 30,\n        'SEC_FilingsPrompt': SEC_FILINGS_PROMPT,  # Now imported from module\n    },\n    'edgar': {\n        'identity': 'SmartReach BizIntel amir.alpert@gmail.com',\n        'rate_limit_delay': 0.1,\n        'max_retries': 3,\n        'timeout_seconds': 30\n    }\n}\n\n# Error checking for required secrets\nrequired_secrets = ['NEON_HOST', 'NEON_DATABASE', 'NEON_USER', 'NEON_PASSWORD', 'GITHUB_TOKEN']\nmissing_secrets = []\n\nfor secret in required_secrets:\n    try:\n        value = user_secrets.get_secret(secret)\n        if not value:\n            missing_secrets.append(secret)\n    except Exception as e:\n        missing_secrets.append(secret)\n\nif missing_secrets:\n    print(f\"âŒ Missing required secrets: {missing_secrets}\")\n    print(\"   Please add these secrets in Kaggle's Settings > Secrets\")\n    raise ValueError(\"Missing required secrets\")\n\nprint(\"âœ… All required secrets validated\")\n\n# Configuration validation and display\nprint(\"\\nðŸ”§ Configuration Summary:\")\nprint(f\"   â€¢ Database: {NEON_CONFIG['host']} / {NEON_CONFIG['database']}\")\nprint(f\"   â€¢ Models: {len(['biobert', 'bert', 'roberta', 'finbert'])} NER models + Llama 3.1-8B\")\nprint(f\"   â€¢ GLiNER: {'Enabled' if CONFIG['gliner']['enabled'] else 'Disabled'} - {CONFIG['gliner']['model_size']} model\")\nprint(f\"   â€¢ Processing: {CONFIG['processing']['filing_batch_size']} filings/batch\")\nprint(f\"   â€¢ Cache: {CONFIG['cache']['max_size_mb']}MB limit\")\nprint(f\"   â€¢ Relationships: {'Enabled' if CONFIG['processing']['enable_relationships'] else 'Disabled'}\")\nprint(f\"   â€¢ Text Chunking: {CONFIG['entity_extraction']['max_chunk_size']} chars/chunk with {CONFIG['entity_extraction']['chunk_overlap']} overlap\")\n\n# ============================================================================\n# INITIALIZE COMPONENTS\n# ============================================================================\n\n# Initialize global cache for section extraction using imported class\nSECTION_CACHE = SizeLimitedLRUCache(max_size_mb=CONFIG['cache']['max_size_mb'])\n\n# Create database connection function with NEON_CONFIG\ndef get_db_connection_configured():\n    \"\"\"Database connection using our configuration\"\"\"\n    return get_db_connection(NEON_CONFIG)\n\n# ============================================================================\n# MODULE CLEARING AND EDGARTOOLS SETUP\n# ============================================================================\n\nprint(\"\\nðŸ§¹ Clearing modules and setting up EdgarTools...\")\n\n# Clear any existing modules to ensure fresh imports\nmodules_to_clear = [mod for mod in sys.modules.keys() if 'SmartReach' in mod]\nfor module in modules_to_clear:\n    del sys.modules[module]\n\n# Configure EdgarTools identity\nset_identity(CONFIG['edgar']['identity'])\nprint(f\"   âœ… EdgarTools identity set: {CONFIG['edgar']['identity']}\")\n\n# ============================================================================\n# FINAL INITIALIZATION MESSAGES\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"ðŸŽ‰ CELL 1 INITIALIZATION COMPLETE\")\nprint(\"=\"*80)\n\nprint(f\"âœ… GitHub repository ready at: {LOCAL_PATH}\")\nprint(f\"âœ… Database connection configured: {NEON_CONFIG['host']}\")\nprint(f\"âœ… Configuration loaded with {len(CONFIG)} main sections\")\nprint(f\"âœ… Modular components imported from EntityExtractionEngine\")\nprint(f\"âœ… Size-limited cache initialized: {CONFIG['cache']['max_size_mb']}MB limit\")\nprint(f\"âœ… EdgarTools identity configured\")\nprint(f\"âœ… Logging functions available: log_error, log_warning, log_info\")\nprint(f\"âœ… Database context manager available: get_db_connection_configured()\")\nprint(f\"âœ… Llama 3.1-8B relationship extraction prompt configured\")\nprint(f\"âœ… GLiNER configuration: {CONFIG['gliner']['model_size']} model with {len(CONFIG['gliner']['entity_labels'])} entity types\")\nprint(f\"âœ… Entity extraction chunking: {CONFIG['entity_extraction']['max_chunks_per_section']} chunks max\")\nprint(f\"âœ… Console logging active from Cell -1 - using core.console_logs table\")\n\nprint(f\"\\nðŸš€ Ready to proceed to Cell 2 for EdgarTools section extraction!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:10:05.069606Z",
     "iopub.status.busy": "2025-09-13T18:10:05.069316Z",
     "iopub.status.idle": "2025-09-13T18:10:12.336615Z",
     "shell.execute_reply": "2025-09-13T18:10:12.336382Z",
     "shell.execute_reply.started": "2025-09-13T18:10:05.069581Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 2: Database Functions and ORM-like Models with Batching - SIMPLIFIED\n\n# Start real-time console logging for this cell\nstart_cell_logging(2)\n\n# Basic startup check - restart kernel if issues persist\nprint(\"Starting Cell 2 - EdgarTools section extraction\")\n\n# Ensure identity is set\nset_identity(CONFIG['edgar']['identity'])\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    TimeoutError,\n    get_filing_sections,\n    route_sections_to_models, \n    process_sec_filing_with_sections,\n    get_unprocessed_filings\n)\n\nprint(\"âœ… Imported EdgarTools processing components from EntityExtractionEngine\")\n\n# ============================================================================\n# WRAPPER FUNCTIONS FOR CONFIGURED COMPONENTS\n# ============================================================================\n\ndef get_filing_sections_configured(accession_number: str, filing_type: str = None) -> Dict[str, str]:\n    \"\"\"Get filing sections using global configuration and cache\"\"\"\n    return get_filing_sections(accession_number, filing_type, SECTION_CACHE, CONFIG)\n\ndef process_sec_filing_configured(filing_data: Dict) -> Dict:\n    \"\"\"Process SEC filing using global configuration and cache\"\"\"\n    return process_sec_filing_with_sections(filing_data, SECTION_CACHE, CONFIG)\n\ndef get_unprocessed_filings_configured(limit: int = 5) -> List[Dict]:\n    \"\"\"Get unprocessed filings using configured database connection\"\"\"\n    return get_unprocessed_filings(get_db_connection_configured, limit)\n\n# ============================================================================\n# TESTING AND VALIDATION\n# ============================================================================\n\n# Test the simplified extraction with timeout protection\nlog_info(\"Test\", \"Starting section extraction test with timeout protection\")\n\ntest_filings = get_unprocessed_filings_configured(limit=1)\n\nif test_filings:\n    print(f\"\\nðŸ§ª Testing with filing: {test_filings[0]['company_domain']} - {test_filings[0]['filing_type']}\")\n    print(f\"   Accession: {test_filings[0]['accession_number']}\")\n    \n    test_result = process_sec_filing_configured(test_filings[0])\n    \n    if test_result['processing_status'] == 'success':\n        log_info(\"Test\", f\"âœ… Successfully extracted {test_result['total_sections']} sections\")\n    elif test_result['processing_status'] == 'timeout':\n        log_warning(\"Test\", f\"â±ï¸ Processing timed out - filing may be too large or slow\")\n    elif test_result['processing_status'] == 'skipped':\n        log_info(\"Test\", f\"â­ï¸ Skipped problematic filing\")\n    else:\n        log_error(\"Test\", f\"âŒ Section extraction failed: {test_result.get('error')}\")\nelse:\n    log_info(\"Test\", \"No test filings available (all may be processed or problematic)\")\n\nprint(\"âœ… Cell 2 complete - EdgarTools section extraction with timeout protection ready\")"
  },
  {
   "cell_type": "code",
   "source": "# Cell 3: Entity Extraction Pipeline Initialization\n\n# Start real-time console logging for this cell\nstart_cell_logging(3)\n\nprint(\"ðŸ”¬ Entity Extraction Pipeline - GLiNER Integration Mode\")\n\n# ============================================================================\n# EXISTING MULTI-MODEL APPROACH (COMMENTED OUT FOR GLINER TESTING)\n# ============================================================================\n\nprint(\"âš ï¸  Multi-model NER pipeline temporarily disabled for GLiNER integration\")\nprint(\"   The existing 4-model system (BioBERT, BERT-base, RoBERTa, FinBERT) is commented out\")\nprint(\"   This preserves the original functionality while testing GLiNER enhancement\")\n\n# # ============================================================================\n# # IMPORT MODULAR COMPONENTS (COMMENTED OUT)\n# # ============================================================================\n# \n# # Import from our modular EntityExtractionEngine\n# from EntityExtractionEngine import EntityExtractionPipeline\n# \n# print(\"âœ… Imported EntityExtractionPipeline from modular components\")\n# \n# # ============================================================================\n# # INITIALIZE ENTITY EXTRACTION PIPELINE (COMMENTED OUT)\n# # ============================================================================\n# \n# print(\"ðŸ”§ Initializing multi-model NER pipeline...\")\n# \n# # Initialize the entity extraction pipeline with configuration\n# entity_pipeline = EntityExtractionPipeline(CONFIG)\n# \n# print(\"âœ… Entity extraction pipeline initialized with:\")\n# print(f\"   ðŸ“Š Models: BioBERT, BERT-base, RoBERTa, FinBERT\")\n# print(f\"   ðŸŽ¯ Confidence threshold: {CONFIG['models']['confidence_threshold']}\")\n# print(f\"   ðŸ“ Entity batch size: {CONFIG['processing']['entity_batch_size']}\")\n# print(f\"   ðŸ”„ Text chunking: {CONFIG['entity_extraction']['max_chunk_size']} chars/chunk\")\n# print(f\"   âš¡ Consensus scoring enabled for quality metrics\")\n# \n# # ============================================================================\n# # WARM-UP MODELS (OPTIONAL) (COMMENTED OUT)\n# # ============================================================================\n# \n# if CONFIG['models']['warm_up_enabled']:\n#     print(\"\\nðŸƒ Warming up NER models...\")\n#     try:\n#         test_text = CONFIG['models']['warm_up_text']\n#         test_entities = entity_pipeline.extract_entities(test_text, \"test\")\n#         print(f\"   âœ… Models warmed up - extracted {len(test_entities)} test entities\")\n#     except Exception as e:\n#         print(f\"   âš ï¸ Model warm-up skipped: {e}\")\n\n# ============================================================================\n# GLINER INTEGRATION PREPARATION\n# ============================================================================\n\nprint(\"\\nðŸ§ª Preparing GLiNER integration...\")\n\n# Import GLiNER components\nfrom EntityExtractionEngine import (\n    GLiNEREntityExtractor,\n    GLiNEREntity,\n    GLiNERRelationship,\n    GLINER_AVAILABLE\n)\n\nif GLINER_AVAILABLE:\n    print(\"âœ… GLiNER components imported successfully\")\n    print(f\"   ðŸ“Š GLiNER entity extraction ready\")\n    print(f\"   ðŸ”— GLiREL relationship extraction ready\")\n    print(f\"   ðŸŽ¯ Configuration loaded from Cell 1 CONFIG['gliner']\")\nelse:\n    print(\"âŒ GLiNER components not available\")\n    print(\"   Ensure Cell 0 ran successfully with GLiNER package installation\")\n\n# Create placeholder for GLiNER pipeline (will be initialized when first used)\ngliner_extractor = None\n\nprint(\"\\nâœ… Cell 3 complete - GLiNER integration mode ready\")\nprint(\"   ðŸ”„ Multi-model approach: Commented out (preserved for restoration)\")\nprint(\"   ðŸ§ª GLiNER approach: Ready for initialization\")\nprint(\"   ðŸ“ Note: GLiNER extractor will be initialized on first use to save memory\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_4",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 5: Relationship Extractor with Local Llama 3.1-8B - MODULARIZED\n\n# Start real-time console logging for this cell\nstart_cell_logging(5)\n\nprint(\"ðŸ¦™ Loading Relationship Extractor with Local Llama 3.1-8B...\")\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import (\n    RelationshipExtractor,\n    SemanticRelationshipStorage,\n    PipelineEntityStorage,\n    process_filings_batch,\n    generate_pipeline_analytics_report\n)\n\nprint(\"âœ… Imported relationship processing components from EntityExtractionEngine\")\n\n# ============================================================================\n# INITIALIZE GLOBAL OBJECTS\n# ============================================================================\n\nprint(\"ðŸ”§ Initializing pipeline components...\")\n\n# Initialize relationship extraction and storage components\nrelationship_extractor = RelationshipExtractor(CONFIG)\nsemantic_storage = SemanticRelationshipStorage(CONFIG['database'])\npipeline_storage = PipelineEntityStorage(CONFIG['database'])\n\nprint(\"âœ… Pipeline components initialized:\")\nprint(f\"   ðŸ¦™ Llama model status: {'âœ… Loaded' if relationship_extractor.model else 'âŒ Failed'}\")\nprint(f\"   ðŸ’¾ Storage systems: âœ… Entity & âœ… Relationship storage initialized\")\n\n# ============================================================================\n# WRAPPER FUNCTIONS FOR CONFIGURED PROCESSING\n# ============================================================================\n\ndef process_filings_batch_configured(limit: int = None) -> Dict:\n    \"\"\"Process multiple SEC filings using configured pipeline components\"\"\"\n    return process_filings_batch(\n        entity_pipeline, relationship_extractor, pipeline_storage, \n        semantic_storage, CONFIG, limit\n    )\n\nprint(\"âœ… Cell 5 complete - Relationship extraction and storage ready\")\nprint(f\"   ðŸŽ¯ Batch processing: process_filings_batch_configured() function ready\")\nprint(f\"   ðŸ“Š Analytics: generate_pipeline_analytics_report() function ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T18:14:18.239404Z",
     "iopub.status.busy": "2025-09-13T18:14:18.239084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": "# Cell 6: Main Processing Pipeline with Relationship Extraction - MODULARIZED\n\n# Start real-time console logging for this cell\nstart_cell_logging(6)\n\n# ============================================================================\n# IMPORT MODULAR COMPONENTS\n# ============================================================================\n\n# Import from our modular EntityExtractionEngine\nfrom EntityExtractionEngine import execute_main_pipeline\n\nprint(\"âœ… Imported main pipeline orchestrator from EntityExtractionEngine\")\n\n# ============================================================================\n# EXECUTE MAIN PIPELINE\n# ============================================================================\n\n# Execute the complete SEC filing processing pipeline\nresults = execute_main_pipeline(\n    entity_pipeline, \n    relationship_extractor, \n    pipeline_storage, \n    semantic_storage, \n    CONFIG\n)\n\nprint(\"âœ… Cell 6 complete - Main pipeline execution finished\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_6",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell_7",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}